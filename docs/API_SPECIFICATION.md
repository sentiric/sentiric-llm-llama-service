# ğŸ“¡ API Spesifikasyonu (v2.1 - OpenAI Uyumlu)

Bu belge, `llm-llama-service`'in sunduÄŸu gRPC ve HTTP arayÃ¼zlerini tanÄ±mlar. Servis, endÃ¼stri standardÄ± **OpenAI API** formatÄ±nÄ± destekler.

## 1. gRPC Servisi: `LLMLocalService`

Bu servis, `sentiric-contracts v1.11.0` ile tanÄ±mlanmÄ±ÅŸtÄ±r ve akÄ±ÅŸ tabanlÄ±, dÃ¼ÅŸÃ¼k gecikmeli diyalog yÃ¶netimi iÃ§in tasarlanmÄ±ÅŸtÄ±r.

### 1.1. Servis TanÄ±mÄ±
```protobuf
service LLMLocalService {
  // Verilen diyalog baÄŸlamÄ±na gÃ¶re token-token metin Ã¼retir.
  rpc GenerateStream(LLMLocalServiceGenerateStreamRequest) 
      returns (stream LLMLocalServiceGenerateStreamResponse);
}
```

### 1.2. Ana Mesaj Tipleri
```protobuf
// Ä°stek MesajÄ±
message LLMLocalServiceGenerateStreamRequest {
  // AI'nÄ±n genel kiÅŸiliÄŸini ve kurallarÄ±nÄ± belirleyen ana talimat.
  string system_prompt = 1;

  // KullanÄ±cÄ±nÄ±n son sÃ¶ylediÄŸi cÃ¼mle veya sorduÄŸu soru.
  string user_prompt = 2;

  // (Opsiyonel) RAG iÃ§in kullanÄ±lan ek bilgi metni.
  optional string rag_context = 3;

  // (Opsiyonel) KonuÅŸmanÄ±n geÃ§miÅŸi.
  repeated ConversationTurn history = 4;

  // (Opsiyonel) Token Ã¼retme ayarlarÄ±nÄ± geÃ§ersiz kÄ±lmak iÃ§in.
  optional GenerationParams params = 5;
}

// YanÄ±t MesajÄ±
message LLMLocalServiceGenerateStreamResponse {
  oneof type {
    string token = 1;
    FinishDetails finish_details = 2;
  }
}
```
*Not: `ConversationTurn`, `GenerationParams` ve `FinishDetails` gibi yardÄ±mcÄ± mesajlarÄ±n detaylarÄ± iÃ§in `sentiric-contracts` reposuna bakÄ±nÄ±z.*

## 2. HTTP Endpoint'leri (OpenAI Uyumlu)

Servis, standart OpenAI istemcileri (WebUI, LangChain vb.) ile entegrasyon iÃ§in aÅŸaÄŸÄ±daki endpoint'leri sunar.

### 2.1. Model Listesi (`GET /v1/models`)
Mevcut aktif modeli dÃ¶ndÃ¼rÃ¼r. Gateway ve Client Discovery iÃ§in kullanÄ±lÄ±r.

```json
{
  "object": "list",
  "data": [
    {
      "id": "ggml-org/gemma-3-1b-it-qat-GGUF",
      "object": "model",
      "created": 1763763803,
      "owned_by": "sentiric-llm-service"
    }
  ]
}
```

### 2.2. Chat Completions (`POST /v1/chat/completions`)
Metin Ã¼retimi iÃ§in ana endpoint. Streaming (SSE) destekler.

**Ä°stek:**
```json
{
  "model": "gemma-3",
  "messages": [
    {"role": "system", "content": "Sen yardÄ±msever bir asistansÄ±n."},
    {"role": "user", "content": "Merhaba!"}
  ],
  "stream": true,
  "temperature": 0.8,
  "max_tokens": 1024,
  "response_format": { "type": "json_object" } // Opsiyonel: JSON Modu iÃ§in
}
```

**Grammar DesteÄŸi (GeliÅŸmiÅŸ):**
OpenAI standardÄ±na ek olarak, `grammar` alanÄ± ile saf GBNF string'i gÃ¶nderilebilir.

### 2.3. SaÄŸlÄ±k KontrolÃ¼ (`GET /health`)
```http
GET /health

Response (BaÅŸarÄ±lÄ±):
Status: 200 OK
{
  "status": "healthy",
  "model_ready": true,
  "engine": "llama.cpp"
}

Response (Model YÃ¼kleniyor):
Status: 503 Service Unavailable
{
  "status": "unhealthy",
  "model_ready": false,
  "engine": "llama.cpp"
}
```

## 3. Hata KodlarÄ±

*   **gRPC:** `UNAVAILABLE` (14) - Model hazÄ±r deÄŸil, `INVALID_ARGUMENT` (3) - Gerekli alanlar eksik, `INTERNAL` (13) - Beklenmedik motor hatasÄ±.
*   **HTTP:** `200 OK` - SaÄŸlÄ±klÄ±, `503 Service Unavailable` - Model hazÄ±r deÄŸil, `400 Bad Request` - HatalÄ± JSON veya parametre.

---
