# ğŸ—ï¸ Sistem Mimarisi (Rev. 2)

## 1. Sistem DiyagramÄ±

Servis, gelen istemci isteklerini iÅŸleyen, bir model motoru aracÄ±lÄ±ÄŸÄ±yla token Ã¼reten ve bu token'larÄ± stream eden bir yapÄ±dÄ±r. EÅŸzamanlÄ±lÄ±k, bir `LlamaContextPool` tarafÄ±ndan yÃ¶netilir.

```mermaid
graph TD
    subgraph Clients
        A[llm_cli]
        B[Python Client]
        C[llm-gateway]
    end

    subgraph LLM Service Container
        direction LR
        subgraph "API Endpoints"
            gRPC_Server[gRPC Server (mTLS)]
            HTTP_Server[HTTP Server (Health)]
        end

        LLM_Engine[LLM Engine]
        
        subgraph "Concurrency Management"
            LlamaContextPool(Llama Context Pool)
        end

        gRPC_Server --> LLM_Engine
        HTTP_Server --> LLM_Engine
        LLM_Engine --> LlamaContextPool
    end

    subgraph "llama.cpp Backend"
        libllama[libllama.so + common]
        ModelFile[(GGUF Model)]
    end
    
    Clients -- gRPC / HTTP --> LLM Service Container
    LlamaContextPool -- "Acquires/Releases Context" --> libllama
    libllama -- "Loads/Interacts" --> ModelFile

    classDef client fill:#d4edda,stroke:#155724
    classDef service fill:#cce5ff,stroke:#004085
    classDef backend fill:#f8d7da,stroke:#721c24
    
    class A,B,C client
    class gRPC_Server,HTTP_Server,LLM_Engine,LlamaContextPool service
    class libllama,ModelFile backend
```

## 2. EÅŸzamanlÄ±lÄ±k Modeli (Concurrency)

Mimari, bir **context havuzu (`LlamaContextPool`)** kullanarak gerÃ§ek eÅŸzamanlÄ±lÄ±k saÄŸlar. Bu, servisin en kritik performans bileÅŸenidir.

-   **Ä°lklendirme:** Servis baÅŸladÄ±ÄŸÄ±nda, `LLM_LLAMA_SERVICE_THREADS` ortam deÄŸiÅŸkeni ile belirlenen sayÄ±da `llama_context` oluÅŸturulur ve havuza eklenir.
-   **Ä°stek Ä°ÅŸleme:** Her gelen gRPC isteÄŸi, havuzdan boÅŸta bir `llama_context` "kiralar". Bu sÄ±rada diÄŸer istekler, havuzdaki diÄŸer boÅŸ context'leri kullanarak **paralel olarak** iÅŸlenir.
-   **Kaynak Ä°adesi ve Temizlik (Kritik AdÄ±m):** Ä°ÅŸlem bittiÄŸinde veya istemci baÄŸlantÄ±yÄ± kapattÄ±ÄŸÄ±nda, kullanÄ±lan context'in KV cache'i `llama_kv_cache_seq_rm(ctx, -1, 0, -1)` Ã§aÄŸrÄ±sÄ± ile **mutlaka temizlenir** ve context tekrar havuza bÄ±rakÄ±lÄ±r. Bu, bir sonraki isteÄŸin, Ã¶nceki isteÄŸin "hafÄ±zasÄ±" olmadan temiz bir state ile baÅŸlamasÄ±nÄ± garanti eder. Bu adÄ±mÄ±n atlanmasÄ±, state sÄ±zÄ±ntÄ±sÄ±na (state leak) ve hatalÄ± Ã§Ä±ktÄ±lara yol aÃ§ar.

## 3. Build ve BaÄŸÄ±mlÄ±lÄ±k Mimarisi

Sistem, baÄŸÄ±mlÄ±lÄ±klarÄ± derleme anÄ±nda Ã§Ã¶zÃ¼mleyen, taÅŸÄ±nabilir ve kendi kendine yeten (self-contained) bir Docker imaj yapÄ±sÄ± kullanÄ±r.

1.  **vcpkg Kurulumu:** `vcpkg` paket yÃ¶neticisi, `vcpkg.json` dosyasÄ±nda belirtilen C++ kÃ¼tÃ¼phanelerini (`gRPC`, `spdlog` vb.) derler.
2.  **`llama.cpp` Klonlama:** `Dockerfile` iÃ§inde belirtilen **sabit bir commit hash'i** kullanÄ±larak `ggerganov/llama.cpp` reposu klonlanÄ±r. Bu, tekrarlanabilir ve stabil build'leri garanti eder.
3.  **Uygulama Derlemesi:** Projenin ana kodu (`llm_service`, `llm_cli`), `vcpkg` kÃ¼tÃ¼phanelerine ve anlÄ±k derlenen `llama` ve `common` kÃ¼tÃ¼phanelerine karÅŸÄ± derlenir. `CMakeLists.txt`, `LLAMA_BUILD_COMMON=ON` bayraÄŸÄ±nÄ± ayarlayarak `common` kÃ¼tÃ¼phanesinin derlenmesini zorunlu kÄ±lar.
4.  **Runtime Ä°majÄ±:** Minimal bir `ubuntu` veya `nvidia/cuda` runtime imajÄ± Ã¼zerine sadece Ã§alÄ±ÅŸtÄ±rÄ±labilir dosyalar ve `llama.cpp`'nin gerektirdiÄŸi paylaÅŸÄ±lan kÃ¼tÃ¼phaneler (`*.so`) kopyalanÄ±r ve `ldconfig` ile linklenir.

---
