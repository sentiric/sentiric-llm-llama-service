# ğŸ—ï¸ Sistem Mimarisi

## 1. Sistem DiyagramÄ±

Servis, gelen istemci isteklerini iÅŸleyen, bir model motoru aracÄ±lÄ±ÄŸÄ±yla token Ã¼reten ve bu token'larÄ± stream eden bir yapÄ±dÄ±r. EÅŸzamanlÄ±lÄ±k, bir `LlamaContextPool` tarafÄ±ndan yÃ¶netilir.

```mermaid
graph TD
    subgraph Clients
        A[llm_cli]
        B[Python Client]
        C[...]
    end

    subgraph LLM Service Container
        direction LR
        subgraph "API Endpoints"
            gRPC_Server[gRPC Server]
            HTTP_Server[HTTP Server]
        end

        LLM_Engine[LLM Engine]
        
        subgraph "Concurrency Management"
            LlamaContextPool(Llama Context Pool)
        end

        gRPC_Server --> LLM_Engine
        HTTP_Server --> LLM_Engine
        LLM_Engine --> LlamaContextPool
    end

    subgraph "llama.cpp Backend"
        libllama[libllama.so + deps]
        ModelFile[(Phi-3 GGUF Model)]
    end
    
    Clients -- gRPC / HTTP --> LLM Service Container
    LlamaContextPool -- "Acquires/Releases" --> libllama
    libllama -- "Loads/Interacts" --> ModelFile

    classDef client fill:#d4edda,stroke:#155724
    classDef service fill:#cce5ff,stroke:#004085
    classDef backend fill:#f8d7da,stroke:#721c24
    
    class A,B,C client
    class gRPC_Server,HTTP_Server,LLM_Engine,LlamaContextPool service
    class libllama,ModelFile backend
```

## 2. EÅŸzamanlÄ±lÄ±k Modeli (Concurrency)

Mimari, bir **context havuzu (`LlamaContextPool`)** kullanarak gerÃ§ek eÅŸzamanlÄ±lÄ±k saÄŸlar:

-   Servis baÅŸladÄ±ÄŸÄ±nda, `LLM_THREADS` sayÄ±sÄ± kadar `llama_context` oluÅŸturulur ve havuza eklenir.
-   Her gelen gRPC isteÄŸi, havuzdan boÅŸta bir `llama_context` talep eder.
-   Ä°stek, bu context'i kullanarak token Ã¼retme iÅŸlemini gerÃ§ekleÅŸtirir. Bu sÄ±rada diÄŸer istekler, havuzdaki diÄŸer boÅŸ context'leri kullanarak paralel olarak iÅŸlenebilir.
-   Ä°ÅŸlem bittiÄŸinde, context'in KV cache'i `llama_memory_seq_rm` ile temizlenir ve tekrar havuza bÄ±rakÄ±lÄ±r. Bu, bir sonraki isteÄŸin temiz bir state ile baÅŸlamasÄ±nÄ± garanti eder.

## 3. Build ve BaÄŸÄ±mlÄ±lÄ±k Mimarisi

Sistem, baÄŸÄ±mlÄ±lÄ±klarÄ± derleme anÄ±nda Ã§Ã¶zÃ¼mleyen, taÅŸÄ±nabilir ve kendi kendine yeten (self-contained) bir Docker imaj yapÄ±sÄ± kullanÄ±r.

1.  **vcpkg Kurulumu:** `vcpkg` paket yÃ¶neticisi, `vcpkg.json` dosyasÄ±nda belirtilen C++ kÃ¼tÃ¼phanelerini (`gRPC`, `spdlog` vb.) derler.
2.  **`llama.cpp` Klonlama:** `ggerganov/llama.cpp` reposunun en gÃ¼ncel `master` branch'i, derleme ortamÄ±na klonlanÄ±r.
3.  **Uygulama Derlemesi:** Projenin ana kodu, `vcpkg` ve anlÄ±k derlenen `llama.cpp` kÃ¼tÃ¼phanelerine karÅŸÄ± derlenir.
4.  **Runtime Ä°majÄ±:** Minimal bir Ubuntu imajÄ± Ã¼zerine sadece Ã§alÄ±ÅŸtÄ±rÄ±labilir dosyalar ve `llama.cpp`'nin gerektirdiÄŸi paylaÅŸÄ±lan kÃ¼tÃ¼phaneler (`*.so`) kopyalanÄ±r ve `ldconfig` ile linklenir.


---
