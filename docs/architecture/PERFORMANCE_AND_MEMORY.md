# ğŸš€ Bellek YÃ¶netimi ve Performans AyarlarÄ±

`sentiric-llm-llama-service`, kÄ±sÄ±tlÄ± donanÄ±m kaynaklarÄ±ndan bile maksimum verim alacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r. Ancak bu, doÄŸru yapÄ±landÄ±rma ile mÃ¼mkÃ¼ndÃ¼r. Bu belge, servisin performansÄ±nÄ± ve bellek kullanÄ±mÄ±nÄ± nasÄ±l yÃ¶neteceÄŸinizi aÃ§Ä±klar.

## Temel Bellek FormÃ¼lÃ¼

Bir GPU Ã¼zerinde Ã§alÄ±ÅŸÄ±rken, toplam VRAM kullanÄ±mÄ±nÄ± ÅŸu basit formÃ¼l belirler:

**Toplam VRAM â‰ˆ Model VRAM + ( `THREADS` Ã— Context BaÅŸÄ±na VRAM )**

-   **Model VRAM:** Modelin GPU'ya yÃ¼klenen katmanlarÄ±nÄ±n (`LLM_LLAMA_SERVICE_GPU_LAYERS`) kapladÄ±ÄŸÄ± alan.
-   **THREADS:** AynÄ± anda iÅŸlenebilecek istek sayÄ±sÄ± (`LLM_LLAMA_SERVICE_THREADS`).
-   **Context BaÅŸÄ±na VRAM:** Her bir `context`'in KV cache'i iÃ§in ayÄ±rdÄ±ÄŸÄ± bellek. Bu, `LLM_LLAMA_SERVICE_CONTEXT_SIZE` ile doÄŸru orantÄ±lÄ±dÄ±r.

EÄŸer bu toplam, GPU'nuzun toplam VRAM'ini aÅŸarsa, `cudaMalloc failed: out of memory` hatasÄ± alÄ±rsÄ±nÄ±z.

### Ã–rnek Senaryo: 6GB VRAM'li Bir GPU

-   **Model:** `gemma-3-4b-it-Q4_K_M.gguf` (YaklaÅŸÄ±k **2.8 GB** VRAM kullanÄ±r).
-   **Kalan VRAM:** 6 GB - 2.8 GB = **3.2 GB**

| `CONTEXT_SIZE` | Context BaÅŸÄ±na VRAM (Tahmini) | `THREADS` | Toplam KV Cache VRAM | Toplam VRAM | SonuÃ§ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| `8192` | ~1.6 GB | `2` | 3.2 GB | 2.8 + 3.2 = **6.0 GB** | **Riskli** âŒ |
| `8192` | ~1.6 GB | `1` | 1.6 GB | 2.8 + 1.6 = **4.4 GB** | **GÃ¼venli** âœ… |
| `4096` | ~0.8 GB | `3` | 2.4 GB | 2.8 + 2.4 = **5.2 GB** | **GÃ¼venli** âœ… |
| `4096` | ~0.8 GB | `2` | 1.6 GB | 2.8 + 1.6 = **4.4 GB** | **Ã‡ok GÃ¼venli** âœ… |

**Kural:** Bellek hatasÄ± alÄ±yorsanÄ±z, ilk olarak `LLM_LLAMA_SERVICE_THREADS` veya `LLM_LLAMA_SERVICE_CONTEXT_SIZE` deÄŸerlerini dÃ¼ÅŸÃ¼rÃ¼n.

## 1. Bellek Haritalama (Memory Mapping - mmap)

-   **Ne Ä°ÅŸe Yarar?** Normalde, bir LLM servisi Ã§alÄ±ÅŸtÄ±ÄŸÄ±nda, model dosyasÄ±nÄ±n (GB'larca bÃ¼yÃ¼klÃ¼kte olabilir) tamamÄ±nÄ± RAM'e yÃ¼kler. `mmap` etkinleÅŸtirildiÄŸinde (`LLM_LLAMA_SERVICE_USE_MMAP=true`), servis modeli RAM'e kopyalamak yerine, iÅŸletim sisteminin sanal bellek yÃ¶neticisini kullanarak doÄŸrudan disk Ã¼zerindeki dosyaya eriÅŸir.
-   **AvantajÄ±:** RAM kullanÄ±mÄ± dramatik ÅŸekilde dÃ¼ÅŸer. Ã–zellikle RAM'i model boyutundan daha az olan sistemlerde servisin Ã§alÄ±ÅŸabilmesini saÄŸlar ve baÅŸlangÄ±Ã§ sÃ¼resini kÄ±saltÄ±r.

## 2. KV Cache GPU Offloading

-   **Ne Ä°ÅŸe Yarar?** Bir LLM, metin Ã¼retirken daha Ã¶nce Ã¼rettiÄŸi token'larÄ± "hatÄ±rlamak" iÃ§in bir KV cache (anahtar-deÄŸer Ã¶nbelleÄŸi) kullanÄ±r. Bu cache, context boyutu bÃ¼yÃ¼dÃ¼kÃ§e Ã§ok fazla bellek tÃ¼ketir. Bu ayar (`LLM_LLAMA_SERVICE_KV_OFFLOAD=true`) etkinleÅŸtirildiÄŸinde, bu cache'in tamamÄ± CPU'nun RAM'i yerine GPU'nun VRAM'ine yÃ¼klenir.
-   **AvantajÄ±:**
    1.  DeÄŸerli sistem RAM'ini serbest bÄ±rakÄ±r.
    2.  Token Ã¼retimi sÄ±rasÄ±nda CPU ve GPU arasÄ±nda veri transferini ortadan kaldÄ±rarak **Ã¼retim hÄ±zÄ±nÄ± (token/saniye) ciddi ÅŸekilde artÄ±rÄ±r.**

## 3. AkÄ±llÄ± BaÄŸlam KaydÄ±rma (Context Shifting)

Bu, servisin en gÃ¼Ã§lÃ¼ yeteneklerinden biridir.

-   **Problem:** EÄŸer modele, `CONTEXT_SIZE` (Ã¶rn: 8192 token) ayarÄ±ndan daha bÃ¼yÃ¼k bir metin (Ã¶rn: 20,000 token'lÄ±k bir RAG dokÃ¼manÄ±) verirseniz ne olur? Ã‡oÄŸu servis hata verir veya Ã§Ã¶ker.
-   **Ã‡Ã¶zÃ¼m:** `llm-llama-service`, bu durumu akÄ±llÄ±ca yÃ¶netir.
    1.  Prompt'un, context penceresine sÄ±ÄŸmayacak kadar bÃ¼yÃ¼k olduÄŸunu anlar.
    2.  Prompt'u tek seferde iÅŸlemek yerine, bir dÃ¶ngÃ¼ iÃ§inde parÃ§alar halinde iÅŸlemeye baÅŸlar.
    3.  Modelin hafÄ±zasÄ± (KV cache) dolduÄŸunda, en eski bilgileri (prompt'un baÅŸlangÄ±cÄ±) hafÄ±zadan atar ve yeni bilgilere yer aÃ§ar. Bunu `llama_kv_cache_seq_rm` fonksiyonu ile yapar.
-   **AvantajÄ±:** Bu mekanizma sayesinde servis, teorik olarak **sonsuz uzunluktaki metinleri bile** sÄ±nÄ±rlÄ± bir bellek (VRAM/RAM) ile iÅŸleyebilir. Bu, Ã§ok bÃ¼yÃ¼k dokÃ¼manlar Ã¼zerinde RAG analizi yapmak iÃ§in servisi ideal hale getirir.

---