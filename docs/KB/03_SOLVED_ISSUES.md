# ğŸ’¡ Ã‡Ã¶zÃ¼lmÃ¼ÅŸ Sorunlar VeritabanÄ± (Knowledge Base)

Bu dokÃ¼man, geliÅŸtirme sÄ±rasÄ±nda karÅŸÄ±laÅŸÄ±lan Ã¶nemli sorunlarÄ±, kÃ¶k nedenlerini ve Ã§Ã¶zÃ¼mlerini "post-mortem" formatÄ±nda belgeler.

---

### SORUN-001: API UyumsuzluÄŸu Nedeniyle Derleme HatalarÄ±

-   **Tarih:** 2025-11-11
-   **Belirtiler:** `llama_kv_cache_clear`, `llama_sample_token_greedy`, `llama_batch_add` gibi fonksiyonlarÄ±n derleyici tarafÄ±ndan bulunamamasÄ±. `llama_tokenize` gibi fonksiyonlarÄ±n yanlÄ±ÅŸ argÃ¼man aldÄ±ÄŸÄ±nÄ± belirten hatalar.
-   **KÃ¶k Neden:** Proje kodu, `llama.cpp` kÃ¼tÃ¼phanesinin eski bir API setini kullanÄ±yordu. Ancak `Dockerfile`, `git clone` ile kÃ¼tÃ¼phanenin en gÃ¼ncel sÃ¼rÃ¼mÃ¼nÃ¼ Ã§ekiyordu. Bu, API imzalarÄ± ve fonksiyon adlarÄ± arasÄ±nda bÃ¼yÃ¼k bir uyumsuzluÄŸa yol aÃ§tÄ±.
-   **Ã‡Ã¶zÃ¼m:**
    1.  `llm_engine.cpp` ve `.h` dosyalarÄ±, gÃ¼ncel `llama.cpp` API'sine gÃ¶re tamamen yeniden yazÄ±ldÄ± (refactor edildi).
    2.  Model yÃ¼kleme `llama_model_load_from_file` ile, context oluÅŸturma `llama_init_from_model` ile gÃ¼ncellendi.
    3.  Token Ã¼retimi, `llama_sampler` (Ã¶rnekleyici zinciri) mimarisine geÃ§irildi. `llama_sampler_sample` ve `llama_sampler_accept` fonksiyonlarÄ± kullanÄ±ldÄ±.
    4.  Eski manuel KV cache yÃ¶netimi, `llama_memory_seq_rm` ile deÄŸiÅŸtirildi.

---

### SORUN-002: `libllama.so` / `libggml.so` YÃ¼klenememe HatasÄ± (Runtime)

-   **Tarih:** 2025-11-11
-   **Belirtiler:** Konteyner baÅŸlarken `error while loading shared libraries: libXXX.so: cannot open shared object file` hatasÄ± vererek Ã§Ã¶kÃ¼yordu.
-   **KÃ¶k Neden:** Docker multi-stage build sÃ¼recinde, `llama.cpp` tarafÄ±ndan derlenen paylaÅŸÄ±lan kÃ¼tÃ¼phaneler (`libllama.so`, `libggml.so` vb.) `builder` aÅŸamasÄ±nda kalÄ±yor ve son `runtime` imajÄ±na kopyalanmÄ±yordu.
-   **Ã‡Ã¶zÃ¼m:** `Dockerfile`'Ä±n `runtime` aÅŸamasÄ±na ÅŸu adÄ±mlar eklendi:
    1.  `builder` aÅŸamasÄ±ndaki `build/bin/` dizininde bulunan tÃ¼m `.so` dosyalarÄ±nÄ± (`*.so`) `runtime` aÅŸamasÄ±ndaki `/usr/local/lib/` dizinine kopyalayan bir `COPY` komutu eklendi.
    2.  Ä°ÅŸletim sisteminin bu yeni kÃ¼tÃ¼phaneleri tanÄ±masÄ± iÃ§in `RUN ldconfig` komutu eklendi.

---

### SORUN-003: `spdlog`/`fmt` Derleme HatasÄ± (`Cannot format an argument`)

-   **Tarih:** 2025-11-11
-   **Belirtiler:** `static assertion failed: Cannot format an argument. To make type T formattable provide a formatter<T> specialization...` hatasÄ±.
-   **KÃ¶k Neden:** `spdlog` kÃ¼tÃ¼phanesi, `grpc::StatusCode` gibi Ã¶zel bir `enum` tipini nasÄ±l string'e formatlayacaÄŸÄ±nÄ± bilmiyordu.
-   **Ã‡Ã¶zÃ¼m:** `src/cli/grpc_client.cpp` dosyasÄ±nÄ±n baÅŸÄ±na, `grpc::StatusCode` iÃ§in bir `fmt::formatter` uzmanlaÅŸmasÄ± (specialization) eklendi. Bu yapÄ±, `fmt` kÃ¼tÃ¼phanesine `grpc::StatusCode` enum'unu gÃ¶rdÃ¼ÄŸÃ¼nde onu anlamlÄ± bir metne (Ã¶r: "CANCELLED (1)") nasÄ±l Ã§evireceÄŸini Ã¶ÄŸretti.

---

### SORUN-004: `llama.cpp`'nin Eski Versiyonu (`b7046`) ile Ciddi API UyumsuzluklarÄ±

-   **Tarih:** 2025-11-13
-   **Belirtiler:** `llama_eval`, `llama_kv_cache_clear`, `llama_sample_top_k` gibi temel fonksiyonlarÄ±n derleyici tarafÄ±ndan bulunamamasÄ± (`was not declared in this scope`). `llama_tokenize` gibi fonksiyonlarÄ±n yanlÄ±ÅŸ imza (`cannot convert 'llama_model*' to 'const llama_vocab*'`) ile Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nÄ± belirten hatalar.
-   **KÃ¶k Neden:** Proje kodu, `llama.cpp` kÃ¼tÃ¼phanesinin modern API konseptlerine (Ã¶rn: `llama_sampler_chain`, `vocab*` soyutlamasÄ±) dayalÄ± olarak yazÄ±lmÄ±ÅŸtÄ±. Ancak `Dockerfile` ile sabitlenen `b7046` commit'i, bu modern soyutlamalarÄ±n **hiÃ§birini** iÃ§ermeyen Ã§ok daha eski ve temel bir API setine sahiptir. Temel hata, `b7046` API'sinin yetenekleri ve yapÄ±sÄ± hakkÄ±ndaki yanlÄ±ÅŸ varsayÄ±mlardÄ±. Ã–zellikle, Ã¶rnekleme (sampling) fonksiyonlarÄ±nÄ±n `libllama.so`'nun bir parÃ§asÄ± olmadÄ±ÄŸÄ±, `examples/` dizinindeki yardÄ±mcÄ± kodlar olduÄŸu anlaÅŸÄ±ldÄ±.
-   **Ã‡Ã¶zÃ¼m:**
    1.  **Tam Geri Ã‡ekilme:** `LLMEngine`, `b7046`'nÄ±n `include/llama.h` dosyasÄ±nda `LLAMA_API` ile iÅŸaretlenmiÅŸ **sadece** temel ve halka aÃ§Ä±k fonksiyonlarÄ± kullanacak ÅŸekilde sÄ±fÄ±rdan yeniden yazÄ±ldÄ±.
    2.  **Temel DeÄŸerlendirme DÃ¶ngÃ¼sÃ¼:** Modern `llama_decode` (batch) yerine, `llama_eval` kullanÄ±larak tek tek token iÅŸleme mantÄ±ÄŸÄ±na geri dÃ¶nÃ¼ldÃ¼. *(Not: Sonraki dÃ¼zeltmelerle bunun `llama_decode` ve `llama_sampler_sample` olduÄŸu teyit edildi.)*
    3.  **DoÄŸru Fonksiyon Ä°mzalarÄ±:** TÃ¼m `llama_*` fonksiyon Ã§aÄŸrÄ±larÄ±, `b7046`'nÄ±n gerektirdiÄŸi doÄŸru parametre tÃ¼rlerine (Ã¶rn: `llama_tokenize` iÃ§in `vocab*` yerine `model*`) gÃ¶re dÃ¼zeltildi.
    4.  **EÅŸzamanlÄ±lÄ±k Modelini BasitleÅŸtirme:** Sorunu izole etmek iÃ§in `LlamaContextPool`, geÃ§ici olarak tek bir, mutex korumalÄ± `llama_context` ile deÄŸiÅŸtirildi. Bu, karmaÅŸÄ±k state yÃ¶netimi hatalarÄ±nÄ± ortadan kaldÄ±rdÄ±.
    5.  **Ã–ÄŸrenilen Ders:** Harici bir C/C++ kÃ¼tÃ¼phanesinin eski bir versiyonuna sabitlenirken, sadece `git checkout` yapmak yeterli deÄŸildir. O versiyona ait `include` dizinindeki baÅŸlÄ±k dosyalarÄ±, projenin tek ve mutlak referansÄ± olarak kabul edilmelidir.

---