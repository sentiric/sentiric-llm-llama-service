# ğŸ’¡ KB-04: Proje Ä°Ã§i `llama.cpp` API BaÄŸlayÄ±cÄ± KontratÄ±

**AMAÃ‡:** Bu dokÃ¼man, projenin kullandÄ±ÄŸÄ± `llama.cpp` versiyonuna (b7415) Ã¶zel API kullanÄ±m desenlerini ve bellek yÃ¶netimi kurallarÄ±nÄ± tanÄ±mlayan **baÄŸlayÄ±cÄ± teknik ÅŸartnamedir**. `LLMEngine` ve `Service` katmanlarÄ± bu kurallara sÄ±kÄ± sÄ±kÄ±ya uymalÄ±dÄ±r.

## 1. BaÄŸlÄ± Olunan Versiyon ve Kaynaklar

-   **Versiyon Tag:** `b7415` (master)
-   **Referans Commit:** ``
-   **Header DosyalarÄ±:**
    -   `include/llama.h`: Ã‡ekirdek API.
    -   `common/common.h`: YardÄ±mcÄ± araÃ§lar (Batch ekleme, string iÅŸleme vb.).

**KURAL:** `llama.h` iÃ§indeki fonksiyon imzalarÄ± esastÄ±r. `llama_token_...` fonksiyonlarÄ±nÄ±n Ã§oÄŸu `deprecated` olmuÅŸ, yerini `llama_vocab_...` fonksiyonlarÄ±na bÄ±rakmÄ±ÅŸtÄ±r.

---

## 2. BaÅŸlatma (Initialization) AkÄ±ÅŸÄ±

Eski `llama_new_context_with_model` yapÄ±sÄ± **deprecated** olmuÅŸtur. Model ve Context yÃ¼kleme yaÅŸam dÃ¶ngÃ¼sÃ¼ ayrÄ±lmÄ±ÅŸtÄ±r.

```cpp
// 1. Backend BaÅŸlatma (Program baÅŸÄ±nda 1 kez)
llama_backend_init();

// 2. Model YÃ¼kleme
auto mparams = llama_model_default_params();
mparams.n_gpu_layers = 99; // GPU offload
struct llama_model* model = llama_model_load_from_file("model.gguf", mparams);

if (!model) { /* Hata YÃ¶netimi */ }

// 3. Context (Oturum) OluÅŸturma
auto cparams = llama_context_default_params();
cparams.n_ctx = 4096;
cparams.n_batch = 512;
cparams.flash_attn_type = LLAMA_FLASH_ATTN_TYPE_ENABLED; // Flash Attention desteÄŸi

struct llama_context* ctx = llama_init_from_model(model, cparams);

if (!ctx) { /* Hata YÃ¶netimi */ }

// 4. Vocab (Kelime DaÄŸarcÄ±ÄŸÄ±) EriÅŸimi (Ã–NEMLÄ°: Token iÅŸlemleri iÃ§in gerekli)
const struct llama_vocab* vocab = llama_model_get_vocab(model);
```

---

## 3. Tokenizasyon ve Detokenizasyon

**KRÄ°TÄ°K DEÄÄ°ÅÄ°KLÄ°K:** ArtÄ±k tokenizasyon iÅŸlemleri `llama_context` deÄŸil, `llama_vocab` Ã¼zerinden yapÄ±lmaktadÄ±r.

```cpp
// Metni Token'a Ã‡evirme
std::string text = "Merhaba dÃ¼nya";
int n_tokens_max = text.length() + 2;
std::vector<llama_token> tokens(n_tokens_max);

// NOT: llama_tokenize artÄ±k 'vocab' alÄ±r
int n_tokens = llama_tokenize(vocab, text.c_str(), text.length(), tokens.data(), n_tokens_max, true, false);
if (n_tokens < 0) { /* Buffer yetersiz hatasÄ± */ }
tokens.resize(n_tokens);

// Token'Ä± Metne Ã‡evirme
char buf[256];
// NOT: llama_token_to_piece artÄ±k 'vocab' alÄ±r
int n = llama_token_to_piece(vocab, tokens[0], buf, sizeof(buf), 0, true);
std::string piece(buf, n);
```

---

## 4. Inference (Ã‡Ä±karÄ±m) DÃ¶ngÃ¼sÃ¼ ve Batch YÃ¶netimi

Batch yÃ¶netimi iÃ§in `llama_batch` struct'Ä± ve `common.h` iÃ§indeki yardÄ±mcÄ± fonksiyon kullanÄ±lmalÄ±dÄ±r.

```cpp
// Batch HazÄ±rlama
llama_batch batch = llama_batch_init(512, 0, 1); // n_tokens, embd, n_seq_max

// Prompt'u Batch'e Ekleme (common.h yardÄ±mcÄ±sÄ± ile)
for (size_t i = 0; i < prompt_tokens.size(); ++i) {
    // Son token hariÃ§ logits hesaplama (false), son token iÃ§in hesapla (true)
    bool calc_logits = (i == prompt_tokens.size() - 1);
    common_batch_add(batch, prompt_tokens[i], i, {0}, calc_logits);
}

// Decode Ä°ÅŸlemi
if (llama_decode(ctx, batch) != 0) {
    // 1 = KV Cache dolu, <0 = Hata
    /* Hata YÃ¶netimi */
}

// Sonraki dÃ¶ngÃ¼ler iÃ§in batch temizliÄŸi
// llama_batch_free(batch); // Sadece iÅŸ tamamen bitince
common_batch_clear(batch); // DÃ¶ngÃ¼ iÃ§inde yeniden kullanÄ±m iÃ§in
```

---

## 5. Ã–rnekleme (Sampling) Zinciri

`llama.cpp` b7415, modÃ¼ler bir `llama_sampler` yapÄ±sÄ± kullanÄ±r. En sonda mutlaka bir seÃ§im yapÄ±cÄ± (Greedy veya Dist) olmalÄ±dÄ±r.

```cpp
// Zincir parametreleri
auto sparams = llama_sampler_chain_default_params();
llama_sampler* chain = llama_sampler_chain_init(sparams);

// Zincire Kurallar Ekleme (SÄ±ralama Ã–nemli)
llama_sampler_chain_add(chain, llama_sampler_init_penalties(
    64,     // last_n
    1.1f,   // repeat penalty
    0.0f,   // frequency penalty
    0.0f    // presence penalty
));
llama_sampler_chain_add(chain, llama_sampler_init_top_k(40));
llama_sampler_chain_add(chain, llama_sampler_init_top_p(0.95f, 1));
llama_sampler_chain_add(chain, llama_sampler_init_temp(0.8f));

// KRÄ°TÄ°K: Zincirin sonunda mutlaka bir seÃ§im sampler'Ä± olmalÄ±
// llama_sampler_init_greedy() veya llama_sampler_init_dist(seed)
llama_sampler_chain_add(chain, llama_sampler_init_dist(LLAMA_DEFAULT_SEED));

// Token SeÃ§imi
llama_token new_token_id = llama_sampler_sample(chain, ctx, -1); // -1: son token
llama_sampler_accept(chain, new_token_id); // SeÃ§imi kabul et (state gÃ¼ncelle)

// Temizlik
llama_sampler_free(chain);
```

---

## 6. Bellek ve KV Cache YÃ¶netimi (KRÄ°TÄ°K)

`llama.cpp` modern API'sinde KV Cache manipÃ¼lasyonu `llama_context` Ã¼zerinden deÄŸil, `llama_get_memory(ctx)` ile alÄ±nan `llama_memory_t` arayÃ¼zÃ¼ Ã¼zerinden yapÄ±lÄ±r.

**YanlÄ±ÅŸ KullanÄ±m:** `llama_kv_cache_seq_rm(ctx, ...)` (ArtÄ±k yok veya deprecated)
**DoÄŸru KullanÄ±m:**

```cpp
// 1. Bellek YÃ¶neticisini Al
llama_memory_t mem = llama_get_memory(ctx);

// 2. Bir Sequence'i Tamamen Silme (Context havuza iade edilirken)
llama_memory_seq_rm(mem, 0, -1, -1); 
// seq_id=0, p0=-1 (baÅŸtan), p1=-1 (sona kadar)

// 3. Context Shifting (KaydÄ±rma)
// Ã–rn: Ä°lk 10 tokenÄ± sil, kalanlarÄ± baÅŸa kaydÄ±r
llama_memory_seq_rm(mem, 0, 0, 10);      // [0, 10) aralÄ±ÄŸÄ±nÄ± sil
llama_memory_seq_add(mem, 0, 10, -1, -10); // [10, son) aralÄ±ÄŸÄ±nÄ± -10 pozisyon kaydÄ±r
```

---

## 7. Temizlik (Teardown)

SÄ±ralama Ã¶nemlidir.

```cpp
llama_batch_free(batch);
llama_sampler_free(chain);
llama_free(ctx);
llama_model_free(model);
llama_backend_free();
```

---


## 8. LoRA AdaptÃ¶r YÃ¶netimi (YENÄ°)

LoRA adaptÃ¶rleri **Model** seviyesinde yÃ¼klenir, ancak **Context** seviyesinde aktif edilir. Bu, aynÄ± modeli kullanan farklÄ± context'lerin farklÄ± LoRA ayarlarÄ±na sahip olabilmesini saÄŸlar.

```cpp
// 1. LoRA AdaptÃ¶rÃ¼nÃ¼ YÃ¼kleme (Dosyadan)
// AdaptÃ¶r model ile iliÅŸkilendirilir.
struct llama_adapter_lora* lora_adapter = llama_adapter_lora_init(model, "path/to/adapter.gguf");

if (!lora_adapter) { /* Hata YÃ¶netimi: Dosya bulunamadÄ± veya uyumsuz */ }

// 2. AdaptÃ¶rÃ¼ Context'e Uygulama
// scale: Etki oranÄ± (0.0 - 1.0 arasÄ± veya daha yÃ¼ksek). 
// Birden fazla LoRA eklenebilir.
float scale = 0.8f;
int32_t err = llama_set_adapter_lora(ctx, lora_adapter, scale);

if (err != 0) { /* Hata YÃ¶netimi */ }

// 3. AdaptÃ¶rÃ¼ Context'ten KaldÄ±rma (Ä°steÄŸe baÄŸlÄ±)
// Sadece bu context Ã¼zerindeki etkiyi kaldÄ±rÄ±r, adaptÃ¶r modelde yÃ¼klÃ¼ kalÄ±r.
llama_rm_adapter_lora(ctx, lora_adapter);

// 4. TÃ¼m AdaptÃ¶rleri Temizleme
llama_clear_adapter_lora(ctx);

// 5. AdaptÃ¶rÃ¼ Bellekten Silme
// DÄ°KKAT: `llama_model_free(model)` Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nda baÄŸlÄ± adaptÃ¶rler otomatik silinir.
// Ancak manuel olarak erkenden silmek isterseniz:
llama_adapter_lora_free(lora_adapter);
```

**KURAL:** LoRA adaptÃ¶rleri `llama_decode` Ã§aÄŸrÄ±sÄ±ndan **Ã¶nce** set edilmelidir. Decode iÅŸlemi sÄ±rasÄ±nda dinamik olarak deÄŸiÅŸtirmek mÃ¼mkÃ¼ndÃ¼r ancak KV Cache Ã¼zerindeki etkileri (cache'in yeniden hesaplanmasÄ± gerekip gerekmediÄŸi) dikkate alÄ±nmalÄ±dÄ±r. Genellikle session baÅŸÄ±nda set edilmesi Ã¶nerilir.

---

## 9. Temizlik (Teardown)

SÄ±ralama Ã¶nemlidir. Model silindiÄŸinde baÄŸlÄ± LoRA'lar da silinir.

```cpp
llama_batch_free(batch);
llama_sampler_free(chain);
// Context silindiÄŸinde Ã¼zerindeki LoRA baÄŸlarÄ± kopar (adaptÃ¶r silinmez)
llama_free(ctx); 
// Model silindiÄŸinde baÄŸlÄ± LoRA adaptÃ¶rleri (llama_adapter_lora_init ile gelenler) serbest bÄ±rakÄ±lÄ±r
llama_model_free(model);
llama_backend_free();
```


---
