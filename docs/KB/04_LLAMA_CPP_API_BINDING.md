# ğŸ’¡ KB-04: Proje Ä°Ã§i `llama.cpp` API BaÄŸlayÄ±cÄ± KontratÄ±

**AMAÃ‡:** Bu dokÃ¼man, bu projenin kullandÄ±ÄŸÄ± `llama.cpp` versiyonuna Ã¶zel API kullanÄ±m desenlerini tanÄ±mlayan **tek ve mutlak doÄŸru kaynaktÄ±r**. `LLMEngine` Ã¼zerinde geliÅŸtirme yaparken baÅŸka hiÃ§bir varsayÄ±mda bulunulmamalÄ±dÄ±r. Bir API hatasÄ± alÄ±ndÄ±ÄŸÄ±nda, ilk olarak buraya baÅŸvurulmalÄ±, eÄŸer bilgi eksikse, `llama.cpp` kaynak kodu incelenerek **Ã¶nce bu belge gÃ¼ncellenmeli, sonra kod dÃ¼zeltilmelidir.**

## 1. BaÄŸlÄ± Olunan Versiyon

-   **Commit Hash:** `92bb442ad999a0d52df0af2730cd861012e8ac5c`
-   **Commit Linki:** [https://github.com/ggml-org/llama.cpp/commit/92bb442ad999a0d52df0af2730cd861012e8ac5c](https://github.com/ggml-org/llama.cpp/commit/92bb442ad999a0d52df0af2730cd861012e8ac5c)
-   **Referans `llama.h`:** [llama.h @ 92bb442](https://github.com/ggml-org/llama.cpp/blob/92bb442ad999a0d52df0af2730cd861012e8ac5c/include/llama.h)
-   **Referans `common.h`:** [common.h @ 92bb442](https://github.com/ggml-org/llama.cpp/blob/92bb442ad999a0d52df0af2730cd861012e8ac5c/common/common.h)
-   **Son DoÄŸrulama Tarihi:** 2025-11-14

**KURAL:** `Dockerfile`'daki `LLAMA_CPP_VERSION` deÄŸiÅŸtirilirse, bu belge **mutlaka** gÃ¼ncellenmelidir.

---

## 2. Derleme ve Linkleme (CMake)

-   **`common` KÃ¼tÃ¼phanesi:** `llama.cpp`, yardÄ±mcÄ± fonksiyonlarÄ± iÃ§eren `common` adÄ±nda statik bir kÃ¼tÃ¼phane oluÅŸturur. Ancak bu, `LLAMA_BUILD_COMMON` CMake seÃ§eneÄŸi `ON` olarak ayarlandÄ±ÄŸÄ±nda gerÃ§ekleÅŸir. Bizim projemiz, `add_subdirectory` Ã§aÄŸÄ±rmadan Ã¶nce bu seÃ§eneÄŸi `ON` olarak ayarlayarak `common` kÃ¼tÃ¼phanesinin derlenmesini garanti eder.
-   **Linkleme:** `llm_service` ve `llm_cli` hedefleri, `target_link_libraries` aracÄ±lÄ±ÄŸÄ±yla hem `llama` hem de `common` hedeflerine linklenmelidir.
-   **BaÅŸlÄ±k DosyalarÄ±:** `llama.h` ve `common.h` dosyalarÄ±nÄ± kullanabilmek iÃ§in `llama.cpp/include` ve `llama.cpp/common` dizinleri `target_include_directories` ile projeye dahil edilmelidir.

---

## 3. Temel API KullanÄ±m Desenleri

### 3.1. Token Ãœretim (Inference) DÃ¶ngÃ¼sÃ¼

Bu desen, `LlamaContextPool`'dan kiralanan bir `llama_context` Ã¼zerinde Ã§alÄ±ÅŸÄ±r.

```cpp
// --- Gerekli BaÅŸlÄ±k DosyalarÄ± ---
#include "llama.h"
#include "common.h"

// --- 1. Prompt'u Ä°ÅŸleme ---

const auto* vocab = llama_model_get_vocab(model);
std::vector<llama_token> prompt_tokens;
// ... llama_tokenize(vocab, ...) Ã§aÄŸrÄ±sÄ± ile tokenleri al ...
prompt_tokens.resize(n_tokens);

llama_batch batch = llama_batch_init(n_tokens, 0, 1);
for (int i = 0; i < n_tokens; ++i) {
    // DOÄRU KULLANIM: `common.h`'dan gelen yardÄ±mcÄ± fonksiyonu kullan.
    common_batch_add(batch, prompt_tokens[i], i, {0}, false);
}
batch.logits[batch.n_tokens - 1] = true; // Sadece son token'Ä±n logit'lerine ihtiyacÄ±mÄ±z var

if (llama_decode(ctx, batch) != 0) { /* Hata yÃ¶netimi */ }

// --- 2. Yeni Token'larÄ± Ãœretme DÃ¶ngÃ¼sÃ¼ ---

llama_pos n_past = batch.n_tokens;
while (n_past < n_ctx) {
    llama_token new_token_id = llama_sampler_sample(sampler_chain, ctx, -1);
    llama_sampler_accept(sampler_chain, new_token_id);
    if (llama_vocab_is_eog(vocab, new_token_id)) break;

    // ... token'Ä± metne Ã§evir (`llama_token_to_piece`) ...

    llama_batch_free(batch); // Ã–nceki batch'i temizle
    batch = llama_batch_init(1, 0, 1);
    common_batch_add(batch, new_token_id, n_past, {0}, true);

    if (llama_decode(ctx, batch) != 0) { /* Hata yÃ¶netimi */ }
    n_past++;
}

llama_batch_free(batch);
```

### 3.2. Ã–rnekleme (Sampling) Zinciri OluÅŸturma

`llama_sampler_init_penalties` fonksiyonu `llama_context*` **almaz**.

```cpp
llama_sampler_chain_params sparams = llama_sampler_chain_default_params();
llama_sampler* sampler_chain = llama_sampler_chain_init(sparams);

// Ä°lk parametre `penalty_last_n` (int32_t), context boyutu olabilir.
llama_sampler_chain_add(sampler_chain, llama_sampler_init_penalties(
    llama_n_ctx(ctx), // penalty_last_n
    1.1f,             // penalty_repeat
    0.0f,             // penalty_freq
    0.0f              // penalty_present
));
llama_sampler_chain_add(sampler_chain, llama_sampler_init_top_k(40));
// ... diÄŸer sampler'lar ...
```

### 3.3. KV Cache YÃ¶netimi (KRÄ°TÄ°K)

Bir `llama_context`, `LlamaContextPool`'a iade edilmeden Ã¶nce, iÃ§indeki tÃ¼m KV cache verileri temizlenmelidir.

**DOÄRU YÃ–NTEM:**
```cpp
// Bir context'i havuza iade etmeden hemen Ã¶nce Ã§aÄŸrÄ±lacak kod:
// `llama_get_memory` ile context'in bellek yÃ¶neticisini al.
// seq_id = -1 -> TÃ¼m dizinler
// p0 = -1, p1 = -1 -> TÃ¼m pozisyonlar (veya p0=0, p1=-1)
llama_memory_seq_rm(llama_get_memory(ctx), -1, -1, -1);
```

---