# ğŸ’¡ KB-04: Proje Ä°Ã§i `llama.cpp` API BaÄŸlayÄ±cÄ± KontratÄ±

**AMAÃ‡:** Bu dokÃ¼man, bu projenin kullandÄ±ÄŸÄ± `llama.cpp` versiyonuna Ã¶zel API kullanÄ±m desenlerini tanÄ±mlayan **tek ve mutlak doÄŸru kaynaktÄ±r**. `LLMEngine` Ã¼zerinde geliÅŸtirme yaparken baÅŸka hiÃ§bir varsayÄ±mda bulunulmamalÄ±dÄ±r. Bir API hatasÄ± alÄ±ndÄ±ÄŸÄ±nda, ilk olarak buraya baÅŸvurulmalÄ±, eÄŸer bilgi eksikse, `llama.cpp` kaynak kodu incelenerek **Ã¶nce bu belge gÃ¼ncellenmeli, sonra kod dÃ¼zeltilmelidir.**

## 1. BaÄŸlÄ± Olunan Versiyon

-   **Commit Hash:** `92bb442ad999a0d52df0af2730cd861012e8ac5c`
-   **Commit Linki:** [https://github.com/ggml-org/llama.cpp/commit/92bb442ad999a0d52df0af2730cd861012e8ac5c](https://github.com/ggml-org/llama.cpp/commit/92bb442ad999a0d52df0af2730cd861012e8ac5c)
-   **Referans `llama.h`:** [llama.h @ 92bb442](https://github.com/ggml-org/llama.cpp/blob/92bb442ad999a0d52df0af2730cd861012e8ac5c/include/llama.h)
-   **Referans `common.h`:** [common.h @ 92bb442](https://github.com/ggml-org/llama.cpp/blob/92bb442ad999a0d52df0af2730cd861012e8ac5c/common/common.h)
-   **Son DoÄŸrulama Tarihi:** 2025-11-13

**KURAL:** `Dockerfile`'daki `LLAMA_CPP_VERSION` deÄŸiÅŸtirilirse, bu belge **mutlaka** gÃ¼ncellenmelidir.

---

## 2. Temel API KullanÄ±m Desenleri

### 2.1. BaÅŸlatma ve SonlandÄ±rma SÄ±rasÄ±

```cpp
// 1. Backend'i baÅŸlat (program baÅŸÄ±na bir kez)
llama_backend_init();

// 2. Model parametrelerini ayarla ve modeli yÃ¼kle
llama_model_params model_params = llama_model_default_params();
llama_model* model = llama_model_load_from_file(model_path.c_str(), model_params);

// 3. Context parametrelerini ayarla ve context'i oluÅŸtur
llama_context_params ctx_params = llama_context_default_params();
ctx_params.n_ctx = 4096;
llama_context* ctx = llama_init_from_model(model, ctx_params);

// ... kullanÄ±m ...

// Temizlik SÄ±rasÄ±
llama_free(ctx);
llama_model_free(model);
llama_backend_free();
```

### 2.2. Token Ãœretim (Inference) DÃ¶ngÃ¼sÃ¼

Bu desen, `LlamaContextPool`'dan kiralanan bir `llama_context` Ã¼zerinde Ã§alÄ±ÅŸÄ±r.

```cpp
// --- Gerekli BaÅŸlÄ±k DosyalarÄ± ---
#include "llama.h"
#include "common.h" // common_batch_add iÃ§in

// --- 1. Prompt'u Ä°ÅŸleme ---

const auto* vocab = llama_model_get_vocab(model);
std::vector<llama_token> prompt_tokens;
// ... llama_tokenize(vocab, ...) Ã§aÄŸrÄ±sÄ± ...
prompt_tokens.resize(n_tokens);

llama_batch batch = llama_batch_init(n_tokens, 0, 1);
for (int i = 0; i < n_tokens; ++i) {
    // DOÄRU KULLANIM: `common.h`'dan gelen yardÄ±mcÄ± fonksiyonu kullan.
    common_batch_add(batch, prompt_tokens[i], i, {0}, false);
}
batch.logits[batch.n_tokens - 1] = true;

if (llama_decode(ctx, batch) != 0) { /* Hata yÃ¶netimi */ }

// --- 2. Yeni Token'larÄ± Ãœretme DÃ¶ngÃ¼sÃ¼ ---

llama_pos n_past = batch.n_tokens;
while (n_past < n_ctx) {
    llama_token new_token_id = llama_sampler_sample(sampler_chain, ctx, -1);
    llama_sampler_accept(sampler_chain, new_token_id);
    if (llama_vocab_is_eog(vocab, new_token_id)) break;

    // ... token'Ä± metne Ã§evir ...

    llama_batch_free(batch);
    batch = llama_batch_init(1, 0, 1);
    common_batch_add(batch, new_token_id, n_past, {0}, true);

    if (llama_decode(ctx, batch) != 0) { /* Hata yÃ¶netimi */ }
    n_past++;
}

llama_batch_free(batch);
```

### 2.3. Ã–rnekleme (Sampling) Zinciri OluÅŸturma

`llama_sampler_init_penalties` fonksiyonu `llama_context*` **almaz**.

```cpp
// DOÄRU KULLANIM:
llama_sampler_chain_params sparams = llama_sampler_chain_default_params();
llama_sampler* sampler_chain = llama_sampler_chain_init(sparams);

// Ä°lk parametre `penalty_last_n` (int32_t), context boyutu olabilir.
llama_sampler_chain_add(sampler_chain, llama_sampler_init_penalties(
    llama_n_ctx(ctx), // penalty_last_n
    1.1f,             // penalty_repeat
    0.0f,             // penalty_freq
    0.0f              // penalty_present
));
llama_sampler_chain_add(sampler_chain, llama_sampler_init_top_k(40));
// ... diÄŸer sampler'lar ...

// YANLIÅ KULLANIM:
// llama_sampler_chain_add(sampler_chain, llama_sampler_init_penalties(ctx, ...)); // DERLEME HATASI
```

### 2.4. KV Cache YÃ¶netimi (KRÄ°TÄ°K)

Bir `llama_context` havuza iade edilmeden Ã¶nce, iÃ§indeki tÃ¼m KV cache verileri temizlenmelidir.

**DOÄRU YÃ–NTEM:**
    common.cpp dosyasÄ±, common kÃ¼tÃ¼phanesine deÄŸil, doÄŸrudan ana llama kÃ¼tÃ¼phanesine derleniyor. Bu nedenle -lcommon linkleme hatasÄ± veriyor.

    llama.h dosyasÄ±nda llama_kv_cache_seq_rm veya llama_kv_cache_clear bulunmuyor. Ä°ncelediÄŸin Ã¶rneklerde (server.cpp, main.cpp vb.) bu iÅŸlem llama_memory_seq_rm(llama_get_memory(ctx), ...) veya llama_kv_cache_clear(ctx) (eski versiyonlar) veya bazen llama_memory_clear(llama_get_memory(ctx), true) ile yapÄ±lÄ±yor. En gÃ¼venilir ve modern yÃ¶ntem, tÃ¼m dizinleri (seq_id = -1) ve pozisyonlarÄ± (p0 = -1, p1 = -1) hedef alan llama_memory_seq_rm Ã§aÄŸrÄ±sÄ±dÄ±r.
```cpp
// Bir context'i havuza iade etmeden hemen Ã¶nce 
llama_memory_seq_rm(llama_get_memory(ctx), -1, -1, -1) olarak dÃ¼zelt ve llama_get_memory(ctx) Ã§aÄŸrÄ±sÄ±nÄ±n zorunlu olduÄŸunu belirt.
```

---

## 3. Derleme ve Linkleme NotlarÄ±

- **`common` KÃ¼tÃ¼phanesi:** `llama.cpp` projesi, `common` adÄ±nda ayrÄ± bir linklenebilir kÃ¼tÃ¼phane **oluÅŸturmaz**. `common` dizinindeki kaynak dosyalar, ana `llama` kÃ¼tÃ¼phanesinin iÃ§ine derlenir. Bu nedenle, `target_link_libraries` komutunda `common`'a linkleme yapÄ±lmamalÄ±dÄ±r.
- **BaÅŸlÄ±k DosyalarÄ±:** `common.h` gibi yardÄ±mcÄ± baÅŸlÄ±k dosyalarÄ±nÄ± kullanabilmek iÃ§in `llama.cpp/common` dizini `target_include_directories` ile projemize dahil edilmelidir.
