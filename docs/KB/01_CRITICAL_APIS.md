# ğŸ’¡ KB-01: Kritik llama.cpp API ReferanslarÄ±

Bu dokÃ¼man, projemizde kullandÄ±ÄŸÄ±mÄ±z ve sÄ±k deÄŸiÅŸen `llama.cpp` API fonksiyonlarÄ±nÄ±n gÃ¼ncel kullanÄ±m kalÄ±plarÄ±nÄ± belgeler. Kod yazarken bu referansÄ± kullanÄ±n.

---

### 1. BaÅŸlatma ve YÃ¼kleme SÄ±rasÄ±

```cpp
// 1. Backend'i baÅŸlat (program baÅŸÄ±na bir kez)
llama_backend_init();

// 2. Model parametrelerini ayarla ve modeli yÃ¼kle
llama_model_params model_params = llama_model_default_params();
llama_model* model = llama_model_load_from_file(model_path.c_str(), model_params);

// 3. Modelin vokabÃ¼lerini al
const llama_vocab* vocab = llama_model_get_vocab(model);

// 4. Context parametrelerini ayarla ve context'i oluÅŸtur
llama_context_params ctx_params = llama_context_default_params();
ctx_params.n_ctx = 4096; // Ayarlanabilir
llama_context* ctx = llama_init_from_model(model, ctx_params);
```

### 2. Token Ãœretim (Inference) DÃ¶ngÃ¼sÃ¼

```cpp
// 1. Gelen metni tokenize et
std::vector<llama_token> tokens(prompt_text.size());
int n_tokens = llama_tokenize(
    vocab, 
    prompt_text.c_str(), 
    prompt_text.size(), 
    tokens.data(), 
    tokens.size(), 
    true, // Add BOS (Beginning of Sequence) token
    false
);
tokens.resize(n_tokens);

// 2. Prompt'u iÅŸle (initial decode)
llama_batch batch = llama_batch_get_one(tokens.data(), tokens.size());
if (llama_decode(ctx, batch) != 0) {
    // Hata yÃ¶netimi
}

// 3. Yeni token'larÄ± Ã¼retme dÃ¶ngÃ¼sÃ¼
int n_decoded = 0;
while (n_decoded < max_new_tokens) {
    // 3a. Bir sonraki token'Ä± Ã¶rnekle (-1: son token'Ä±n logit'lerinden)
    llama_token new_token_id = llama_sampler_sample(sampler, ctx, -1);

    // 3b. Ã–rnekleyiciye seÃ§ilen token'Ä± bildir (repetition penalty gibi state'leri gÃ¼nceller)
    llama_sampler_accept(sampler, new_token_id);

    // 3c. Ãœretimin sonuna gelindi mi diye kontrol et
    if (llama_vocab_is_eog(vocab, new_token_id)) {
        break;
    }

    // 3d. Token'Ä± metne Ã§evir ve stream et/kullan
    char piece;
    int n_chars = llama_token_to_piece(vocab, new_token_id, piece, sizeof(piece), 0, false);
    std::string token_str(piece, n_chars);
    // on_token_callback(token_str);

    // 3e. Bir sonraki decode iÅŸlemi iÃ§in tek token'lÄ±k yeni bir batch hazÄ±rla
    batch = llama_batch_get_one(&new_token_id, 1);

    // 3f. Yeni token'Ä± iÅŸle
    if (llama_decode(ctx, batch) != 0) {
        // Hata yÃ¶netimi
    }
    n_decoded++;
}
```

### 3. Temizlik SÄ±rasÄ±

```cpp
// Context'i serbest bÄ±rak
llama_free(ctx);

// Modeli serbest bÄ±rak
llama_model_free(model);

// Backend'i serbest bÄ±rak (program sonunda bir kez)
llama_backend_free();
```


---
