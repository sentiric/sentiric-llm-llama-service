# ğŸ‘¨â€ğŸ’» GeliÅŸtirici Rehberi

## Build Sistemi

### BaÄŸÄ±mlÄ±lÄ±k YÃ¶netimi
-   **vcpkg**: `gRPC`, `spdlog`, `httplib`, `prometheus-cpp` gibi temel C++ baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± yÃ¶netir.
-   **`llama.cpp`**: Docker build sÄ±rasÄ±nda sabitlenmiÅŸ bir commit'ten klonlanÄ±r ve anlÄ±k olarak derlenir. Proje, `llama.cpp`'yi bir alt dizin olarak kullanÄ±r.
-   **Protobuf**: `CMake`'in `FetchContent` modÃ¼lÃ¼, `sentiric-contracts` reposunu derleme anÄ±nda Ã§eker ve `protoc` ile gRPC/Protobuf kodlarÄ±nÄ± Ã¼retir.

### CMake YapÄ±landÄ±rmasÄ±
`CMakeLists.txt` dosyasÄ±, `vcpkg` toolchain'i kullanarak baÄŸÄ±mlÄ±lÄ±klarÄ± bulur. `llama.cpp` projesini `add_subdirectory` ile dahil eder ve `LLAMA_BUILD_COMMON=ON` bayraÄŸÄ±nÄ± ayarlayarak `common` kÃ¼tÃ¼phanesinin derlenmesini saÄŸlar. `llm_service` ve `llm_cli` hedefleri, hem `llama` hem de `common` kÃ¼tÃ¼phanelerine linklenir.

### Docker Best Practices
-   **Multi-stage Build:** `builder` aÅŸamasÄ± tÃ¼m derleme araÃ§larÄ±nÄ± iÃ§erir. `runtime` aÅŸamasÄ± ise sadece Ã§alÄ±ÅŸtÄ±rÄ±labilir dosyalarÄ± ve gerekli paylaÅŸÄ±lan kÃ¼tÃ¼phaneleri (`*.so`) iÃ§erir.
-   **Dinamik KÃ¼tÃ¼phane YÃ¶netimi:** `runtime` imajÄ±, `ldconfig` komutunu Ã§alÄ±ÅŸtÄ±rarak dinamik linker Ã¶nbelleÄŸini gÃ¼nceller.

---

## Kod StandartlarÄ± ve API KullanÄ±mÄ±

-   **C++17** standardÄ± zorunludur.
-   **RAII** prensibi benimsenmelidir.
-   **`llama.cpp` API KullanÄ±mÄ±:** Projenin kullandÄ±ÄŸÄ± `llama.cpp` versiyonu iÃ§in tÃ¼m temel API kullanÄ±m desenleri **`docs/KB/04_LLAMA_CPP_API_BINDING.md`** dosyasÄ±nda belgelenmiÅŸtir. Bu dosya, `llama.cpp` ile etkileÅŸimde tek doÄŸru kaynaktÄ±r. HÄ±zlÄ± bir fonksiyon referansÄ± iÃ§in **`docs/KB/05_LLAMA_API_REFERENCE.md`**'ye de gÃ¶z atabilirsiniz.

---

## GeliÅŸtirme SÄ±rasÄ±nda SÄ±k KullanÄ±lan Komutlar

### 1. Servisi Derleme ve BaÅŸlatma

Her kod deÄŸiÅŸikliÄŸinden sonra bu komut Ã§alÄ±ÅŸtÄ±rÄ±lmalÄ±dÄ±r.

```bash
# CPU iÃ§in
docker compose up --build -d

# GPU iÃ§in
docker compose -f docker-compose.yml -f docker-compose.gpu.yml -f docker-compose.gpu.override.yml up --build -d
```

### 2. `llm_cli` AracÄ±nÄ± Ã‡alÄ±ÅŸtÄ±rma

`llm_cli`'yi Ã§alÄ±ÅŸtÄ±rmak iÃ§in `docker compose run` komutu kullanÄ±lÄ±r. Bu komut, ilgili ortam iÃ§in (`cpu` veya `gpu`) tek seferlik bir konteyner baÅŸlatÄ±r.

#### CPU OrtamÄ±nda CLI KullanÄ±mÄ±

CPU iÃ§in ek bir dosyaya gerek yoktur. `docker-compose.override.yml` dosyasÄ±, `llm_cli` iÃ§in gerekli tanÄ±mÄ± iÃ§erir.

```bash
# CPU'da CLI Ã§alÄ±ÅŸtÄ±rma
docker compose run --rm llm-cli llm_cli <komut>
```

#### GPU OrtamÄ±nda CLI KullanÄ±mÄ±

GPU ortamÄ±nda `llm-cli`'yi Ã§alÄ±ÅŸtÄ±rmak iÃ§in, `docker-compose.run.gpu.yml` dosyasÄ±nÄ± Ã¶zel olarak belirtmeniz gerekir. Bu dosya, konteynere GPU eriÅŸimi saÄŸlar ve Ã§alÄ±ÅŸan servisin aÄŸÄ±na baÄŸlanÄ±r.

```bash
# GPU'da CLI Ã§alÄ±ÅŸtÄ±rma
docker compose -f docker-compose.run.gpu.yml run --rm llm-cli llm_cli <komut>
```

---