# ğŸ‘¨â€ğŸ’» GeliÅŸtirici Rehberi

## Build Sistemi

### BaÄŸÄ±mlÄ±lÄ±k YÃ¶netimi
-   **vcpkg**: `gRPC`, `spdlog`, `httplib` gibi temel C++ baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± yÃ¶netir.
-   **`llama.cpp`**: Docker build sÄ±rasÄ±nda sabitlenmiÅŸ bir commit'ten klonlanÄ±r ve anlÄ±k olarak derlenir. Proje, `llama.cpp`'yi bir alt dizin olarak kullanÄ±r.
-   **Protobuf**: `CMake`'in `FetchContent` modÃ¼lÃ¼, `sentiric-contracts` reposunu derleme anÄ±nda Ã§eker ve `protoc` ile gRPC/Protobuf kodlarÄ±nÄ± Ã¼retir.

### CMake YapÄ±landÄ±rmasÄ±
`CMakeLists.txt` dosyasÄ±, `vcpkg` toolchain'i kullanarak baÄŸÄ±mlÄ±lÄ±klarÄ± bulur. `llama.cpp` projesini `add_subdirectory` ile dahil eder ve `LLAMA_BUILD_COMMON=ON` bayraÄŸÄ±nÄ± ayarlayarak `common` kÃ¼tÃ¼phanesinin derlenmesini saÄŸlar. `llm_service` ve `llm_cli` hedefleri, hem `llama` hem de `common` kÃ¼tÃ¼phanelerine linklenir.

### Docker Best Practices
-   **Multi-stage Build:** `builder` aÅŸamasÄ± tÃ¼m derleme araÃ§larÄ±nÄ± iÃ§erir. `runtime` aÅŸamasÄ± ise sadece Ã§alÄ±ÅŸtÄ±rÄ±labilir dosyalarÄ± ve gerekli paylaÅŸÄ±lan kÃ¼tÃ¼phaneleri (`*.so`) iÃ§erir.
-   **Dinamik KÃ¼tÃ¼phane YÃ¶netimi:** `runtime` imajÄ±, `ldconfig` komutunu Ã§alÄ±ÅŸtÄ±rarak dinamik linker Ã¶nbelleÄŸini gÃ¼nceller.

---

## Kod StandartlarÄ± ve API KullanÄ±mÄ±

-   **C++17** standardÄ± zorunludur.
-   **RAII** prensibi benimsenmelidir.
-   **`llama.cpp` API KullanÄ±mÄ±:** Projenin kullandÄ±ÄŸÄ± `llama.cpp` versiyonu iÃ§in tÃ¼m temel API kullanÄ±m desenleri **`docs/KB/04_LLAMA_CPP_API_BINDING.md`** dosyasÄ±nda belgelenmiÅŸtir. Bu dosya, `llama.cpp` ile etkileÅŸimde tek doÄŸru kaynaktÄ±r.

---

## GeliÅŸtirme SÄ±rasÄ±nda SÄ±k KullanÄ±lan Komutlar

### Servisi BaÅŸlatma
```bash
# CPU iÃ§in
docker compose up --build -d

# GPU iÃ§in
docker compose -f docker-compose.yml -f docker-compose.gpu.yml -f docker-compose.gpu.override.yml up --build -d
```

### `llm_cli` AracÄ±nÄ± Kullanma (mTLS OrtamÄ±nda)

**Ã–NEMLÄ°:** llm_cli aracÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in docker compose exec yerine docker compose run komutunu kullanÄ±n. docker-compose.override.yml dosyalarÄ±, llm-cli adÄ±nda, mTLS iÃ§in gerekli ortam deÄŸiÅŸkenlerini iÃ§eren Ã¶zel bir servis tanÄ±mlar.

Bu yÃ¶ntem, komutlarÄ± basitleÅŸtirir ve ortam deÄŸiÅŸkeni sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nler.

Bunun iÃ§in projenin kÃ¶k dizininde, `docker-compose.yml` tarafÄ±ndan kullanÄ±lan tÃ¼m ortam deÄŸiÅŸkenlerini iÃ§eren bir `.env` dosyasÄ± oluÅŸturun:

**`.env` dosyasÄ± Ã¶rneÄŸi:**
```env
# Network
NETWORK_NAME=sentiric-net
NETWORK_SUBNET=10.88.0.0/16
NETWORK_GATEWAY=10.88.0.1
LLM_LLAMA_SERVICE_IPV4_ADDRESS=10.88.60.7

# Ports
LLM_LLAMA_SERVICE_HTTP_PORT=16070
LLM_LLAMA_SERVICE_GRPC_PORT=16071
LLM_LLAMA_SERVICE_METRICS_PORT=16072

# Security (mTLS) - Bu yollar, yerel makinenizdeki `sentiric-certificates` reposunun konumuna gÃ¶re ayarlanmalÄ±dÄ±r.
GRPC_TLS_CA_PATH=../sentiric-certificates/certs/ca.crt
LLM_LLAMA_SERVICE_CERT_PATH=../sentiric-certificates/certs/llm-llama-service-chain.crt
LLM_LLAMA_SERVICE_KEY_PATH=../sentiric-certificates/certs/llm-llama-service.key

# DiÄŸer servis deÄŸiÅŸkenleri...
```

`.env` dosyasÄ± oluÅŸturulduktan sonra, `llm_cli`'yi Ã§alÄ±ÅŸtÄ±rmak iÃ§in doÄŸru komut ÅŸudur:

```bash
# --env-file parametresi, .env dosyasÄ±ndaki deÄŸiÅŸkenleri exec komutuna aktarÄ±r
docker compose exec --env-file .env llm-llama-service llm_cli <komut>
```

**Ã–rnek `generate` komutu (CPU veya GPU):**
```bash
# `run --rm` komutu, `llm-cli` servisini Ã§alÄ±ÅŸtÄ±rÄ±r, komutu yÃ¼rÃ¼tÃ¼r ve sonra konteyneri kaldÄ±rÄ±r.
docker compose run --rm llm-cli llm_cli generate "Merhaba dÃ¼nya"
```

Ã–rnek health komutu:

```bash
docker compose run --rm llm-cli llm_cli health
```

---
