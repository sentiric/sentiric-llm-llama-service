# ğŸ‘¨â€ğŸ’» GeliÅŸtirici Rehberi

## Build Sistemi

### BaÄŸÄ±mlÄ±lÄ±k YÃ¶netimi
-   **vcpkg**: `gRPC`, `spdlog`, `httplib` gibi temel C++ baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± yÃ¶netir.
-   **`llama.cpp`**: Docker build sÄ±rasÄ±nda `master` branch'ten klonlanÄ±r ve anlÄ±k olarak derlenir. Proje, `llama.cpp`'yi bir alt dizin olarak kullanÄ±r ve ona karÅŸÄ± dinamik olarak linklenir.
-   **Protobuf**: `CMake`'in `FetchContent` modÃ¼lÃ¼, `sentiric-contracts` reposunu derleme anÄ±nda Ã§eker ve `protoc` ile gRPC/Protobuf kodlarÄ±nÄ± Ã¼retir.

### CMake YapÄ±landÄ±rmasÄ±
`CMakeLists.txt` dosyasÄ±, `vcpkg` toolchain'i kullanarak baÄŸÄ±mlÄ±lÄ±klarÄ± bulur. `llama.cpp` projesini `add_subdirectory` ile dahil eder. Ana `llm_service` ve `llm_cli` hedefleri, derlenen `libllama.so`'ya karÅŸÄ± linklenir.

```cmake
# Ã–nemli Linkleme Komutu
target_link_libraries(llm_service PRIVATE
    proto_lib
    llama  # libllama.so'ya linklenir
    spdlog::spdlog
    # ... diÄŸer vcpkg kÃ¼tÃ¼phaneleri
)
```

### Docker Best Practices
-   **Multi-stage Build:** `builder` aÅŸamasÄ± tÃ¼m derleme araÃ§larÄ±nÄ± ve ara dosyalarÄ± iÃ§erir. `runtime` aÅŸamasÄ± ise sadece Ã§alÄ±ÅŸtÄ±rÄ±labilir dosyalarÄ± ve gerekli paylaÅŸÄ±lan kÃ¼tÃ¼phaneleri (`*.so`) iÃ§erir.
-   **Dinamik KÃ¼tÃ¼phane YÃ¶netimi:** `runtime` imajÄ±, `builder`'dan kopyalanan `*.so` dosyalarÄ±nÄ± bulabilmesi iÃ§in `ldconfig` komutunu Ã§alÄ±ÅŸtÄ±rÄ±r.

```dockerfile
# Dockerfile'dan kritik runtime adÄ±mlarÄ±
FROM ubuntu:22.04 AS runtime

# ...
COPY --from=builder /app/build/llm_service /usr/local/bin/
COPY --from=builder /app/build/llm_cli /usr/local/bin/

# GEREKLÄ° TÃœM paylaÅŸÄ±lan kÃ¼tÃ¼phaneleri kopyala
COPY --from=builder /app/build/bin/*.so /usr/local/lib/

# Dinamik linker Ã¶nbelleÄŸini gÃ¼ncelle
RUN ldconfig
# ...
```

## Kod StandartlarÄ±

-   **C++17** standardÄ± zorunludur.
-   **RAII** prensibi benimsenmelidir. `std::unique_ptr` ve `std::shared_ptr` kullanÄ±mÄ± teÅŸvik edilir.
-   **Header Guard:** TÃ¼m header dosyalarÄ± `#pragma once` ile baÅŸlamalÄ±dÄ±r.
-   **Exception Safety:** Fonksiyonlar, istisna durumlarÄ±nda kaynak sÄ±zÄ±ntÄ±sÄ± yapmamalÄ±dÄ±r.

---

## Configuration

### Environment Variables

Servisi yapÄ±landÄ±rmak iÃ§in aÅŸaÄŸÄ±daki ortam deÄŸiÅŸkenlerini kullanÄ±n. Bu deÄŸiÅŸkenler `docker-compose.yml` dosyasÄ±nda veya production ortamÄ±nÄ±zda ayarlanabilir.

| DeÄŸiÅŸken                          | AÃ§Ä±klama                                                                | VarsayÄ±lan DeÄŸer              |
| --------------------------------- | ----------------------------------------------------------------------- | ----------------------------- |
| `LLM_LLAMA_SERVICE_IPV4_ADDRESS`  | Servisin dinleyeceÄŸi IP adresi. `0.0.0.0` tÃ¼m arayÃ¼zleri dinler.        | `0.0.0.0`                     |
| `LLM_LLAMA_SERVICE_HTTP_PORT`     | HTTP health check sunucusunun portu.                                    | `16070`                       |
| `LLM_LLAMA_SERVICE_GRPC_PORT`     | gRPC sunucusunun portu.                                                 | `16071`                       |
| `LLM_MODEL_PATH`                  | Konteyner iÃ§indeki GGUF model dosyasÄ±nÄ±n tam yolu.                      | `/models/phi-3-mini.q4.gguf`  |
| `LLM_CONTEXT_SIZE`                | Modelin maksimum context penceresi.                                     | `4096`                        |
| `LLM_THREADS`                     | Token Ã¼retimi iÃ§in kullanÄ±lacak CPU thread sayÄ±sÄ±.                      | (DonanÄ±mÄ±n yarÄ±sÄ±, max 8)     |
| `LOG_LEVEL`                       | Log seviyesi (`trace`, `debug`, `info`, `warn`, `error`, `critical`). | `info`                        |

**Ã–rnek `docker-compose.yml` YapÄ±landÄ±rmasÄ±:**

```yaml
# docker-compose.yml
services:
  llm-llama-service:
    # ...
    ports:
      - "127.0.0.1:16070:16070"
      - "127.0.0.1:16071:16071"
    environment:
      - LLM_LLAMA_SERVICE_IPV4_ADDRESS=0.0.0.0
      - LLM_LLAMA_SERVICE_HTTP_PORT=16070
      - LLM_LLAMA_SERVICE_GRPC_PORT=16071
      - LLM_THREADS=4
```

---