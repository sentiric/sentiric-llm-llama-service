# ğŸš€ Deployment Rehberi

Bu servis, esnek bir `docker-compose` yapÄ±sÄ± kullanarak farklÄ± senaryolarda kolayca daÄŸÄ±tÄ±labilir. Bu rehber, hem Ã¼retim (pre-built imajlarÄ± kullanarak) hem de geliÅŸtirme (kaynaktan derleyerek) ortamlarÄ± iÃ§in adÄ±mlarÄ± aÃ§Ä±klar.

## Mimari YaklaÅŸÄ±mÄ±: Temel + Profil + GeÃ§ersiz KÄ±lma

Tekrarlardan kaÃ§Ä±nmak ve yapÄ±landÄ±rmayÄ± basitleÅŸtirmek iÃ§in aÅŸaÄŸÄ±daki mimariyi kullanÄ±yoruz:
- **`docker-compose.yml`:** TÃ¼m ortak yapÄ±landÄ±rmalarÄ± iÃ§eren temel dosyadÄ±r.
- **`docker-compose.cpu.yml` / `docker-compose.gpu.yml`:** Sadece CPU veya GPU'ya Ã¶zel farklarÄ± (imaj adÄ±, kaynaklar) tanÄ±mlayan "profil" dosyalarÄ±dÄ±r.
- **`docker-compose.override.yml` / `docker-compose.gpu.override.yml`:** Sadece yerel geliÅŸtirme iÃ§in kaynaktan derleme (`build`) talimatlarÄ±nÄ± iÃ§eren "geÃ§ersiz kÄ±lma" dosyalarÄ±dÄ±r.

---

## 1. Ãœretim DaÄŸÄ±tÄ±mÄ± (Pre-built Ä°majlarÄ± Ã‡ekerek)

Bu senaryo, GitHub Container Registry'den (ghcr.io) hazÄ±r imajlarÄ± Ã§eker. En hÄ±zlÄ± ve en kararlÄ± yÃ¶ntemdir.

### 1.1. CPU Ãœzerinde Ã‡alÄ±ÅŸtÄ±rma

```bash
# Temel ve CPU profili dosyalarÄ±nÄ± kullanarak servisi baÅŸlat
# Bu komut, 'ghcr.io/sentiric/sentiric-llm-llama-service:latest' imajÄ±nÄ± Ã§eker
docker compose -f docker-compose.yml -f docker-compose.cpu.yml up -d
```

### 1.2. GPU Ãœzerinde Ã‡alÄ±ÅŸtÄ±rma (NVIDIA)

```bash
# Temel ve GPU profili dosyalarÄ±nÄ± kullanarak servisi baÅŸlat
# Bu komut, 'ghcr.io/sentiric/sentiric-llm-llama-service:latest-gpu' imajÄ±nÄ± Ã§eker
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
```

---

## 2. GeliÅŸtirme OrtamÄ± (Kaynaktan Derleyerek)

Bu senaryo, yerel kod deÄŸiÅŸikliklerinizi test etmek iÃ§in kullanÄ±lÄ±r.

### 2.1. CPU Ãœzerinde Derleme ve Ã‡alÄ±ÅŸtÄ±rma

`docker-compose.override.yml` dosyasÄ±, `docker compose` tarafÄ±ndan otomatik olarak algÄ±lanÄ±r.

```bash
# Bu komut, Dockerfile kullanarak yerel bir imaj oluÅŸturur ve servisi baÅŸlatÄ±r
docker compose -f docker-compose.yml -f docker-compose.cpu.yml -f docker-compose.override.yml up --build -d

```

### 2.2. GPU Ãœzerinde Derleme ve Ã‡alÄ±ÅŸtÄ±rma (NVIDIA)

GPU derlemesi iÃ§in geÃ§ersiz kÄ±lma dosyasÄ±nÄ± manuel olarak belirtmemiz gerekir.

```bash
# Temel, GPU profili ve GPU geÃ§ersiz kÄ±lma dosyalarÄ±nÄ± birleÅŸtirerek servisi baÅŸlat
# Bu komut, Dockerfile.gpu kullanarak yerel bir imaj oluÅŸturur
docker compose -f docker-compose.yml -f docker-compose.gpu.yml -f docker-compose.gpu.override.yml up --build -d
```

---

## 3. Servisi Durdurma

Hangi profille baÅŸlattÄ±ÄŸÄ±nÄ±zdan baÄŸÄ±msÄ±z olarak, servisi durdurmak iÃ§in:

```bash
docker compose down
```

## 4. Kaynak Gereksinimleri ve Ã–lÃ§eklendirme NotlarÄ±

Bu servis, `LlamaContextPool` mimarisi sayesinde gerÃ§ek eÅŸzamanlÄ±lÄ±k sunar. Ancak bu, kaynak kullanÄ±mÄ± Ã¼zerinde doÄŸrudan bir etkiye sahiptir.

### Temel FormÃ¼l

Gerekli toplam bellek (RAM veya VRAM), aÅŸaÄŸÄ±daki formÃ¼lle kabaca tahmin edilebilir:

**Toplam Bellek â‰ˆ Model Boyutu + ( `LLM_LLAMA_SERVICE_THREADS` Ã— Her Context iÃ§in KV Cache Boyutu )**

-   **Model Boyutu:** KullandÄ±ÄŸÄ±nÄ±z GGUF dosyasÄ±nÄ±n boyutu.
-   **KV Cache Boyutu:** Bu, `context_size` ve modelin mimarisine baÄŸlÄ±dÄ±r. `phi-3-mini-4k-instruct-q4.gguf` iÃ§in `4096` context ile yaklaÅŸÄ±k **1.5 GB**'tÄ±r.

### Ã–rnek Senaryolar

-   **CPU (16GB RAM):** `LLM_LLAMA_SERVICE_THREADS=3` ayarÄ±yla:
    `~2.3 GB (Model) + (3 * ~1.5 GB) = ~6.8 GB` RAM sadece servis iÃ§in gereklidir. Ä°ÅŸletim sistemi ve diÄŸer servislerle birlikte bu, 16 GB'lÄ±k bir sistemde yavaÅŸlamaya (swapping) neden olabilir. EÅŸzamanlÄ±lÄ±k seviyesini, mevcut sistem belleÄŸine gÃ¶re dikkatli bir ÅŸekilde ayarlayÄ±n.

-   **GPU (6GB VRAM):** `LLM_LLAMA_SERVICE_THREADS=3` ve `n_gpu_layers=-1` (tam offload) ayarÄ±yla:
    `~2.3 GB (Model) + (3 * ~1.5 GB) = ~6.8 GB` VRAM gereklidir. Bu, 6 GB VRAM'i aÅŸar ve `out of memory` hatasÄ±na yol aÃ§ar.
    -   **Ã‡Ã¶zÃ¼m:** Bu donanÄ±mda eÅŸzamanlÄ±lÄ±ÄŸÄ± saÄŸlamak iÃ§in ya `LLM_LLAMA_SERVICE_THREADS` sayÄ±sÄ±nÄ± `1` veya `2` gibi VRAM'e sÄ±ÄŸacak bir deÄŸere dÃ¼ÅŸÃ¼rÃ¼n ya da modelin sadece bir kÄ±smÄ±nÄ± GPU'ya offload edin (`n_gpu_layers`).

**Ã–neri:** Ãœretim ortamÄ±nda, hedeflenen eÅŸzamanlÄ±lÄ±k seviyesine yetecek kadar RAM veya VRAM kaynaÄŸÄ± ayarlanmalÄ±dÄ±r. GeliÅŸtirme ortamÄ±nda, `LLM_LLAMA_SERVICE_THREADS` deÄŸiÅŸkenini `1` olarak ayarlayarak sÄ±ralÄ± ama stabil bir ÅŸekilde Ã§alÄ±ÅŸabilirsiniz.

## Configuration

### Environment Variables

Servisi yapÄ±landÄ±rmak iÃ§in aÅŸaÄŸÄ±daki ortam deÄŸiÅŸkenlerini kullanÄ±n. TÃ¼m deÄŸiÅŸkenler `LLM_LLAMA_SERVICE_` Ã¶neki ile baÅŸlar.

| DeÄŸiÅŸken                                   | AÃ§Ä±klama                                                                          | VarsayÄ±lan DeÄŸer                  |
| ------------------------------------------ | --------------------------------------------------------------------------------- | --------------------------------- |
| **Network**                                |                                                                                   |                                   |
| `LLM_LLAMA_SERVICE_LISTEN_ADDRESS`           | Servisin dinleyeceÄŸi IP adresi. `0.0.0.0` tÃ¼m arayÃ¼zleri dinler.                  | `0.0.0.0`                         |
| `LLM_LLAMA_SERVICE_HTTP_PORT`              | HTTP health check sunucusunun portu.                                              | `16070`                           |
| `LLM_LLAMA_SERVICE_GRPC_PORT`              | gRPC sunucusunun portu.                                                           | `16071`                           |
| **Model Management**                       |                                                                                   |                                   |
| `LLM_LLAMA_SERVICE_MODEL_DIR`              | Modellerin indirileceÄŸi ve saklanacaÄŸÄ± konteyner iÃ§indeki dizin.                  | `/models`                         |
| `LLM_LLAMA_SERVICE_MODEL_ID`               | Hugging Face repo ID'si (Ã¶r: `microsoft/Phi-3-mini-4k-instruct-gguf`).          | `microsoft/Phi-3-mini-4k-instruct-gguf` |
| `LLM_LLAMA_SERVICE_MODEL_FILENAME`         | Ä°ndirilecek GGUF dosyasÄ±nÄ±n tam adÄ±.                                              | `Phi-3-mini-4k-instruct-q4.gguf`  |
| `LLM_LLAMA_SERVICE_MODEL_PATH`             | *[Legacy]* `MODEL_ID` belirtilmezse kullanÄ±lacak modelin tam yolu.                  | `""`                              |
| **Engine & Performance**                   |                                                                                   |                                   |
| `LLM_LLAMA_SERVICE_GPU_LAYERS`             | GPU'ya yÃ¼klenecek model katmanÄ± sayÄ±sÄ±. `-1` tÃ¼m katmanlarÄ± yÃ¼kler.               | `0`                               |
| `LLM_LLAMA_SERVICE_CONTEXT_SIZE`           | Modelin maksimum context penceresi.                                               | `4096`                            |
| `LLM_LLAMA_SERVICE_THREADS`                | Token Ã¼retimi iÃ§in kullanÄ±lacak CPU thread sayÄ±sÄ±.                                | (DonanÄ±mÄ±n yarÄ±sÄ±, max 8)         |
| **Logging**                                |                                                                                   |                                   |
| `LLM_LLAMA_SERVICE_LOG_LEVEL`              | Log seviyesi (`trace`, `debug`, `info`, `warn`, `error`, `critical`).           | `info`                            |
| **Default Sampling**                       | *gRPC isteÄŸinde belirtilmezse kullanÄ±lacak varsayÄ±lan deÄŸerler.*                    |                                   |
| `LLM_LLAMA_SERVICE_DEFAULT_MAX_TOKENS`     | VarsayÄ±lan maksimum Ã¼retilecek token sayÄ±sÄ±.                                      | `1024`                            |
| `LLM_LLAMA_SERVICE_DEFAULT_TEMPERATURE`    | VarsayÄ±lan sampling sÄ±caklÄ±ÄŸÄ±. YaratÄ±cÄ±lÄ±ÄŸÄ± artÄ±rÄ±r.                               | `0.8`                             |
| `LLM_LLAMA_SERVICE_DEFAULT_TOP_K`          | VarsayÄ±lan Top-K sampling deÄŸeri.                                                 | `40`                              |
| `LLM_LLAMA_SERVICE_DEFAULT_TOP_P`          | VarsayÄ±lan Top-P (nucleus) sampling deÄŸeri.                                       | `0.95`                            |
| `LLM_LLAMA_SERVICE_DEFAULT_REPEAT_PENALTY` | VarsayÄ±lan tekrar cezasÄ±. Tekrar eden metinleri engeller.                           | `1.1`                             |


**Ã–rnek `docker-compose.yml` YapÄ±landÄ±rmasÄ±:**

```yaml
# docker-compose.yml
services:
  llm-llama-service:
    # ...
    environment:
      - LLM_LLAMA_SERVICE_LISTEN_ADDRESS=0.0.0.0
      - LLM_LLAMA_SERVICE_HTTP_PORT=16070
      - LLM_LLAMA_SERVICE_GRPC_PORT=16071
      - LLM_LLAMA_SERVICE_THREADS=1
      - LLM_LLAMA_SERVICE_LOG_LEVEL=info
```

### Resource Limits
```yaml
# docker-compose.yml
deploy:
  resources:
    limits:
      memory: 4G
      cpus: '2.0'
    reservations:
      memory: 2G
      cpus: '1.0'
```

## Monitoring

### Health Checks
```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:16070/health"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 5m
```

### Log Management
```bash
# Log rotation
docker run --log-driver json-file \
  --log-opt max-size=10m \
  --log-opt max-file=3

# Log monitoring
docker logs -f --tail 100 llm-llama-service
```

## Scaling

### Vertical Scaling
- **Memory**: 8GB for larger models
- **CPU**: More cores for parallel processing
- **Storage**: SSD for faster model loading

### Horizontal Scaling
```yaml
# docker-compose.yml
deploy:
  replicas: 1
  # Note: Stateful service, scaling requires careful design
```

## Backup & Recovery

### Model Backup
```bash
# Backup model
cp models/phi-3-mini.q4.gguf /backup/

# Restore model
cp /backup/phi-3-mini.q4.gguf models/
```

### Configuration Backup
```bash
# Backup configuration
tar czf config-backup.tar.gz \
  docker-compose.yml \
  models/download.sh \
  src/config.h
```

## Security Hardening

### Container Security
```yaml
# docker-compose.yml
security_opt:
  - no-new-privileges:true
read_only: true
tmpfs:
  - /tmp
```

### Network Security
```yaml
# Only expose necessary ports
ports:
  - "127.0.0.1:16070:16070"  # Local only
  - "127.0.0.1:16071:16071"  # Local only
```

## Maintenance

### Updates
```bash
# Service update
git pull origin main
docker compose down
docker compose up --build -d

# Model update
rm models/phi-3-mini.q4.gguf
./models/download.sh
docker compose restart
```

### Cleanup
```bash
# Clean Docker
docker system prune -f
docker volume prune -f

# Log cleanup
find /var/lib/docker/containers -name "*.log" -mtime +7 -delete
```

## Troubleshooting

### Common Issues
- **Port conflicts**: Change ports in docker-compose.yml
- **Memory issues**: Increase resource limits
- **Model corruption**: Re-download model

### Recovery Script
```bash
#!/bin/bash
# recovery.sh
docker compose down
docker system prune -f
./models/download.sh
docker compose up -d
echo "Recovery completed"
```

---