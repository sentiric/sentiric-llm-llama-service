# ğŸš€ Deployment Rehberi

Bu servis, GitHub Actions aracÄ±lÄ±ÄŸÄ±yla otomatik olarak `ghcr.io/sentiric/sentiric-llm-llama-service` adresine Docker imajÄ± olarak yayÄ±nlanÄ±r. Ãœretim ortamÄ±nda daÄŸÄ±tÄ±m yapmanÄ±n en iyi yolu bu Ã¶nceden oluÅŸturulmuÅŸ imajÄ± kullanmaktÄ±r.

## Sistem Gereksinimleri
-   **Docker**: 20.10+
-   **Docker Compose**: 2.0+ (opsiyonel, `docker run` da kullanÄ±labilir)
-   **RAM**: 4GB minimum
-   **Depolama**: Model dosyasÄ± iÃ§in ~3GB boÅŸ alan

## Ãœretim DaÄŸÄ±tÄ±mÄ± (Production Deployment)

1.  **`docker-compose.yml` DosyasÄ±nÄ± HazÄ±rlayÄ±n:**
    Projenin kÃ¶k dizinindeki `docker-compose.yml` dosyasÄ±, Ã¼retim daÄŸÄ±tÄ±mÄ± iÃ§in tasarlanmÄ±ÅŸtÄ±r. Bu dosyayÄ± sunucunuza kopyalayÄ±n.

2.  **Ortam DeÄŸiÅŸkenlerini AyarlayÄ±n (Opsiyonel):**
    Gerekirse, `docker-compose.yml` dosyasÄ±nÄ±n yanÄ±na bir `.env` dosyasÄ± oluÅŸturarak veya doÄŸrudan sistem ortam deÄŸiÅŸkenlerini ayarlayarak konfigÃ¼rasyonu Ã¶zelleÅŸtirin. (TÃ¼m deÄŸiÅŸkenler iÃ§in `Configuration` bÃ¶lÃ¼mÃ¼ne bakÄ±n.)

3.  **Servisi BaÅŸlatÄ±n:**
    `docker-compose.yml`'nin bulunduÄŸu dizinde aÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rÄ±n.

    ```bash
    # En gÃ¼ncel imajÄ± Ã§ek ve servisi baÅŸlat
    docker compose up -d
    ```
    Bu komut, `build` yapmaz, bunun yerine GHCR'den `:latest` etiketli imajÄ± Ã§eker.

4.  **DoÄŸrulama:**
    Servisin baÅŸlamasÄ±, modelin indirilmesi nedeniyle birkaÃ§ dakika sÃ¼rebilir.

    ```bash
    # Konteyner durumunu kontrol et
    docker compose ps

    # Servis hazÄ±r olduÄŸunda health check yap
    curl http://localhost:16070/health
    ```

## GeliÅŸtirme OrtamÄ± (Development)

GeliÅŸtirme yaparken kaynak kodundan build yapmak iÃ§in, projenin iÃ§indeki `docker-compose.override.yml` dosyasÄ± otomatik olarak kullanÄ±lÄ±r.

```bash
# GeliÅŸtirme ortamÄ±nda, yerel kaynak kodunu kullanarak build et ve baÅŸlat
docker compose up --build -d
```

## Configuration

### Environment Variables

Servisi yapÄ±landÄ±rmak iÃ§in aÅŸaÄŸÄ±daki ortam deÄŸiÅŸkenlerini kullanÄ±n. TÃ¼m deÄŸiÅŸkenler `LLM_LLAMA_SERVICE_` Ã¶neki ile baÅŸlar.

| DeÄŸiÅŸken                                   | AÃ§Ä±klama                                                                          | VarsayÄ±lan DeÄŸer                  |
| ------------------------------------------ | --------------------------------------------------------------------------------- | --------------------------------- |
| **Network**                                |                                                                                   |                                   |
| `LLM_LLAMA_SERVICE_IPV4_ADDRESS`           | Servisin dinleyeceÄŸi IP adresi. `0.0.0.0` tÃ¼m arayÃ¼zleri dinler.                  | `0.0.0.0`                         |
| `LLM_LLAMA_SERVICE_HTTP_PORT`              | HTTP health check sunucusunun portu.                                              | `16070`                           |
| `LLM_LLAMA_SERVICE_GRPC_PORT`              | gRPC sunucusunun portu.                                                           | `16071`                           |
| **Model Management**                       |                                                                                   |                                   |
| `LLM_LLAMA_SERVICE_MODEL_DIR`              | Modellerin indirileceÄŸi ve saklanacaÄŸÄ± konteyner iÃ§indeki dizin.                  | `/models`                         |
| `LLM_LLAMA_SERVICE_MODEL_ID`               | Hugging Face repo ID'si (Ã¶r: `microsoft/Phi-3-mini-4k-instruct-gguf`).          | `microsoft/Phi-3-mini-4k-instruct-gguf` |
| `LLM_LLAMA_SERVICE_MODEL_FILENAME`         | Ä°ndirilecek GGUF dosyasÄ±nÄ±n tam adÄ±.                                              | `Phi-3-mini-4k-instruct-q4.gguf`  |
| `LLM_LLAMA_SERVICE_MODEL_PATH`             | *[Legacy]* `MODEL_ID` belirtilmezse kullanÄ±lacak modelin tam yolu.                  | `""`                              |
| **Engine & Performance**                   |                                                                                   |                                   |
| `LLM_LLAMA_SERVICE_GPU_LAYERS`             | GPU'ya yÃ¼klenecek model katmanÄ± sayÄ±sÄ±. `-1` tÃ¼m katmanlarÄ± yÃ¼kler.               | `0`                               |
| `LLM_LLAMA_SERVICE_CONTEXT_SIZE`           | Modelin maksimum context penceresi.                                               | `4096`                            |
| `LLM_LLAMA_SERVICE_THREADS`                | Token Ã¼retimi iÃ§in kullanÄ±lacak CPU thread sayÄ±sÄ±.                                | (DonanÄ±mÄ±n yarÄ±sÄ±, max 8)         |
| **Logging**                                |                                                                                   |                                   |
| `LLM_LLAMA_SERVICE_LOG_LEVEL`              | Log seviyesi (`trace`, `debug`, `info`, `warn`, `error`, `critical`).           | `info`                            |
| **Default Sampling**                       | *gRPC isteÄŸinde belirtilmezse kullanÄ±lacak varsayÄ±lan deÄŸerler.*                    |                                   |
| `LLM_LLAMA_SERVICE_DEFAULT_MAX_TOKENS`     | VarsayÄ±lan maksimum Ã¼retilecek token sayÄ±sÄ±.                                      | `1024`                            |
| `LLM_LLAMA_SERVICE_DEFAULT_TEMPERATURE`    | VarsayÄ±lan sampling sÄ±caklÄ±ÄŸÄ±. YaratÄ±cÄ±lÄ±ÄŸÄ± artÄ±rÄ±r.                               | `0.8`                             |
| `LLM_LLAMA_SERVICE_DEFAULT_TOP_K`          | VarsayÄ±lan Top-K sampling deÄŸeri.                                                 | `40`                              |
| `LLM_LLAMA_SERVICE_DEFAULT_TOP_P`          | VarsayÄ±lan Top-P (nucleus) sampling deÄŸeri.                                       | `0.95`                            |
| `LLM_LLAMA_SERVICE_DEFAULT_REPEAT_PENALTY` | VarsayÄ±lan tekrar cezasÄ±. Tekrar eden metinleri engeller.                           | `1.1`                             |


**Ã–rnek `docker-compose.yml` YapÄ±landÄ±rmasÄ±:**

```yaml
# docker-compose.yml
services:
  llm-llama-service:
    # ...
    environment:
      - LLM_LLAMA_SERVICE_IPV4_ADDRESS=0.0.0.0
      - LLM_LLAMA_SERVICE_HTTP_PORT=16070
      - LLM_LLAMA_SERVICE_GRPC_PORT=16071
      - LLM_LLAMA_SERVICE_THREADS=4
      - LLM_LLAMA_SERVICE_LOG_LEVEL=debug
```

### Resource Limits
```yaml
# docker-compose.yml
deploy:
  resources:
    limits:
      memory: 4G
      cpus: '2.0'
    reservations:
      memory: 2G
      cpus: '1.0'
```

## Monitoring

### Health Checks
```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:16070/health"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 5m
```

### Log Management
```bash
# Log rotation
docker run --log-driver json-file \
  --log-opt max-size=10m \
  --log-opt max-file=3

# Log monitoring
docker logs -f --tail 100 llm-llama-service
```

## Scaling

### Vertical Scaling
- **Memory**: 8GB for larger models
- **CPU**: More cores for parallel processing
- **Storage**: SSD for faster model loading

### Horizontal Scaling
```yaml
# docker-compose.yml
deploy:
  replicas: 1
  # Note: Stateful service, scaling requires careful design
```

## Backup & Recovery

### Model Backup
```bash
# Backup model
cp models/phi-3-mini.q4.gguf /backup/

# Restore model
cp /backup/phi-3-mini.q4.gguf models/
```

### Configuration Backup
```bash
# Backup configuration
tar czf config-backup.tar.gz \
  docker-compose.yml \
  models/download.sh \
  src/config.h
```

## Security Hardening

### Container Security
```yaml
# docker-compose.yml
security_opt:
  - no-new-privileges:true
read_only: true
tmpfs:
  - /tmp
```

### Network Security
```yaml
# Only expose necessary ports
ports:
  - "127.0.0.1:16070:16070"  # Local only
  - "127.0.0.1:16071:16071"  # Local only
```

## Maintenance

### Updates
```bash
# Service update
git pull origin main
docker compose down
docker compose up --build -d

# Model update
rm models/phi-3-mini.q4.gguf
./models/download.sh
docker compose restart
```

### Cleanup
```bash
# Clean Docker
docker system prune -f
docker volume prune -f

# Log cleanup
find /var/lib/docker/containers -name "*.log" -mtime +7 -delete
```

## Troubleshooting

### Common Issues
- **Port conflicts**: Change ports in docker-compose.yml
- **Memory issues**: Increase resource limits
- **Model corruption**: Re-download model

### Recovery Script
```bash
#!/bin/bash
# recovery.sh
docker compose down
docker system prune -f
./models/download.sh
docker compose up -d
echo "Recovery completed"
```

---