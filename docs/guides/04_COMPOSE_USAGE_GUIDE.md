# ğŸ³ Docker Compose KullanÄ±m Rehberi

## ğŸ“‹ Genel BakÄ±ÅŸ

Bu proje, farklÄ± kullanÄ±m senaryolarÄ± iÃ§in optimize edilmiÅŸ multiple Docker Compose dosyalarÄ± kullanÄ±r:

### ğŸ—ï¸ Compose Dosya Mimarisi
- **`docker-compose.yml`** - Temel yapÄ±landÄ±rma (asla tek baÅŸÄ±na kullanÄ±lmaz)
- **`docker-compose.cpu.yml`** - Production CPU profili
- **`docker-compose.gpu.yml`** - Production GPU profili  
- **`docker-compose.override.yml`** - Local development (CPU)
- **`docker-compose.gpu.override.yml`** - Local development (GPU)
- **`docker-compose.run.gpu.yml`** - CLI iÃ§in GPU konteyneri

---

## ğŸš€ KULLANIM SENARYOLARI

### 1. ğŸ–¥ï¸ PRODUCTION - CPU DaÄŸÄ±tÄ±mÄ±

**AmaÃ§:** Pre-built imaj ile production CPU ortamÄ±

```bash
# GitHub Container Registry'den imajÄ± Ã§ek ve Ã§alÄ±ÅŸtÄ±r
docker compose -f docker-compose.yml -f docker-compose.cpu.yml up -d

# Servisi durdur
docker compose -f docker-compose.yml -f docker-compose.cpu.yml down
```

**Ã–zellikler:**
- âœ… Pre-built imaj kullanÄ±r (`ghcr.io/sentiric/sentiric-llm-llama-service:latest`)
- âœ… Optimize edilmiÅŸ CPU ayarlarÄ±
- âœ… Production-ready health checks
- âœ… mTLS gÃ¼venlik aktif

---

### 2. ğŸ® PRODUCTION - GPU DaÄŸÄ±tÄ±mÄ±

**AmaÃ§:** Pre-built imaj ile production GPU ortamÄ±

```bash
# GPU imajÄ±nÄ± Ã§ek ve Ã§alÄ±ÅŸtÄ±r
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d

# Servisi durdur
docker compose -f docker-compose.yml -f docker-compose.gpu.yml down
```

**GPU AyarlarÄ±:**
```yaml
- LLM_LLAMA_SERVICE_GPU_LAYERS=28
- NVIDIA_VISIBLE_DEVICES=all
```

**Ã–zellikler:**
- âœ… Pre-built GPU imajÄ±
- âœ… NVIDIA GPU desteÄŸi
- âœ… Optimize edilmiÅŸ 6GB VRAM ayarlarÄ±
- âœ… Otomatik GPU resource allocation

---

### 3. ğŸ”§ LOCAL DEVELOPMENT - CPU GeliÅŸtirme

**AmaÃ§:** Yerel kod deÄŸiÅŸiklikleriyle development

```bash
# Kaynaktan build et ve Ã§alÄ±ÅŸtÄ±r (override otomatik kullanÄ±lÄ±r)
docker compose up --build -d

# LoglarÄ± izle
docker compose logs -f llm-llama-service

# Servisi durdur
docker compose down
```

**Ã–zellikler:**
- âœ… Yerel kod deÄŸiÅŸiklikleriyle otomatik rebuild
- âœ… `docker-compose.override.yml` otomatik kullanÄ±lÄ±r
- âœ… GeliÅŸtirme iÃ§in optimize edilmiÅŸ ayarlar
- âœ… CLI konteyneri dahil

---

### 4. ğŸ¯ LOCAL DEVELOPMENT - GPU GeliÅŸtirme

**AmaÃ§:** Yerel GPU ile development ve test

```bash
# GPU iÃ§in build et ve Ã§alÄ±ÅŸtÄ±r
docker compose -f docker-compose.yml -f docker-compose.gpu.yml -f docker-compose.gpu.override.yml up --build -d

# LoglarÄ± izle
docker compose -f docker-compose.yml -f docker-compose.gpu.yml -f docker-compose.gpu.override.yml logs -f

# Servisi durdur
docker compose -f docker-compose.yml -f docker-compose.gpu.yml -f docker-compose.gpu.override.yml down
```

**Optimize GPU AyarlarÄ± (6GB VRAM):**
```yaml
- LLM_LLAMA_SERVICE_GPU_LAYERS=28
- LLM_LLAMA_SERVICE_CONTEXT_SIZE=1024
- LLM_LLAMA_SERVICE_THREADS=1
- LLM_LLAMA_SERVICE_THREADS_BATCH=1
- LLM_LLAMA_SERVICE_ENABLE_BATCHING=false
- LLM_LLAMA_SERVICE_ENABLE_WARM_UP=true
```

---

### 5. ğŸ› ï¸ CLI ARAÃ‡LARI - GPU OrtamÄ±nda

**AmaÃ§:** GPU destekli CLI komutlarÄ±nÄ± Ã§alÄ±ÅŸtÄ±rma

```bash
# Health check
docker compose -f docker-compose.run.gpu.yml run --rm llm-cli llm_cli health

# Metin Ã¼retme
docker compose -f docker-compose.run.gpu.yml run --rm llm-cli llm_cli generate "Merhaba dÃ¼nya"

# RAG testi (run_request.sh alternatifi)
docker compose -f docker-compose.run.gpu.yml run --rm llm-cli llm_cli generate "Soru" --rag-context "Context metni"
```

**Ã–zellikler:**
- âœ… GeÃ§ici konteyner (--rm)
- âœ… GPU eriÅŸimi
- âœ… mTLS sertifikalarÄ±
- âœ… Servis aÄŸÄ±na baÄŸlÄ±

---

### 6. ğŸ§ª TEST VE DEBUG SENARYOLARI

#### A. HÄ±zlÄ± Test
```bash
# Servis saÄŸlÄ±klÄ± mÄ±?
curl http://localhost:16070/health

# Metrikleri kontrol et
curl http://localhost:16072/metrics

# Web UI'yi aÃ§
http://localhost:16070
```

#### B. GeliÅŸmiÅŸ Test
```bash
# Paralel istek testi
./run_request.sh examples/health_service_context.txt "Test 1" &
./run_request.sh examples/legal_service_context.txt "Test 2" &

# Warm-up kontrolÃ¼
docker compose logs llm-llama-service | grep -E "(Warming up|warm-up completed)"

# Batching kontrolÃ¼
docker compose logs llm-llama-service | grep -E "(DynamicBatcher|Processing batch)"
```

#### C. Performans Testi
```bash
# Benchmark Ã§alÄ±ÅŸtÄ±r
docker compose -f docker-compose.run.gpu.yml run --rm llm-cli llm_cli benchmark --iterations 5

# DetaylÄ± sistem durumu
docker compose -f docker-compose.run.gpu.yml run --rm llm-cli llm_cli health
```

---

## âš™ï¸ ORTAM DEÄÄ°ÅKENLERÄ° REFERANSI

### Temel Ayarlar
```bash
# Model
LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-1b-it-qat-GGUF
LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-1b-it-qat-Q4_0.gguf

# Performans
LLM_LLAMA_SERVICE_THREADS=1
LLM_LLAMA_SERVICE_CONTEXT_SIZE=1024
LLM_LLAMA_SERVICE_GPU_LAYERS=0  # CPU: 0, GPU: 28
```

### Yeni GeliÅŸmiÅŸ Ã–zellikler
```bash
# Warm-up (Ã–nerilen: true)
LLM_LLAMA_SERVICE_ENABLE_WARM_UP=true

# Dynamic Batching (Sadece THREADS > 1 iÃ§in)
LLM_LLAMA_SERVICE_ENABLE_BATCHING=false
LLM_LLAMA_SERVICE_MAX_BATCH_SIZE=1
LLM_LLAMA_SERVICE_BATCH_TIMEOUT_MS=10
```

### GÃ¼venlik
```bash
# mTLS sertifikalarÄ±
GRPC_TLS_CA_PATH=../sentiric-certificates/certs/ca.crt
LLM_LLAMA_SERVICE_CERT_PATH=../sentiric-certificates/certs/llm-llama-service-chain.crt
LLM_LLAMA_SERVICE_KEY_PATH=../sentiric-certificates/certs/llm-llama-service.key
```

---

## ğŸš¨ SIK KARÅILAÅILAN SORUNLAR

### 1. "orphan containers" UyarÄ±sÄ±
```bash
# Ã‡Ã¶zÃ¼m: --remove-orphans kullan
docker compose down --remove-orphans
```

### 2. GPU Bellek HatasÄ±
```bash
# Ã‡Ã¶zÃ¼m: GPU layers azalt
LLM_LLAMA_SERVICE_GPU_LAYERS=24
LLM_LLAMA_SERVICE_THREADS=1
```

### 3. mTLS BaÄŸlantÄ± HatasÄ±
```bash
# Ã‡Ã¶zÃ¼m: Sertifika yollarÄ±nÄ± kontrol et
ls -la ../sentiric-certificates/certs/
```

### 4. Model Ä°ndirme HatasÄ±
```bash
# Ã‡Ã¶zÃ¼m: Modeli manuel indir
wget -O models/gemma-3-1b-it-qat-Q4_0.gguf \
  "https://huggingface.co/ggml-org/gemma-3-1b-it-qat-GGUF/resolve/main/gemma-3-1b-it-qat-Q4_0.gguf"
```

---

## ğŸ“Š PERFORMANS OPTÄ°MÄ°ZASYONLARI

### 6GB GPU iÃ§in Optimal Ayarlar
```yaml
LLM_LLAMA_SERVICE_GPU_LAYERS: 28
LLM_LLAMA_SERVICE_CONTEXT_SIZE: 1024  
LLM_LLAMA_SERVICE_THREADS: 1
LLM_LLAMA_SERVICE_ENABLE_WARM_UP: true
LLM_LLAMA_SERVICE_ENABLE_BATCHING: false
```

### 8GB+ GPU iÃ§in GeliÅŸmiÅŸ Ayarlar
```yaml
LLM_LLAMA_SERVICE_GPU_LAYERS: 32
LLM_LLAMA_SERVICE_CONTEXT_SIZE: 2048
LLM_LLAMA_SERVICE_THREADS: 2
LLM_LLAMA_SERVICE_ENABLE_BATCHING: true
LLM_LLAMA_SERVICE_MAX_BATCH_SIZE: 2
```

---

## ğŸ”„ HIZLI REFERANS TABLOSU

| Senaryo | Komut | Build | GPU | Use Case |
|---------|--------|--------|-----|----------|
| Production CPU | `-f docker-compose.yml -f docker-compose.cpu.yml` | âŒ | âŒ | Production |
| Production GPU | `-f docker-compose.yml -f docker-compose.gpu.yml` | âŒ | âœ… | Production |
| Dev CPU | `docker compose up --build -d` | âœ… | âŒ | GeliÅŸtirme |
| Dev GPU | `-f docker-compose.yml -f docker-compose.gpu.yml -f docker-compose.gpu.override.yml` | âœ… | âœ… | GeliÅŸtirme |
| CLI GPU | `-f docker-compose.run.gpu.yml run --rm llm-cli` | âŒ | âœ… | Test |

---

## ğŸ¯ EN Ä°YÄ° UYGULAMALAR

1. **Development'da** her zaman `--build` kullan
2. **Production'da** pre-built imajlarÄ± kullan  
3. **GPU iÃ§in** memory limit'leri kontrol et
4. **DeÄŸiÅŸiklik sonrasÄ±** her zaman `docker compose down` ve yeniden baÅŸlat
5. **LoglarÄ±** her zaman monitor et: `docker compose logs -f`

---
