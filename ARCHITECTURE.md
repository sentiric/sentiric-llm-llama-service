# ğŸ—ï¸ Teknik Mimari

## YENÄ°: Static Build Architecture

### Build Time Dependencies
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Host System   â”‚
â”‚  - git          â”‚
â”‚  - cmake        â”‚  
â”‚  - docker       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Builder       â”‚
â”‚  - vcpkg        â”‚
â”‚  - llama.cpp    â”‚
â”‚  - protobuf     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Runtime       â”‚
â”‚  - libgomp1     â”‚ â† TEK RUNTIME DEPENDENCY
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Optimization
- **Model Weights**: MMAP ile lazy loading (~2.23GB)
- **KV Cache**: 1536MB sabit allocation  
- **Context**: 4096 token capacity
- **Total**: ~2.5GB optimizasyonu

## Sistem DiagramÄ±
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    GRPC    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GRPC Client   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  LLM Service     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Streaming â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP    â”‚  - LLM Engine    â”‚
â”‚   HTTP Health   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  - GRPC Server   â”‚
â”‚     Checker     â”‚            â”‚  - HTTP Server   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                               â”‚ llama.cpp   â”‚
                               â”‚   Library   â”‚
                               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                               â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                               â”‚ Phi-3-mini  â”‚
                               â”‚   Model     â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## BileÅŸenler

### 1. LLM Engine (`src/llm_engine.cpp`)
- **Model YÃ¼kleme**: `llama_model_load_from_file()`
- **Context Management**: `llama_init_from_model()`
- **Token Generation**: Greedy sampling + streaming
- **Thread Safety**: `std::mutex` ile thread-safe

### 2. GRPC Server (`src/grpc_server.cpp`)
- **Protocol**: sentiric-contracts v1.10.0
- **Streaming**: Real-time token delivery
- **Error Handling**: Graceful client disconnect

### 3. HTTP Server (`src/http_server.cpp`)
- **Health Check**: Model status monitoring
- **REST API**: JSON responses
- **Port**: 16060

## Build SÃ¼reci

### Static Build ZorunluluklarÄ±
```cmake
set(LLAMA_STATIC ON)
set(BUILD_SHARED_LIBS OFF)
target_link_libraries(llm_service PRIVATE llama)
```

### Docker Multi-stage
1. **Builder Stage**: TÃ¼m baÄŸÄ±mlÄ±lÄ±klar + derleme
2. **Runtime Stage**: Sadece executable + libgomp1

## Data Flow

1. **GRPC Request** â†’ `LocalGenerateStreamRequest`
2. **Tokenization** â†’ `llama_tokenize()`
3. **Decoding** â†’ `llama_decode()`
4. **Sampling** â†’ Greedy selection
5. **Streaming** â†’ Token-by-token response

## GÃ¼venlik

- **No Network Access**: Tamamen yerel
- **Container Isolation**: Docker sandbox
- **Static Binary**: Minimal attack surface