# ğŸ—ï¸ Teknik Mimari

## 1. KatmanlÄ± BaÄŸÄ±mlÄ±lÄ±k Mimarisi

Sistem, derleme sÃ¼relerini optimize etmek ve baÄŸÄ±mlÄ±lÄ±klarÄ± modÃ¼lerleÅŸtirmek iÃ§in katmanlÄ± bir Docker imaj yapÄ±sÄ± kullanÄ±r.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ghcr.io/sentiric/vcpkg-base â”‚ (Build Tools, vcpkg)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ghcr.io/sentiric/llama-cpp  â”‚ (libllama.so, Headers)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sentiric-llm-llama-service  â”‚ (Application Logic)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. Sistem DiagramÄ±

Servis, bir model motoru ve iki sunucu arayÃ¼zÃ¼nden oluÅŸur. EÅŸzamanlÄ± istekler, bir `LlamaContextPool` tarafÄ±ndan yÃ¶netilir.

```
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”Œâ”€â”€â”€â–º   gRPC Request   â”‚
                                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    gRPC/HTTP   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Clients   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â–º   gRPC Request   â”‚
   â”‚ (llm_cli)   â”‚              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â””â”€â”€â”€â–º   gRPC Request   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚  LLM Service     â”‚
                                     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                                     â”‚ â”‚  gRPC Server â”‚ â”‚
                                     â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
                                     â”‚ â”‚  HTTP Server â”‚ â”‚
                                     â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
                                     â”‚ â”‚  LLM Engine  â”‚ â”‚
                                     â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚ LlamaContextPool â”‚
                                     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                                     â”‚ â”‚ llama_contextâ”‚ â”‚
                                     â”‚ â”‚ llama_contextâ”‚ â”‚
                                     â”‚ â”‚ ... (N adet) â”‚ â”‚
                                     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                       â”‚ libllama.so â”‚ (Shared Library)
                                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                       â”‚  Phi-3 Model  â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. EÅŸzamanlÄ±lÄ±k Modeli (Concurrency)

Ã–nceki mimarideki global `std::mutex` darboÄŸazÄ± giderilmiÅŸtir. Yeni mimari, bir **context havuzu (`LlamaContextPool`)** kullanÄ±r:

-   Servis baÅŸladÄ±ÄŸÄ±nda, `n_threads` sayÄ±sÄ± kadar `llama_context` oluÅŸturulur ve havuza eklenir.
-   Her gelen gRPC isteÄŸi, havuzdan boÅŸta bir `llama_context` talep eder.
-   Ä°stek, bu context'i kullanarak token Ã¼retme iÅŸlemini gerÃ§ekleÅŸtirir. Bu sÄ±rada diÄŸer istekler, havuzdaki diÄŸer boÅŸ context'leri kullanarak paralel olarak iÅŸlenebilir.
-   Ä°ÅŸlem bittiÄŸinde, context temizlenir (KV cache sÄ±fÄ±rlanÄ±r) ve tekrar havuza bÄ±rakÄ±lÄ±r.

Bu yapÄ±, servisin CPU kaynaklarÄ±nÄ± tam olarak kullanarak **gerÃ§ek eÅŸzamanlÄ±lÄ±k** saÄŸlar.

## 4. Build SÃ¼reci

-   **CMake:** `find_package` kullanarak baÄŸÄ±mlÄ±lÄ±klarÄ± (gRPC, llama, spdlog vb.) modern ve taÅŸÄ±nabilir bir ÅŸekilde bulur.
-   **FetchContent:** `sentiric-contracts` reposunu derleme anÄ±nda Ã§eker ve proto dosyalarÄ±nÄ± iÅŸler.
-   **Dockerfile:** Multi-stage build kullanÄ±r. `builder` aÅŸamasÄ±nda tÃ¼m derlemeler yapÄ±lÄ±r. `runtime` aÅŸamasÄ±na ise sadece Ã§alÄ±ÅŸtÄ±rÄ±labilir dosyalar ve gerekli paylaÅŸÄ±lan kÃ¼tÃ¼phaneler (`libllama.so`, `libgomp1.so`) kopyalanÄ±r.


---
