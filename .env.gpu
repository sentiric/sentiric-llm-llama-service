# ==============================================================================
# üåç GLOBAL AYARLAR
# ==============================================================================
ENV="development"
LOG_LEVEL="INFO"
ENV_FILE_PATH=.env

CONFIG_REPO_PATH=../sentiric-config
ASSETS_REPO_PATH=../sentiric-assets

# --- Security (mTLS) ---
CERTIFICATES_REPO_PATH=../sentiric-certificates
GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DISABLE_REQUIRE=true

# ==============================================================================
# üÜî PROJE Kƒ∞MLƒ∞ƒûƒ∞ VE Aƒû
# ==============================================================================
PROJECT_NAME="sentiric"
PROJECT_TLD="cloud"

# Network (Docker Compose i√ßin)
NETWORK_NAME="sentiric.cloud"
NETWORK_SUBNET=10.88.0.0/16
NETWORK_GATEWAY=10.88.0.1

# Discovery Service (DNS)
DISCOVERY_DATACENTER_NAME="${PROJECT_NAME}-main"
DISCOVERY_DOMAIN="${PROJECT_NAME}.${PROJECT_TLD}"
DISCOVERY_SERVICE_SUBDOMAIN="service"
DISCOVERY_DNS_SEARCH_DOMAIN="${DISCOVERY_SERVICE_SUBDOMAIN}.${DISCOVERY_DOMAIN}"
DISCOVERY_METHOD="DNS"
DISCOVERY_SERVICE_IPV4_ADDRESS=10.88.5.1

# ==============================================================================
# üèóÔ∏è ALTYAPI (INFRASTRUCTURE)
# ==============================================================================

# ==============================================================================
# üß† LLM SERVICE (Metin √úretim) - DENGELƒ∞ PROFƒ∞L
# ==============================================================================
LLM_LLAMA_SERVICE_HOST=llm-llama-service
LLM_LLAMA_SERVICE_IPV4_ADDRESS=10.88.60.7
LLM_LLAMA_SERVICE_HTTP_PORT=16070
LLM_LLAMA_SERVICE_GRPC_PORT=16071
LLM_LLAMA_SERVICE_METRICS_PORT=16072
LLM_LLAMA_SERVICE_LOG_LEVEL=info
LLM_LLAMA_SERVICE_LISTEN_ADDRESS=0.0.0.0

# --- Sistem Ayarlarƒ± ---
LLM_LLAMA_SERVICE_LOG_LEVEL=info

# --- Model Se√ßimi
LLM_LLAMA_SERVICE_MODEL_DIR=/models

# [01] Product Ready [ 6GB VRAM]
LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-1b-it-GGUF
LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-1b-it-Q8_0.gguf

# [02] Development [ 12GB VRAM ]
# [2025-12-09 01:14:50.665] [error] [llama.cpp] ggml_backend_cuda_buffer_type_alloc_buffer: allocating 4640.00 MiB on device 0: cudaMalloc failed: out of memory
# [2025-12-09 01:14:50.665] [error] [llama.cpp] alloc_tensor_range: failed to allocate CUDA0 buffer of size 4865392640
# [2025-12-09 01:14:50.668] [error] [llama.cpp] llama_init_from_model: failed to initialize the context: failed to allocate buffer for kv cache
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-4b-it-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-4b-it-Q8_0.gguf

# [03] Development [ 6GB VRAM] Alternatif 
# Not ready: [error] [llama.cpp] llama_model_load: error loading model: error loading model hyperparameters: key not found in model: llama.context_length
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/LoRA-Llama-3-Instruct-abliteration-8B-F16-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=Llama-3-Instruct-abliteration-LoRA-8B-f16.gguf

# [04] Development [ 6GB VRAM]
# Not ready : Response payload is not completed: <TransferEncodingError: 400, message='Not enough data to satisfy transfer length header.'>
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/Qwen3-1.7B-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=Qwen3-1.7B-Q8_0.gguf

# [05] Development [ 12GB VRAM] Alternatif 
# Not ready : error] [llama.cpp] llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'mistral3'
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/Ministral-3-3B-Instruct-2512-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=Ministral-3-3B-Instruct-2512-Q8_0.gguf


# --- GPU Ayarlarƒ± ---
# Modelin tamamƒ±nƒ± GPU'ya y√ºkle. Bu model VRAM'de yer kaplar.
LLM_LLAMA_SERVICE_GPU_LAYERS=100
# 
LLM_LLAMA_SERVICE_CONTEXT_SIZE=32000
# ---
LLM_LLAMA_SERVICE_KV_OFFLOAD=true
LLM_LLAMA_SERVICE_THREADS=1
LLM_LLAMA_SERVICE_THREADS_BATCH=1
LLM_LLAMA_SERVICE_USE_MMAP=false
LLM_LLAMA_SERVICE_NUMA=disabled

# --- Advanced √ñzellikler ---
LLM_LLAMA_SERVICE_ENABLE_BATCHING=false
LLM_LLAMA_SERVICE_MAX_BATCH_SIZE=1
LLM_LLAMA_SERVICE_BATCH_TIMEOUT_MS=10
LLM_LLAMA_SERVICE_ENABLE_WARM_UP=false

# --- VoIP ƒ∞√ßin Kritik Sampling Ayarlarƒ± ---
LLM_LLAMA_SERVICE_DEFAULT_MAX_TOKENS=${LLM_LLAMA_SERVICE_CONTEXT_SIZE}
LLM_LLAMA_SERVICE_DEFAULT_TEMPERATURE=0.5
LLM_LLAMA_SERVICE_DEFAULT_TOP_K=40
LLM_LLAMA_SERVICE_DEFAULT_TOP_P=0.9
LLM_LLAMA_SERVICE_DEFAULT_REPEAT_PENALTY=1.15

# --- G√ºvenlik ---
LLM_LLAMA_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-llama-service-chain.crt
LLM_LLAMA_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-llama-service.key
