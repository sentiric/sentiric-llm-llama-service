# ==============================================================================
# üåç GLOBAL AYARLAR
# ==============================================================================
ENV="development"
LOG_LEVEL="INFO"
ENV_FILE_PATH=.env

CONFIG_REPO_PATH=../sentiric-config
ASSETS_REPO_PATH=../sentiric-assets

# --- Security (mTLS) ---
CERTIFICATES_REPO_PATH=../sentiric-certificates
GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DISABLE_REQUIRE=true

# ==============================================================================
# üÜî PROJE Kƒ∞MLƒ∞ƒûƒ∞ VE Aƒû
# ==============================================================================
PROJECT_NAME="sentiric"
PROJECT_TLD="cloud"

# Network (Docker Compose i√ßin)
NETWORK_NAME="sentiric.cloud"
NETWORK_SUBNET=10.88.0.0/16
NETWORK_GATEWAY=10.88.0.1

# Discovery Service (DNS)
DISCOVERY_DATACENTER_NAME="${PROJECT_NAME}-main"
DISCOVERY_DOMAIN="${PROJECT_NAME}.${PROJECT_TLD}"
DISCOVERY_SERVICE_SUBDOMAIN="service"
DISCOVERY_DNS_SEARCH_DOMAIN="${DISCOVERY_SERVICE_SUBDOMAIN}.${DISCOVERY_DOMAIN}"
DISCOVERY_METHOD="DNS"
DISCOVERY_SERVICE_IPV4_ADDRESS=10.88.5.1

# ==============================================================================
# üèóÔ∏è ALTYAPI (INFRASTRUCTURE)
# ==============================================================================

# ==============================================================================
# üß† LLM SERVICE (Metin √úretim) - DENGELƒ∞ PROFƒ∞L
# ==============================================================================
LLM_LLAMA_SERVICE_HOST=llm-llama-service
LLM_LLAMA_SERVICE_IPV4_ADDRESS=10.88.60.7
LLM_LLAMA_SERVICE_HTTP_PORT=16070
LLM_LLAMA_SERVICE_GRPC_PORT=16071
LLM_LLAMA_SERVICE_METRICS_PORT=16072
LLM_LLAMA_SERVICE_LISTEN_ADDRESS=0.0.0.0

# --- Sistem Ayarlarƒ± ---
LLM_LLAMA_SERVICE_LOG_LEVEL=info

# --- Model Dizin ---
LLM_LLAMA_SERVICE_MODEL_DIR=/models

# ------------------------------------------------------------------------------
# Model se√ßimleri STT + TTS + LLAMA + KNOWLEDGE Birlikte hesaplanmalƒ±.
# [MODEL SE√áƒ∞Mƒ∞ - Mƒ∞MARƒ∞ KARAR]
# 6GB VRAM i√ßin en iyi F/P: Llama-3.2-3B veya Phi-3.5-mini
# 12GB VRAM i√ßin en iyi F/P: Llama-3.1-8B-Instruct-Q5_K_M
# ------------------------------------------------------------------------------

# [SE√áENEK A] 6GB VRAM (Safe Mode - High Speed)
# Bu model yakla≈üƒ±k 2.2GB VRAM yer. 4k Context ile toplam ~3.5GB olur.
# terminate called after throwing an instance of 'nlohmann::json_abi_v3_12_0::detail::type_error'
#   what():  [json.exception.type_error.316] incomplete UTF-8 string; last byte: 0x98
# LLM_LLAMA_SERVICE_MODEL_ID=Bartowski/Llama-3.2-3B-Instruct-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=Llama-3.2-3B-Instruct-Q6_K.gguf

# [SE√áENEK B] 12GB VRAM (Performance Mode)
# Bu model yakla≈üƒ±k 5.5GB VRAM yer. 8k Context ile toplam ~8GB olur.
LLM_LLAMA_SERVICE_MODEL_ID=MaziyarPanahi/Llama-3-8B-Instruct-v0.1-GGUF
LLM_LLAMA_SERVICE_MODEL_FILENAME=Llama-3-8B-Instruct-v0.1.Q5_K_M.gguf

# [01] Product Ready [ 6GB VRAM]
LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-1b-it-GGUF
LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-1b-it-Q8_0.gguf

# [02] Development [ 12GB VRAM ]
# [2025-12-09 01:14:50.665] [error] [llama.cpp] ggml_backend_cuda_buffer_type_alloc_buffer: allocating 4640.00 MiB on device 0: cudaMalloc failed: out of memory
# [2025-12-09 01:14:50.665] [error] [llama.cpp] alloc_tensor_range: failed to allocate CUDA0 buffer of size 4865392640
# [2025-12-09 01:14:50.668] [error] [llama.cpp] llama_init_from_model: failed to initialize the context: failed to allocate buffer for kv cache
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-4b-it-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-4b-it-Q8_0.gguf

# [03] Development [ 6GB VRAM] Alternatif 
# Not ready: [error] [llama.cpp] llama_model_load: error loading model: error loading model hyperparameters: key not found in model: llama.context_length
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/LoRA-Llama-3-Instruct-abliteration-8B-F16-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=Llama-3-Instruct-abliteration-LoRA-8B-f16.gguf

# [04] Development [ 6GB VRAM]
# Not ready : Response payload is not completed: <TransferEncodingError: 400, message='Not enough data to satisfy transfer length header.'>
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/Qwen3-1.7B-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=Qwen3-1.7B-Q8_0.gguf

# [05] Development [ 12GB VRAM] Alternatif 
# Not ready : error] [llama.cpp] llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'mistral3'
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/Ministral-3-3B-Instruct-2512-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=Ministral-3-3B-Instruct-2512-Q8_0.gguf

# ------------------------------------------------------------------------------
# [SANS√úRS√úZ MODEL SE√áƒ∞Mƒ∞]
# FailSpy: Modellerin "Hayƒ±r yapamam" diyen n√∂ronlarƒ±nƒ± matematiksel olarak siler.
# ------------------------------------------------------------------------------

# [SE√áENEK 1] 6GB VRAM ƒ∞√áƒ∞N KRAL (√ñNERƒ∞LEN)
# Model: Llama-3-8B-Instruct-Abliterated-v3
# Format: Q4_K_M (Yakla≈üƒ±k 4.9 GB yer kaplar. 6GB karta tam sƒ±ƒüar.)
# √ñzellik: Llama 3'√ºn t√ºm zekasƒ±na sahip ama "etik kurallarƒ±" iptal edilmi≈ü.
# Not REady: Response payload is not completed: <TransferEncodingError: 400, message='Not enough data to satisfy transfer length header.'>
# Ynƒ±tlarƒ± iyi g√∂r√ºn√ºyor.
LLM_LLAMA_SERVICE_MODEL_ID=bartowski/Meta-Llama-3-8B-Instruct-abliterated-v3-GGUF
LLM_LLAMA_SERVICE_MODEL_FILENAME=Meta-Llama-3-8B-Instruct-abliterated-v3-Q4_K_M.gguf

# [SE√áENEK 2] 12GB VRAM VARSA (DAHA Y√úKSEK ZEKA)
# Model: Dolphin-2.9.4-Llama-3.1-8B
# Format: Q6_K (Yakla≈üƒ±k 6.6 GB yer kaplar + Context = ~8GB VRAM)
# √ñzellik: Eric Hartford'un efsanevi Dolphin serisi. Asla reddetmez.
# KOntrol edilmesi gerekiyor tam yanƒ±tlar alamadƒ±k
# LLM_LLAMA_SERVICE_MODEL_ID=bartowski/dolphin-2.9.4-llama3.1-8b-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=dolphin-2.9.4-llama3.1-8b-Q6_K.gguf

# --- GPU Ayarlarƒ± ---
# Model katmanlarƒ±nƒ±n GPU'ya y√ºklenme oranƒ±.
# OOM alƒ±rsan bunu 33 (Llama 8B i√ßin) veya 0 (CPU only) yapƒ±p dene.
LLM_LLAMA_SERVICE_GPU_LAYERS=100

# [KRƒ∞Tƒ∞K D√úZELTME] Context Size
# 32000 -> 6GB/12GB kartlarda KV Cache ta≈ümasƒ±na neden olur.
# 4096 = Standart Chat
# 8192 = RAG Dok√ºman Analizi (Maksimum √∂nerilen)
# LLM_LLAMA_SERVICE_CONTEXT_SIZE=8192

# [HASSAS AYAR] 6GB Kartta 8B Model √áalƒ±≈ütƒ±rmak ƒ∞√ßin
# Q4_K_M modeli 4.9GB yer. 6GB kartƒ±n var. Geriye 1.1GB kalƒ±r.
# Context'i 4096'da tutmak zorundayƒ±z, yoksa OOM yersin.
LLM_LLAMA_SERVICE_CONTEXT_SIZE=4096

# --- Performans Tuning ---
LLM_LLAMA_SERVICE_KV_OFFLOAD=true
LLM_LLAMA_SERVICE_THREADS=4
LLM_LLAMA_SERVICE_THREADS_BATCH=4
LLM_LLAMA_SERVICE_USE_MMAP=false
LLM_LLAMA_SERVICE_NUMA=disabled
# Flash Attention: VRAM kullanƒ±mƒ±nƒ± d√º≈ü√ºr√ºr, hƒ±zƒ± artƒ±rƒ±r (Destekleniyorsa true yap)
LLM_LLAMA_SERVICE_FLASH_ATTN=true 

# --- Batching ---
LLM_LLAMA_SERVICE_ENABLE_BATCHING=true
LLM_LLAMA_SERVICE_MAX_BATCH_SIZE=1
# D√º≈ü√ºk gecikme (latency) i√ßin timeout azaltƒ±ldƒ±
LLM_LLAMA_SERVICE_BATCH_TIMEOUT_MS=5
LLM_LLAMA_SERVICE_ENABLE_WARM_UP=true

# --- Sampling ---
LLM_LLAMA_SERVICE_DEFAULT_MAX_TOKENS=2048
LLM_LLAMA_SERVICE_DEFAULT_TEMPERATURE=0.6
LLM_LLAMA_SERVICE_DEFAULT_TOP_K=40
LLM_LLAMA_SERVICE_DEFAULT_TOP_P=0.9
LLM_LLAMA_SERVICE_DEFAULT_REPEAT_PENALTY=1.1

# --- G√ºvenlik ---
LLM_LLAMA_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-llama-service-chain.crt
LLM_LLAMA_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-llama-service.key

