#!/bin/bash
set -e

# ==============================================================================
# Sentiric LLM Service - GeliÅŸmiÅŸ Test ve Ã–rnek KullanÄ±m Script'i (v2.1)
# ==============================================================================
#
# Bu script, llm-llama-service'e zengin baÄŸlam (context) iÃ§eren RAG sorgularÄ±
# gÃ¶ndermeyi kolaylaÅŸtÄ±rÄ±r. Hem CPU hem de GPU ortamlarÄ±nÄ± destekler.
#
# KullanÄ±m:
#   ./run_request.sh [seÃ§enekler] "<context_dosyasÄ±_yolu>" "<sorgu>"
#
# SeÃ§enekler:
#   -c, --cpu : Testi CPU geliÅŸtirme ortamÄ±nda Ã§alÄ±ÅŸtÄ±rÄ±r. (VarsayÄ±lan: GPU)
#
# Ã–rnekler iÃ§in `examples/README.md` dosyasÄ±na bakÄ±n.
# ==============================================================================

# --- DeÄŸiÅŸkenleri ve VarsayÄ±lanlarÄ± Ayarla ---
DOCKER_CMD_BASE="docker compose"
# VarsayÄ±lan olarak GPU iÃ§in olan `run.gpu.yml` dosyasÄ±nÄ± kullan.
DOCKER_CMD_FLAGS="-f docker-compose.run.gpu.yml" 
TARGET_SERVICE="llm-cli"

# --- Komut SatÄ±rÄ± ArgÃ¼manlarÄ±nÄ± Ä°ÅŸle ---
if [[ "$1" == "-c" || "$1" == "--cpu" ]]; then
    # CPU modu seÃ§ilirse, hiÃ§bir ek -f bayraÄŸÄ±na gerek yok.
    # `docker compose run` komutu, `docker-compose.yml` ve `docker-compose.override.yml`
    # dosyalarÄ±nÄ± otomatik olarak kullanÄ±r.
    DOCKER_CMD_FLAGS=""
    shift # ArgÃ¼manlarÄ± sola kaydÄ±r
    echo "â„¹ï¸ CPU modu seÃ§ildi."
else
    echo "â„¹ï¸ GPU modu varsayÄ±lan olarak kullanÄ±lÄ±yor. CPU iÃ§in '-c' bayraÄŸÄ±nÄ± kullanÄ±n."
fi

if [ "$#" -ne 2 ]; then
    echo "âŒ HATA: Eksik argÃ¼man."
    echo "KullanÄ±m: $0 [-c|--cpu] <context_dosyasÄ±_yolu> <sorgu>"
    exit 1
fi

CONTEXT_FILE="$1"
QUERY="$2"

if [ ! -r "$CONTEXT_FILE" ]; then
    echo "âŒ HATA: Context dosyasÄ± okunamÄ±yor: $CONTEXT_FILE"
    exit 1
fi

CONTEXT_CONTENT=$(cat "$CONTEXT_FILE")

# --- Sistem Prompt'unu HazÄ±rla ---
SYSTEM_PROMPT=$(cat <<'EOF'
Sen, Sentiric platformunda Ã§alÄ±ÅŸan, yardÄ±msever ve profesyonel bir AI asistansÄ±n.
AÅŸaÄŸÄ±daki 'Ä°lgili Bilgiler' bÃ¶lÃ¼mÃ¼ndeki iÃ§eriÄŸi kullanarak kullanÄ±cÄ±nÄ±n sorusuna doÄŸal, akÄ±cÄ± ve en fazla 2 cÃ¼mleyle cevap ver.
Cevap yalnÄ±zca verilen baÄŸlama dayanmalÄ±; tahmin, uydurma veya baÄŸlam dÄ±ÅŸÄ± bilgi yok.
EÄŸer cevap baÄŸlamda yer almÄ±yorsa, bunu nazik ve doÄŸal bir ÅŸekilde belirt.

### Ä°lgili Bilgiler:
{context}

### KullanÄ±cÄ±nÄ±n Sorusu:
{query}

### Cevap:
EOF
)

# --- Testi Ã‡alÄ±ÅŸtÄ±r ---
echo ""
echo "ğŸ‘¤ KullanÄ±cÄ± Sorusu: ${QUERY}"
echo "----------------------------------------------------"

# Final komutunu birleÅŸtir ve Ã§alÄ±ÅŸtÄ±r.
# $DOCKER_CMD_FLAGS deÄŸiÅŸkeni boÅŸ olabilir (CPU durumu iÃ§in), bu bir sorun teÅŸkil etmez.
$DOCKER_CMD_BASE $DOCKER_CMD_FLAGS run --rm $TARGET_SERVICE \
    llm_cli generate "${QUERY}" \
    --system-prompt "${SYSTEM_PROMPT}" \
    --rag-context "${CONTEXT_CONTENT}" \
    --timeout 120