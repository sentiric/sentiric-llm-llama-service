services:
  llm-llama-service:
    build:
      context: .
      dockerfile: Dockerfile
    image: sentiric/llm-llama-service:local-cpu
    env_file: [".env.cpu"]    
    networks:
      - sentiric-net