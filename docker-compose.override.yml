services:
  llm-llama-service:
    build:
      context: ../sentiric-llm-llama-service
      dockerfile: Dockerfile.gpu
      