services:
  llm-llama-service:
    build:
      context: .
      dockerfile: Dockerfile
    image: sentiric/llm-llama-service:local-cpu 
    networks:
      - sentiric-net
    environment:

      # --- CPU TEMEL AYARLARI ---
      - LLM_LLAMA_SERVICE_GPU_LAYERS=0       # 0 = Sadece CPU
      - LLM_LLAMA_SERVICE_KV_OFFLOAD=false   # VRAM yok, RAM kullan
      - LLM_LLAMA_SERVICE_USE_MMAP=true      # RAM kullanımını dengeler

  # CLI ayarları aynı kalabilir
  llm-cli:
    build:
      context: .
      dockerfile: Dockerfile
    image: sentiric/llm-llama-service:local-cpu
    networks:
      - sentiric-net
    depends_on:
      - llm-llama-service
    volumes:
      - ../sentiric-certificates:/sentiric-certificates:ro
    environment:
      - GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
      - LLM_LLAMA_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-llama-service-chain.crt
      - LLM_LLAMA_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-llama-service.key