# ==============================================================================
# üåç GLOBAL AYARLAR
# ==============================================================================
ENV="development"
LOG_LEVEL="INFO"
ENV_FILE_PATH=.env

CONFIG_REPO_PATH=../sentiric-config
ASSETS_REPO_PATH=../sentiric-assets

# --- Security (mTLS) ---
CERTIFICATES_REPO_PATH=../sentiric-certificates
GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DISABLE_REQUIRE=true

# ==============================================================================
# üÜî PROJE Kƒ∞MLƒ∞ƒûƒ∞ VE Aƒû
# ==============================================================================
PROJECT_NAME="sentiric"
PROJECT_TLD="cloud"

# Network (Docker Compose i√ßin)
NETWORK_NAME="sentiric.cloud"
NETWORK_SUBNET=10.88.0.0/16
NETWORK_GATEWAY=10.88.0.1

# Discovery Service (DNS)
DISCOVERY_DATACENTER_NAME="${PROJECT_NAME}-main"
DISCOVERY_DOMAIN="${PROJECT_NAME}.${PROJECT_TLD}"
DISCOVERY_SERVICE_SUBDOMAIN="service"
DISCOVERY_DNS_SEARCH_DOMAIN="${DISCOVERY_SERVICE_SUBDOMAIN}.${DISCOVERY_DOMAIN}"
DISCOVERY_METHOD="DNS"
DISCOVERY_SERVICE_IPV4_ADDRESS=10.88.5.1

# ==============================================================================
# üèóÔ∏è ALTYAPI (INFRASTRUCTURE)
# ==============================================================================

# ==============================================================================
# üß† LLM SERVICE (Metin √úretim) - DENGELƒ∞ PROFƒ∞L
# ==============================================================================
LLM_LLAMA_SERVICE_HOST=llm-llama-service
LLM_LLAMA_SERVICE_IPV4_ADDRESS=10.88.60.7
LLM_LLAMA_SERVICE_HTTP_PORT=16070
LLM_LLAMA_SERVICE_GRPC_PORT=16071
LLM_LLAMA_SERVICE_METRICS_PORT=16072
LLM_LLAMA_SERVICE_LOG_LEVEL=info
LLM_LLAMA_SERVICE_LISTEN_ADDRESS=0.0.0.0

# --- Sistem Ayarlarƒ± ---
LLM_LLAMA_SERVICE_LOG_LEVEL=info

# --- Model Se√ßimi
LLM_LLAMA_SERVICE_MODEL_DIR=/models

# Model se√ßimi sƒ±rasƒ±nda, TTS + STT + LLM Birlikte hesaplanmalƒ±
# ---

# [01] Product Ready [ 12GB VRAM]
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-4b-it-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-4b-it-Q8_0.gguf

# [02] Product Ready [ 12GB VRAM] Alternatif 
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-4b-it-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-4b-it-f16.gguf

# [03] Product Ready [ 6GB VRAM]
# LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-1b-it-GGUF
# LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-1b-it-f16.gguf

# [02] Product Ready [ 6GB VRAM] Alternatif 
LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-1b-it-GGUF
LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-1b-it-Q8_0.gguf

# --- GPU Ayarlarƒ± ---
# Modelin tamamƒ±nƒ± GPU'ya y√ºkle. Bu model VRAM'de ~2.5GB yer kaplar.
LLM_LLAMA_SERVICE_GPU_LAYERS=100
# 
LLM_LLAMA_SERVICE_CONTEXT_SIZE=128000
# ---
LLM_LLAMA_SERVICE_KV_OFFLOAD=true
LLM_LLAMA_SERVICE_THREADS=1
LLM_LLAMA_SERVICE_THREADS_BATCH=1
LLM_LLAMA_SERVICE_USE_MMAP=false
LLM_LLAMA_SERVICE_NUMA=disabled

# --- Advanced √ñzellikler ---
LLM_LLAMA_SERVICE_ENABLE_BATCHING=false
LLM_LLAMA_SERVICE_MAX_BATCH_SIZE=1
LLM_LLAMA_SERVICE_BATCH_TIMEOUT_MS=10
LLM_LLAMA_SERVICE_ENABLE_WARM_UP=true

# --- VoIP ƒ∞√ßin Kritik Sampling Ayarlarƒ± ---
LLM_LLAMA_SERVICE_DEFAULT_MAX_TOKENS=${LLM_LLAMA_SERVICE_CONTEXT_SIZE}
LLM_LLAMA_SERVICE_DEFAULT_TEMPERATURE=0.5
LLM_LLAMA_SERVICE_DEFAULT_TOP_K=40
LLM_LLAMA_SERVICE_DEFAULT_TOP_P=0.9
LLM_LLAMA_SERVICE_DEFAULT_REPEAT_PENALTY=1.15

# --- G√ºvenlik ---
LLM_LLAMA_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-llama-service-chain.crt
LLM_LLAMA_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-llama-service.key
