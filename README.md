# ğŸ§  Sentiric LLM Llama Service

**Production-Ready**, yÃ¼ksek performanslÄ± C++ tabanlÄ± yerel LLM Ã§Ä±karÄ±m motoru. `llama.cpp` ile desteklenir, NVIDIA GPU'lar iÃ§in optimize edilmiÅŸtir ve yeni **Sentiric Omni-Studio** arayÃ¼zÃ¼ ile birlikte gelir.

[![CI - Build and Push Docker Image](https://github.com/sentiric/sentiric-llm-llama-service/actions/workflows/build-and-push.yml/badge.svg)](https://github.com/sentiric/sentiric-llm-llama-service/actions/workflows/build-and-push.yml)

## ğŸš€ Durum: ÃœRETÄ°M HAZIR

Bu servis, eÅŸzamanlÄ±lÄ±k, bellek kararlÄ±lÄ±ÄŸÄ± ve API uyumluluÄŸu iÃ§in titizlikle test edilmiÅŸtir. Temel ekosistem ortamlarÄ±nda daÄŸÄ±tÄ±m iÃ§in hazÄ±rdÄ±r.

-   âœ… **GerÃ§ek EÅŸzamanlÄ±lÄ±k**: `LlamaContextPool` aracÄ±lÄ±ÄŸÄ±yla birden fazla isteÄŸi aynÄ± anda iÅŸler
-   âœ… **Derin SaÄŸlÄ±k Kontrolleri**: YÃ¼k dengeleyiciler iÃ§in kapasite ve model durumu bilgisi sunar
-   âœ… **GÃ¶zlemlenebilirlik**: YapÄ±landÄ±rÄ±lmÄ±ÅŸ JSON loglarÄ± (Ã¼retim) ve Prometheus metrikleri
-   âœ… **GÃ¼venlik**: gRPC iletiÅŸimi iÃ§in tam mTLS desteÄŸi
-   âœ… **BaÄŸÄ±msÄ±z Mod**: GÃ¶mÃ¼lÃ¼ Studio kullanÄ±larak baÄŸÄ±msÄ±z olarak geliÅŸtirilebilir ve test edilebilir

---

## ğŸ“‹ Ä°Ã§indekiler

- [ğŸ“¸ Sentiric Omni-Studio](#-sentiric-omni-studio)
- [âœ¨ Temel Ã–zellikler](#-temel-Ã¶zellikler)
- [ğŸ—ï¸ Mimari](#ï¸-mimari)
- [ğŸ› ï¸ HÄ±zlÄ± BaÅŸlangÄ±Ã§ (BaÄŸÄ±msÄ±z)](#ï¸-hÄ±zlÄ±-baÅŸlangÄ±Ã§-baÄŸÄ±msÄ±z)
- [âš™ï¸ YapÄ±landÄ±rma](#ï¸-yapÄ±landÄ±rma)
- [ğŸ“Š Ä°zleme & Metrikler](#-iÌ‡zleme--metrikler)
- [ğŸ”§ GeliÅŸtirme](#-geliÅŸtirme)
- [ğŸš€ DaÄŸÄ±tÄ±m](#-daÄŸÄ±tÄ±m)
- [ğŸ“š DokÃ¼mantasyon](#-dokÃ¼mantasyon)
- [ğŸ¤ KatkÄ±da Bulunma](#-katkÄ±da-bulunma)
- [ğŸ“„ Lisans](#-lisans)

---

## ğŸ“¸ Sentiric Omni-Studio

Servis, `http://localhost:16070` adresinde eriÅŸilebilen gÃ¶mÃ¼lÃ¼ bir geliÅŸtirme stÃ¼dyosu iÃ§erir.

**Ã–zellikler:**
-   **Eller Serbest Mod (Hands-Free Mode):** Kesintisiz sesli konuÅŸma dÃ¶ngÃ¼sÃ¼
-   **RAG SÃ¼rÃ¼kle & BÄ±rak:** AnÄ±nda baÄŸlam ekleme
-   **Mobil Optimize:** DuyarlÄ± tasarÄ±m
-   **GÃ¶rsel Ses Analizi:** GerÃ§ek zamanlÄ± mikrofon gÃ¶rselleÅŸtirici
-   **Token-by-Token Streaming:** CanlÄ± yanÄ±t izleme

---

## âœ¨ Temel Ã–zellikler

### ğŸ¯ Ãœretim Ã–zellikleri

- **GerÃ§ek EÅŸzamanlÄ±lÄ±k**: Context pooling ile Ã§oklu paralel istek iÅŸleme
- **AkÄ±llÄ± BaÄŸlam YÃ¶netimi**: Otomatik context shifting ile sÄ±nÄ±rsÄ±z baÄŸlam desteÄŸi
- **GPU HÄ±zlandÄ±rma**: NVIDIA GPU'lar iÃ§in tam destek (CUDA)
- **Model Otomatik Ä°ndirme**: Hugging Face'den otomatik model indirme ve doÄŸrulama
- **Ã‡oklu Model Format DesteÄŸi**: GGUF formatÄ±ndaki tÃ¼m quantized modeller

### ğŸ”Œ API & Entegrasyon

- **OpenAI Uyumlu API**: Standart `/v1/chat/completions` endpoint'i
- **gRPC Streaming**: DÃ¼ÅŸÃ¼k gecikmeli, yÃ¼ksek performanslÄ± RPC
- **mTLS GÃ¼venlik**: Ä°steÄŸe baÄŸlÄ± karÅŸÄ±lÄ±klÄ± TLS kimlik doÄŸrulamasÄ±
- **Prometheus Metrikleri**: Ãœretim ortamÄ± izleme iÃ§in tam metrik desteÄŸi
- **SaÄŸlÄ±k Kontrolleri**: Kubernetes/Docker ready endpoints

### ğŸ§  GeliÅŸmiÅŸ Ã–zellikler

- **Otomatik Prompt FormatÄ± AlgÄ±lama**: Gemma, Llama 3, Qwen 2.5 desteÄŸi
- **RAG DesteÄŸi**: Built-in baÄŸlam injeksiyonu
- **JSON Modu**: Zorunlu JSON Ã§Ä±ktÄ±sÄ± iÃ§in grammar desteÄŸi
- **KonuÅŸma GeÃ§miÅŸi**: Ã‡oklu turlu diyalog yÃ¶netimi
- **Dinamik Batching**: Ä°steÄŸe baÄŸlÄ± batch processing (gelecek sÃ¼rÃ¼mlerde)

---

## ğŸ—ï¸ Mimari

```mermaid
graph TB
    subgraph Clients
        A[llm_cli]
        B[Python Client]
        C[llm-gateway]
        D[Web UI]
    end

    subgraph "LLM Service Container"
        direction LR
        subgraph "API Layer"
            HTTP[HTTP Server :16070]
            GRPC[gRPC Server :16071]
            METRICS[Metrics :16072]
        end

        ENGINE[LLM Engine]
        
        subgraph "Context Management"
            POOL[Context Pool]
            FORMATTER[Prompt Formatter]
        end

        HTTP --> ENGINE
        GRPC --> ENGINE
        ENGINE --> POOL
        ENGINE --> FORMATTER
    end

    subgraph "llama.cpp Backend"
        LLAMA[libllama.so]
        MODEL[(GGUF Model)]
    end
    
    Clients --> HTTP
    Clients --> GRPC
    POOL --> LLAMA
    LLAMA --> MODEL

    classDef client fill:#d4edda,stroke:#155724
    classDef service fill:#cce5ff,stroke:#004085
    classDef backend fill:#f8d7da,stroke:#721c24
    
    class A,B,C,D client
    class HTTP,GRPC,METRICS,ENGINE,POOL,FORMATTER service
    class LLAMA,MODEL backend
```

### Temel BileÅŸenler

1. **LLMEngine**: Ana Ã§Ä±karÄ±m motoru, tokenizasyon, sampling ve Ã¼retim yÃ¶netimi
2. **LlamaContextPool**: RAII tabanlÄ± context pooling ile eÅŸzamanlÄ±lÄ±k yÃ¶netimi
3. **PromptFormatter**: Model mimarisine gÃ¶re otomatik prompt formatlama
4. **ModelManager**: Otomatik model indirme ve doÄŸrulama

---

## ğŸ› ï¸ HÄ±zlÄ± BaÅŸlangÄ±Ã§ (BaÄŸÄ±msÄ±z)

### Ã–n Gereksinimler
-   Docker & Docker Compose
-   NVIDIA GPU (Ã–nerilen - isteÄŸe baÄŸlÄ±)
-   CUDA Toolkit 12.0+ (GPU kullanÄ±mÄ± iÃ§in)

### Ä°zole Ã‡alÄ±ÅŸtÄ±rma

```bash
# 1. Klonla
git clone https://github.com/sentiric/sentiric-llm-llama-service.git
cd sentiric-llm-llama-service

# 2. BaÅŸlat (Model ve sertifikalarÄ± otomatik indirir)
make up-gpu
# VEYA CPU iÃ§in:
# make up-cpu
```

**EriÅŸim NoktalarÄ±:**
-   **Studio UI:** `http://localhost:16070`
-   **Metrikler:** `http://localhost:16072/metrics`
-   **gRPC:** `localhost:16071`

### Ä°lk Ä°stek

```bash
# CLI ile test
docker compose -f docker-compose.run.gpu.yml run --rm llm-cli llm_cli generate "Merhaba!"

# HTTP API ile test
curl -X POST http://localhost:16070/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Merhaba!"}],
    "stream": false,
    "max_tokens": 100
  }'
```

---

## âš™ï¸ YapÄ±landÄ±rma

TÃ¼m yapÄ±landÄ±rma ortam deÄŸiÅŸkenleri ile yÃ¶netilir. DetaylÄ± yapÄ±landÄ±rma referansÄ± iÃ§in:
**[YapÄ±landÄ±rma Rehberi](./docs/guides/03_CONFIGURATION.md)**

### Temel Ortam DeÄŸiÅŸkenleri

```bash
# Model YapÄ±landÄ±rmasÄ±
LLM_LLAMA_SERVICE_MODEL_ID=ggml-org/gemma-3-1b-it-GGUF
LLM_LLAMA_SERVICE_MODEL_FILENAME=gemma-3-1b-it-Q8_0.gguf

# Performans
LLM_LLAMA_SERVICE_GPU_LAYERS=100        # GPU'ya yÃ¼klenecek katman sayÄ±sÄ±
LLM_LLAMA_SERVICE_CONTEXT_SIZE=4096     # Maksimum context penceresi
LLM_LLAMA_SERVICE_THREADS=4             # EÅŸzamanlÄ± context sayÄ±sÄ±

# Bellek Optimizasyonu
LLM_LLAMA_SERVICE_USE_MMAP=false        # RAM'e tam yÃ¼kleme
LLM_LLAMA_SERVICE_KV_OFFLOAD=true       # KV cache GPU offload

# Sampling VarsayÄ±lanlarÄ±
LLM_LLAMA_SERVICE_DEFAULT_TEMPERATURE=0.8
LLM_LLAMA_SERVICE_DEFAULT_MAX_TOKENS=1024
```

### Bellek Hesaplama FormÃ¼lÃ¼

```
Toplam VRAM â‰ˆ Model VRAM + (THREADS Ã— Context BaÅŸÄ±na VRAM)
```

**Ã–rnek (6GB GPU iÃ§in):**
- Model: gemma-3-1b-it-Q8_0 (~2.5GB)
- THREADS=2, CONTEXT_SIZE=4096
- Context BaÅŸÄ±na VRAM: ~800MB
- Toplam: 2.5GB + (2 Ã— 0.8GB) = **4.1GB** âœ…

---

## ğŸ“Š Ä°zleme & Metrikler

### Prometheus Metrikleri

`:16072/metrics` endpoint'inde sunulan temel metrikler:

| Metrik | AÃ§Ä±klama |
| :--- | :--- |
| `llm_active_contexts` | Åu anda meÅŸgul olan context sayÄ±sÄ± |
| `llm_requests_total` | Ä°ÅŸlenen toplam gRPC istekleri |
| `llm_tokens_generated_total` | Ãœretilen toplam token sayÄ±sÄ± |
| `llm_request_latency_seconds` | Ä°stek iÅŸleme sÃ¼relerinin histogram'Ä± |

### SaÄŸlÄ±k KontrolÃ¼

```bash
curl http://localhost:16070/health
```

**YanÄ±t:**
```json
{
  "status": "healthy",
  "model_ready": true,
  "capacity": {
    "active": 1,
    "total": 4,
    "available": 3,
    "has_capacity": true
  },
  "service": "sentiric-llm-llama-service",
  "timestamp": 1734048000
}
```

---

## ğŸ”§ GeliÅŸtirme

### Yerel Derleme (Docker ile)

```bash
# GPU ile
docker compose -f docker-compose.yml \
  -f docker-compose.gpu.yml \
  -f docker-compose.gpu.override.yml \
  up --build -d

# CPU ile
docker compose up --build -d
```

### Test Ã‡alÄ±ÅŸtÄ±rma

```bash
# E2E Test Suite
./e2e-test.sh

# CLI ile manuel test
docker compose -f docker-compose.run.gpu.yml run --rm llm-cli \
  llm_cli generate "Test mesajÄ±" --system-prompt "KÄ±sa cevap ver"
```

### Kod StandartlarÄ±

- **C++ Standard**: C++17
- **Stil**: Google C++ Style Guide
- **Bellek YÃ¶netimi**: RAII prensipleri zorunlu
- **Hata YÃ¶netimi**: Exception'lar yerine std::optional/Result pattern tercih edilir

### GeliÅŸtirici AraÃ§larÄ±

- **llm_cli**: Komut satÄ±rÄ± test aracÄ± (health, generate, benchmark)
- **Studio**: Web tabanlÄ± test ve debugging arayÃ¼zÃ¼
- **Benchmark Mode**: Performans testi (`llm_cli benchmark --concurrent 5`)

---

## ğŸš€ DaÄŸÄ±tÄ±m

### Docker Compose Profilleri

```bash
# Ãœretim - GPU (Pre-built Image)
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d

# Ãœretim - CPU (Pre-built Image)
docker compose -f docker-compose.yml -f docker-compose.cpu.yml up -d

# GeliÅŸtirme - Local Build
docker compose up --build -d
```

### Kubernetes

Helm chart Ã¶rneÄŸi (yakÄ±nda):
```yaml
# values.yaml
image:
  repository: ghcr.io/sentiric/sentiric-llm-llama-service
  tag: latest-gpu

resources:
  limits:
    nvidia.com/gpu: 1
    memory: 8Gi
  requests:
    nvidia.com/gpu: 1
    memory: 4Gi
```

### Kaynak Gereksinimleri

| YapÄ±landÄ±rma | CPU | RAM | VRAM | Model Ã–rnekleri |
|--------------|-----|-----|------|-----------------|
| Minimal (CPU) | 4 core | 8GB | - | gemma-3-1b-it-Q4 |
| Standart (GPU) | 4 core | 8GB | 6GB | gemma-3-1b-it-Q8, llama-3.2-3b |
| YÃ¼ksek (GPU) | 8 core | 16GB | 12GB | llama-3.1-8b, gemma-3-4b-it |

---

## ğŸ“š DokÃ¼mantasyon

### Temel DokÃ¼mantasyon
-   **[API Spesifikasyonu](./docs/API_SPECIFICATION.md)** - OpenAI uyumlu endpoints
-   **[YapÄ±landÄ±rma Rehberi](./docs/guides/03_CONFIGURATION.md)** - TÃ¼m ortam deÄŸiÅŸkenleri
-   **[DaÄŸÄ±tÄ±m Rehberi](./docs/guides/02_DEPLOYMENT.md)** - Production deployment
-   **[Docker Compose KullanÄ±m KÄ±lavuzu](./docs/guides/04_COMPOSE_USAGE_GUIDE.md)** - TÃ¼m senaryolar

### Mimari DokÃ¼mantasyon
-   **[Sistem Mimarisi](./docs/architecture/SYSTEM_ARCHITECTURE.md)** - BileÅŸen diyagramlarÄ±
-   **[Bellek & Performans](./docs/architecture/PERFORMANCE_AND_MEMORY.md)** - Optimizasyon rehberi

### Bilgi TabanÄ±
-   **[llama.cpp API ReferansÄ±](./docs/KB/04_LLAMA_CPP_API_BINDING.md)** - Kritik API kullanÄ±mlarÄ±
-   **[Ã‡Ã¶zÃ¼lmÃ¼ÅŸ Sorunlar](./docs/KB/03_SOLVED_ISSUES.md)** - Post-mortem analizler
-   **[Proje Desenleri](./docs/KB/02_PROJECT_PATTERNS.md)** - Mimari kararlar

---

## ğŸ¯ KullanÄ±m SenaryolarÄ±

### 1. RAG (Retrieval-Augmented Generation)

```python
import requests

response = requests.post("http://localhost:16070/v1/chat/completions", json={
    "messages": [
        {
            "role": "system",
            "content": "Sadece verilen baÄŸlama gÃ¶re cevap ver.\n\nBaÄŸlam:\n" + document_text
        },
        {"role": "user", "content": "DokÃ¼man hangi konuda?"}
    ],
    "temperature": 0.3,
    "max_tokens": 500
})
```

### 2. JSON Mode (Structured Output)

```bash
curl -X POST http://localhost:16070/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "TÃ¼rkiye hakkÄ±nda 3 bilgi ver"}],
    "response_format": {"type": "json_object"},
    "max_tokens": 200
  }'
```

### 3. KonuÅŸma GeÃ§miÅŸi

```bash
curl -X POST http://localhost:16070/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "BaÅŸkent nedir?"},
      {"role": "assistant", "content": "Ankara"},
      {"role": "user", "content": "NÃ¼fusu kaÃ§?"}
    ],
    "max_tokens": 100
  }'
```

---

## ğŸ” GÃ¼venlik

### mTLS YapÄ±landÄ±rmasÄ±

```bash
# Sertifikalar oluÅŸturma (geliÅŸtirme iÃ§in)
./sentiric-certificates/generate-dev-certs.sh

# Ortam deÄŸiÅŸkenleri
export GRPC_TLS_CA_PATH=/path/to/ca.crt
export LLM_LLAMA_SERVICE_CERT_PATH=/path/to/service-chain.crt
export LLM_LLAMA_SERVICE_KEY_PATH=/path/to/service.key
```

### GÃ¼venlik Kontrol Listesi

- âœ… API rate limiting (reverse proxy ile)
- âœ… mTLS kimlik doÄŸrulamasÄ± (opsiyonel)
- âœ… Input validation (tÃ¼m API'lerde)
- âœ… Prometheus metrikleri iÃ§in authn/authz
- âš ï¸ Model dosyasÄ± eriÅŸim kontrolÃ¼ (dosya sistemi izinleri)

---

## ğŸ› Sorun Giderme

### YaygÄ±n Sorunlar

**1. Model indirilmiyor**
```bash
# Log'larÄ± kontrol et
docker compose logs llm-llama-service | grep "Model download"

# Manuel indirme
docker exec -it llm-llama-service curl -L \
  "https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q8_0.gguf" \
  -o /models/gemma-3-1b-it-Q8_0.gguf
```

**2. Out of Memory (CUDA)**
```bash
# GPU katmanlarÄ±nÄ± azalt
LLM_LLAMA_SERVICE_GPU_LAYERS=50

# VEYA context size'Ä± dÃ¼ÅŸÃ¼r
LLM_LLAMA_SERVICE_CONTEXT_SIZE=2048
```

**3. gRPC BaÄŸlantÄ± HatasÄ±**
```bash
# Health check'i test et
curl http://localhost:16070/health

# Sertifika kontrolÃ¼
docker exec llm-llama-service ls -la /sentiric-certificates/certs/
```

---

## ğŸ¤ KatkÄ±da Bulunma

KatkÄ±larÄ±nÄ±zÄ± bekliyoruz! LÃ¼tfen aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyin:

1. Projeyi fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. DeÄŸiÅŸikliklerinizi commit edin (`git commit -m 'feat(engine): add amazing feature'`)
4. Branch'inizi push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

### Commit Mesaj FormatÄ±

Conventional Commits standardÄ±nÄ± kullanÄ±yoruz:
```
<type>(<scope>): <description>

[optional body]
[optional footer]
```

**Types:** feat, fix, docs, style, refactor, perf, test, chore

---

## ğŸ“Š Performans ReferanslarÄ±

### Token/Saniye (TPS) - NVIDIA RTX 3060 (6GB)

| Model | Quantization | Context | TPS | VRAM |
|-------|--------------|---------|-----|------|
| gemma-3-1b-it | Q8_0 | 4096 | ~45 | 2.5GB |
| gemma-3-1b-it | Q4_0 | 4096 | ~60 | 1.8GB |
| llama-3.2-3b | Q6_K | 4096 | ~35 | 3.2GB |
| llama-3.1-8b | Q5_K_M | 4096 | ~18 | 5.8GB |

*Not: TPS deÄŸerleri sistem konfigÃ¼rasyonuna gÃ¶re deÄŸiÅŸiklik gÃ¶sterebilir.*

---

## ğŸ“„ Lisans

Bu proje **GNU Affero General Public License v3.0** (AGPL-3.0) altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

KÄ±saca:
- âœ… Ticari kullanÄ±m
- âœ… DeÄŸiÅŸtirme
- âœ… DaÄŸÄ±tÄ±m
- âš ï¸ AÄŸ Ã¼zerinden kullanÄ±mda kaynak kodu paylaÅŸma zorunluluÄŸu
- âš ï¸ AynÄ± lisansla yeniden lisanslama

Detaylar iÃ§in [LICENSE](./LICENSE) dosyasÄ±na bakÄ±nÄ±z.

---

## ğŸ™ TeÅŸekkÃ¼rler

Bu proje aÅŸaÄŸÄ±daki aÃ§Ä±k kaynak projelerini kullanmaktadÄ±r:
- [llama.cpp](https://github.com/ggml-org/llama.cpp) - Ã‡ekirdek Ã§Ä±karÄ±m motoru
- [gRPC](https://grpc.io/) - RPC framework
- [Prometheus](https://prometheus.io/) - Metrik toplama
- [spdlog](https://github.com/gabime/spdlog) - Loglama kÃ¼tÃ¼phanesi
- [vcpkg](https://vcpkg.io/) - C++ paket yÃ¶neticisi

---

## ğŸ“ Ä°letiÅŸim & Destek

- **GitHub Issues**: [Sorun bildir](https://github.com/sentiric/sentiric-llm-llama-service/issues)
- **Discussions**: [Topluluk forumu](https://github.com/sentiric/sentiric-llm-llama-service/discussions)
- **Email**: support@sentiric.cloud

---

<div align="center">

**Sentiric LLM Llama Service** ile yapÄ±ldÄ± â¤ï¸

[â­ Star](https://github.com/sentiric/sentiric-llm-llama-service) | [ğŸ› Issue](https://github.com/sentiric/sentiric-llm-llama-service/issues) | [ğŸ’¬ Discussions](https://github.com/sentiric/sentiric-llm-llama-service/discussions)

</div>
