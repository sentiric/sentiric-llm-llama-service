# ğŸ§  Sentiric LLM Llama Service

**Sentiric LLM Llama Service**, yerel donanÄ±m Ã¼zerinde (on-premise) BÃ¼yÃ¼k Dil Modeli (LLM) Ã§Ä±karÄ±mÄ± saÄŸlayan, C++ ile yazÄ±lmÄ±ÅŸ yÃ¼ksek performanslÄ± bir uzman AI motorudur. `llama.cpp` kÃ¼tÃ¼phanesini temel alarak, `Phi-3`, `Llama3` gibi popÃ¼ler aÃ§Ä±k kaynaklÄ± modelleri GGUF formatÄ±nda, minimum kaynak tÃ¼ketimi ve gecikme ile Ã§alÄ±ÅŸtÄ±rÄ±r.

Bu servis, `llm-gateway-service` tarafÄ±ndan, en Ã¼st dÃ¼zeyde performans, gÃ¼venlik ve verimlilik gerektiren metin Ã¼retimi ihtiyaÃ§larÄ± iÃ§in Ã§aÄŸrÄ±lÄ±r. Bu servis, Python tabanlÄ± `llm-local-service`'in yerini alacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.

## ğŸ¯ Temel Sorumluluklar

-   **Maksimum Performans:** C++ ve `llama.cpp` sayesinde Python ek yÃ¼kÃ¼ (overhead) olmadan, donanÄ±ma en yakÄ±n hÄ±zda LLM Ã§Ä±karÄ±mÄ± yapar.
-   **DÃ¼ÅŸÃ¼k Kaynak TÃ¼ketimi:** Kuantize edilmiÅŸ GGUF modelleri ile Ã§ok dÃ¼ÅŸÃ¼k bellek (RAM) kullanÄ±mÄ± saÄŸlar.
-   **gRPC Streaming:** Metin yanÄ±tlarÄ±nÄ± token token Ã¼reterek dÃ¼ÅŸÃ¼k algÄ±lanan gecikme saÄŸlar.
-   **DonanÄ±m VerimliliÄŸi:** Modern CPU komut setlerini (AVX, AVX2) ve opsiyonel olarak GPU hÄ±zlandÄ±rmayÄ± (CUDA, Metal) destekler.
-   **Dinamik SaÄŸlÄ±k KontrolÃ¼:** Modelin yÃ¼klenip hazÄ±r olup olmadÄ±ÄŸÄ±nÄ± bildiren `/health` endpoint'i.

## ğŸ› ï¸ Derleme ve Ã‡alÄ±ÅŸtÄ±rma

### Ã–n KoÅŸullar
- Docker ve Docker Compose
- Git
- Bir adet GGUF formatÄ±nda LLM modeli (Ã–rn: `Phi-3-mini-4k-instruct.Q4_K_M.gguf`)
(https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf)
### AdÄ±m AdÄ±m Kurulum

1.  **Repoyu Klonla:**
    ```bash
    git clone https://github.com/sentiric/llm-llama-service.git
    cd llm-llama-service
    ```

2.  **`llama.cpp` Submodule'Ã¼nÃ¼ YÃ¼kle:**
    ```bash
    git submodule update --init --recursive
    ```

3.  **Modeli HazÄ±rla:**
    - Proje kÃ¶k dizininde `model-cache` adÄ±nda bir klasÃ¶r oluÅŸturun.
    - Ä°ndirdiÄŸiniz GGUF model dosyasÄ±nÄ± bu klasÃ¶rÃ¼n iÃ§ine kopyalayÄ±n.

4.  **YapÄ±landÄ±rmayÄ± DÃ¼zenle:**
    - `docker-compose.yml` dosyasÄ±nÄ± aÃ§Ä±n.
    - `environment` bÃ¶lÃ¼mÃ¼ndeki `LLM_LOCAL_SERVICE_MODEL_PATH` deÄŸiÅŸkenini, kendi model dosyanÄ±zÄ±n adÄ±yla gÃ¼ncelleyin.

5.  **Servisi BaÅŸlat:**
    ```bash
    docker compose up --build
    ```
    Ä°lk derleme iÅŸlemi birkaÃ§ dakika sÃ¼rebilir. Sonraki baÅŸlatmalar Ã§ok daha hÄ±zlÄ± olacaktÄ±r.

## âœ… DoÄŸrulama

-   **SaÄŸlÄ±k KontrolÃ¼:** TarayÄ±cÄ±nÄ±zda `http://localhost:16060/health` adresini aÃ§Ä±n veya terminalden `curl http://localhost:16060/health` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n. `{"model_ready":true}` yanÄ±tÄ±nÄ± gÃ¶rmelisiniz.
-   **gRPC Test:** Projeyle birlikte gelen Python test istemcisini kullanarak servisi test edin:
    ```bash
    # Gerekli kÃ¼tÃ¼phaneleri kurun
    pip install grpcio protobuf

    # Test istemcisini Ã§alÄ±ÅŸtÄ±rÄ±n
    python grpc_test_client.py "TÃ¼rkiye'nin baÅŸkenti neresidir?"
    ```