# ğŸ§  Sentiric LLM Llama Service

**Production-ready** yerel LLM servisi - C++ ile yÃ¼ksek performanslÄ± AI motoru. Phi-3-mini modeli ile tam entegre.

## ğŸš€ Ã–zellikler

- âœ… **YÃ¼ksek Performans**: C++ & llama.cpp optimizasyonu
- âœ… **GRPC Streaming**: Token-token real-time yanÄ±t  
- âœ… **HTTP Health Check**: `/health` endpoint
- âœ… **Docker Container**: Tam izole edilmiÅŸ deployment
- âœ… **Stable Build**: Static linking ile gÃ¼venilir Ã§alÄ±ÅŸma
- âœ… **Phi-3-mini Model**: 3B parametre, TÃ¼rkÃ§e destek
- âœ… **Hafif Repo**: SubmodulesÃ¼z, temiz yapÄ±

## ğŸ“¦ Teknik Spesifikasyonlar

### Versiyon Bilgisi
- **Servis Versiyonu**: v1.0.0-stable
- **llama.cpp Commit**: 0750a59903688746883b0ecb24ac5ceed68edbf1
- **Model**: Phi-3-mini-4k-instruct-q4.gguf
- **BaÄŸÄ±mlÄ±lÄ±klar**: Static build (libgomp1 hariÃ§)

### Port YapÄ±landÄ±rmasÄ±
- **HTTP Health**: 16060
- **GRPC Service**: 16061

### Model Ã–zellikleri
- **Boyut**: 2.23GB (Q4_K quantize)
- **Context**: 4096 token
- **Parametre**: 3.82B
- **Dil**: TÃ¼rkÃ§e & Ä°ngilizce

## ğŸ› ï¸ Kurulum

### Ã–n KoÅŸullar
- Docker & Docker Compose
- 4GB+ RAM
- 3GB+ Disk alanÄ±

### HÄ±zlÄ± BaÅŸlangÄ±Ã§
```bash
# 1. Repoyu klonla (--recursive gerekmez!)
git clone https://github.com/sentiric/sentiric-llm-llama-service.git
cd sentiric-llm-llama-service

# 2. Modeli indir
./models/download.sh

# 3. Servisi baÅŸlat
docker compose up --build -d

# 4. SaÄŸlÄ±k kontrolÃ¼
curl http://localhost:16060/health

# 5. Test
docker compose exec llm-llama-service grpc_test_client "Merhaba"
```

## ğŸ”§ GeliÅŸtirici Rehberi

### Build SÃ¼reci
```bash
# Clean build
docker compose down
docker system prune -f
docker compose up --build -d
```

### Debug
```bash
# LoglarÄ± izle
docker logs -f llm-llama-service

# Container'a baÄŸlan
docker exec -it llm-llama-service bash
```

## ğŸ¯ API KullanÄ±mÄ±

### GRPC Ä°stemcisi
```cpp
// Ã–rnek kullanÄ±m
auto client = LlamaClient(grpc::CreateChannel(
    "localhost:16061", 
    grpc::InsecureChannelCredentials()
));
client.GenerateStream("TÃ¼rkiye'nin baÅŸkenti?");
```

### Health Endpoint
```bash
curl http://localhost:16060/health
# {"engine":"llama.cpp","model_ready":true,"status":"healthy"}
```

## ğŸ› Sorun Giderme

### SÄ±k KarÅŸÄ±laÅŸÄ±lan Sorunlar

1. **libgomp.so.1 hatasÄ±**: 
   ```dockerfile
   # Ã‡Ã¶zÃ¼m: libgomp1 paketini yÃ¼kle
   RUN apt-get install -y libgomp1
   ```

2. **Model yÃ¼klenemiyor**:
   - Model dosyasÄ±nÄ± kontrol et: `/models/phi-3-mini.q4.gguf`
   - Disk alanÄ±nÄ± kontrol et

3. **Build baÅŸarÄ±sÄ±z**:
   - Cache'i temizle: `docker system prune -f`
   - Static build flag'lerini kontrol et

## ğŸ“Š Performans

- **Model YÃ¼kleme**: ~30 saniye
- **Token Generation**: ~50 token/saniye
- **Bellek KullanÄ±mÄ±**: ~2.5GB
- **CPU KullanÄ±mÄ±**: 4 thread

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun
3. DeÄŸiÅŸiklikleri commit edin
4. Pull request aÃ§Ä±n

**Ã–NEMLÄ°**: Static build flag'lerini deÄŸiÅŸtirmeyin!