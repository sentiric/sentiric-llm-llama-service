{
  "active_profile": "sentiric_6gb_qwen25_3b",

  "profiles": {

    /* ---------------------------- */
    /*       6 GB VRAM MODELLERÄ°   */
    /* ---------------------------- */

    "sentiric_6gb_qwen25_3b": {
      "description": "ğŸ”¥ [6GB] En kaliteli genel LLM (Qwen2.5 3B)",
      "model_id": "Qwen/Qwen2.5-3B-Instruct-GGUF",
      "filename": "qwen2.5-3b-instruct-Q5_K_M.gguf",
      "context_size": 4096,
      "gpu_layers": 85,
      "temperature": 0.45,
      "system_prompt": "YÃ¼ksek doÄŸrulukta, hÄ±zlÄ± ve net cevaplar Ã¼ret."
    },

    "sentiric_6gb_llama33_3b": {
      "description": "âš¡ [6GB] Stabil + gÃ¼venilir (Llama 3.3 3B)",
      "model_id": "bartowski/Llama-3.3-3B-Instruct-GGUF",
      "filename": "Llama-3.3-3B-Instruct-Q5_K_M.gguf",
      "context_size": 4096,
      "gpu_layers": 90,
      "temperature": 0.45,
      "system_prompt": "KararlÄ± ve tutarlÄ± yanÄ±tlar ver."
    },

    "sentiric_6gb_phi35": {
      "description": "ğŸ“š [6GB] Uzun metin + tutarlÄ±lÄ±k (Phi-3.5 Mini)",
      "model_id": "microsoft/Phi-3.5-mini-gguf",
      "filename": "phi-3.5-mini-Q5_K_M.gguf",
      "context_size": 4096,
      "gpu_layers": 60,
      "temperature": 0.4,
      "system_prompt": "TutarlÄ±, uzun metinlerde gÃ¼Ã§lÃ¼ cevaplar Ã¼ret."
    },

    "sentiric_6gb_code_light": {
      "description": "ğŸ’» [6GB] Hafif kodlama modeli (Qwen2.5 Coder 1.5B)",
      "model_id": "Qwen/Qwen2.5-Coder-1.5B-GGUF",
      "filename": "qwen2.5-coder-1.5b-Q8_0.gguf",
      "context_size": 4096,
      "gpu_layers": 80,
      "temperature": 0.1,
      "system_prompt": "Step-by-step analiz ile temiz kod Ã¼ret."
    },

    "sentiric_6gb_gemma2_2b": {
      "description": "ğŸª¶ [6GB] Ultra hafif + stabil (Gemma 2 2B)",
      "model_id": "google/gemma-2-2b-it-GGUF",
      "filename": "gemma-2-2b-Q6_K.gguf",
      "context_size": 3072,
      "gpu_layers": 100,
      "temperature": 0.5,
      "system_prompt": "HÄ±zlÄ± ve ekonomik gÃ¶revler iÃ§in optimize edilmiÅŸ."
    },

    "sentiric_6gb_phi3_mini": {
      "description": "ğŸ¤– [6GB] KÃ¼Ã§Ã¼k ama akÄ±llÄ± (Phi-3 Mini 3.1B)",
      "model_id": "microsoft/Phi-3-mini-gguf",
      "filename": "phi-3-mini-Q6_K.gguf",
      "context_size": 3072,
      "gpu_layers": 75,
      "temperature": 0.45,
      "system_prompt": "Genel iÅŸlerde hafif ama akÄ±llÄ± bir yardÄ±mcÄ±."
    },

    "sentiric_6gb_mistral_nemo_2b": {
      "description": "ğŸŒ¬ï¸ [6GB] Temiz + hafif reasoning (Mistral Nemo 2B)",
      "model_id": "mistral/Mistral-Nemo-2B-GGUF",
      "filename": "mistral-nemo-2b-Q6_K.gguf",
      "context_size": 4096,
      "gpu_layers": 85,
      "temperature": 0.5,
      "system_prompt": "Dengeli ve hÄ±zlÄ± yanÄ±tlar ver."
    },

    "sentiric_6gb_llama31_1b": {
      "description": "ğŸ§© [6GB] Basit gÃ¶revlerde ultra dÃ¼ÅŸÃ¼k VRAM (Llama 3.1 1B)",
      "model_id": "bartowski/Llama-3.1-1B-Instruct-GGUF",
      "filename": "Llama-3.1-1B-Instruct-Q8_0.gguf",
      "context_size": 2048,
      "gpu_layers": 100,
      "temperature": 0.6,
      "system_prompt": "Basit sorular iÃ§in minimal model."
    },

    "sentiric_6gb_qwen25_0.5b": {
      "description": "ğŸ” [6GB] RAG + bilgi tabanÄ± yardÄ±mcÄ± (Qwen2.5 0.5B)",
      "model_id": "Qwen/Qwen2.5-0.5B-GGUF",
      "filename": "qwen2.5-0.5b-Q8_0.gguf",
      "context_size": 2048,
      "gpu_layers": 100,
      "temperature": 0.6,
      "system_prompt": "Bilgi tabanÄ± Ã§aÄŸÄ±rma gibi hafif iÅŸler iÃ§in."
    },

    "sentiric_6gb_deepseek_coder_1.3b": {
      "description": "ğŸ§  [6GB] Alternatif hafif kodlama modeli",
      "model_id": "deepseek-ai/deepseek-coder-1.3b-GGUF",
      "filename": "deepseek-coder-1.3b-Q8_0.gguf",
      "context_size": 4096,
      "gpu_layers": 95,
      "temperature": 0.2,
      "system_prompt": "Kodlama ve analiz iÃ§in uygun."
    },

    /* ---------------------------- */
    /*       12 GB VRAM MODELLERÄ°  */
    /* ---------------------------- */

    "sentiric_12gb_deepseek_v3": {
      "description": "ğŸš€ [12GB] Ãœretim iÃ§in en gÃ¼Ã§lÃ¼ aÃ§Ä±k kaynak (DeepSeek V3 8B)",
      "model_id": "deepseek-ai/DeepSeek-V3-MoE-8B-GGUF",
      "filename": "deepseek-v3-moe-8b-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 40,
      "temperature": 0.55,
      "system_prompt": "Ãœst seviye reasoning ve detaylÄ± analiz ver."
    },

    "sentiric_12gb_qwen25_7b": {
      "description": "âš–ï¸ [12GB] En iyi TÃ¼rkÃ§e + mantÄ±k (Qwen2.5 7B)",
      "model_id": "Qwen/Qwen2.5-7B-Instruct-GGUF",
      "filename": "qwen2.5-7b-instruct-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 32,
      "temperature": 0.55,
      "system_prompt": "Ãœst dÃ¼zey mantÄ±k ve baÄŸlam analizi yap."
    },

    "sentiric_12gb_coder_pro": {
      "description": "ğŸ’» [12GB] Kodlama iÃ§in en iyi model (Qwen2.5 Coder 7B)",
      "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF",
      "filename": "qwen2.5-coder-7b-instruct-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 35,
      "temperature": 0.15,
      "system_prompt": "Profesyonel dÃ¼zeyde kod ve aÃ§Ä±klama Ã¼ret."
    },

    "sentiric_12gb_mistral_nemo_7b": {
      "description": "ğŸŒ¬ï¸ [12GB] Dengeli + kaliteli (Mistral Nemo 7B)",
      "model_id": "mistral/Mistral-Nemo-7B-GGUF",
      "filename": "mistral-nemo-7b-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 35,
      "temperature": 0.55,
      "system_prompt": "Denge, stabilite ve kalite saÄŸlar."
    },

    "sentiric_12gb_llama33_8b": {
      "description": "ğŸ”· [12GB] Meta tabanlÄ± stabil model (Llama 3.3 8B)",
      "model_id": "bartowski/Llama-3.3-8B-Instruct-GGUF",
      "filename": "Llama-3.3-8B-Instruct-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 33,
      "temperature": 0.6,
      "system_prompt": "KararlÄ± ve gÃ¼venli yanÄ±tlar ver."
    },

    "sentiric_12gb_qwen25_14b_q4": {
      "description": "ğŸ“š [12GB] Uzun context (32k) RAG iÃ§in (Qwen2.5 14B)",
      "model_id": "Qwen/Qwen2.5-14B-Instruct-GGUF",
      "filename": "qwen2.5-14b-instruct-Q4_K_M.gguf",
      "context_size": 32768,
      "gpu_layers": 17,
      "temperature": 0.55,
      "system_prompt": "Uzun belgelerde en yÃ¼ksek tutarlÄ±lÄ±k."
    },

    "sentiric_12gb_mixtral_8x7b_q4": {
      "description": "ğŸ§  [12GB] MoE reasoning (Mixtral 8x7B Q4)",
      "model_id": "mistral/Mixtral-8x7B-GGUF",
      "filename": "mixtral-8x7b-Q4_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 16,
      "temperature": 0.55,
      "system_prompt": "MoE yapÄ±sÄ±yla gÃ¼Ã§lÃ¼ mantÄ±k yÃ¼rÃ¼t."
    },

    "sentiric_12gb_llama31_8b": {
      "description": "ğŸ”¹ [12GB] Hafif ama kaliteli 8B alternatif (Llama 3.1 8B)",
      "model_id": "bartowski/Llama-3.1-8B-Instruct-GGUF",
      "filename": "Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 33,
      "temperature": 0.55,
      "system_prompt": "GÃ¼Ã§lÃ¼ ve dengeli bir genel model."
    },

    "sentiric_12gb_deepseek_coder_v2_7b": {
      "description": "ğŸ’» [12GB] Alternatif profesyonel kodlama modeli (DeepSeek Coder V2 7B)",
      "model_id": "deepseek-ai/deepseek-coder-v2-7b-GGUF",
      "filename": "deepseek-coder-v2-7b-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 33,
      "temperature": 0.2,
      "system_prompt": "KarmaÅŸÄ±k yazÄ±lÄ±m gÃ¶revlerinde etkili."
    },

    "sentiric_12gb_neuralbeagle_7b_tr": {
      "description": "ğŸ‡¹ğŸ‡· [12GB] TÃ¼rkÃ§e aÄŸÄ±rlÄ±klÄ± yerel model",
      "model_id": "NeuralBeagle/NeuralBeagle-7B-Turkish-GGUF",
      "filename": "neuralbeagle-7b-tr-Q5_K_M.gguf",
      "context_size": 8192,
      "gpu_layers": 24,
      "temperature": 0.55,
      "system_prompt": "TÃ¼rkÃ§e diyaloglarda gÃ¼Ã§lÃ¼ performans."
    }
  }
}
