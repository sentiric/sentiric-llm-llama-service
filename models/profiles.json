{
  "active_profile": "qwen3_4b_q5",
  "profiles": {
    "qwen3_4b_q5": {
      "display_name": "Qwen 3 (4B, Q5_K_M)",
      "description": "~2.8 GB VRAM. Akıl yürüten (Reasoning) yetenekli model.",
      "category": "Qwen Ailesi",
      "model_id": "Qwen/Qwen3-4B-GGUF",
      "filename": "Qwen3-4B-Q5_K_M.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.6,
      "system_prompt": "Sen yardımsever ve zeki bir asistansın."
    },
    "qwen3_4b_q8": {
      "display_name": "Qwen 3 (4B, Q8_0)",
      "description": "~4.3 GB VRAM. Yüksek hassasiyetli akıl yürütme.",
      "category": "Qwen Ailesi",
      "model_id": "Qwen/Qwen3-4B-GGUF",
      "filename": "Qwen3-4B-Q8_0.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.6,
      "system_prompt": "Sen yardımsever ve zeki bir asistansın."
    },
    "gemma3_1b_q8": {
      "display_name": "Gemma 3 (1B, Q8_0)",
      "description": "~1.8 GB VRAM. Hızlı cevap.",
      "category": "Gemma Ailesi",
      "model_id": "ggml-org/gemma-3-1b-it-GGUF",
      "filename": "gemma-3-1b-it-Q8_0.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.6,
      "system_prompt": "Sen yardımsever bir asistansın."
    },
    "gemma3_4b_q8": {
      "display_name": "Gemma 3 (4B, Q8_0)",
      "description": "~3.1 GB VRAM. Analitik düşünür.",
      "category": "Gemma Ailesi",
      "model_id": "ggml-org/gemma-3-4b-it-GGUF",
      "filename": "gemma-3-4b-it-Q8_0.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.5,
      "system_prompt": "Sen yardımsever bir asistansın."
    },
    "llama3_8b_q4": {
      "display_name": "Llama 3.1 (8B, Q4_K_M)",
      "description": "~4.8 GB VRAM. Genel amaçlı.",
      "category": "Llama Ailesi",
      "model_id": "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF",
      "filename": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.6,
      "system_prompt": "Sen yardımsever bir asistansın."
    },
    "llama3_1b_q4": {
      "display_name": "Llama 3.2 (1B, Q4_K_M)",
      "description": "~0.8 GB VRAM. En popüler ve çok yönlü model.",
      "category": "Llama Ailesi",
      "model_id": "bartowski/Llama-3.2-1B-Instruct-GGUF",
      "filename": "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.6,
      "system_prompt": "Sen yardımsever bir asistansın."
    },
    "phi3_mini_4k_q4": {
      "display_name": "Phi-3 Mini (3.8B, Q4_K_M)",
      "description": "~2.2 GB VRAM. Kodlama ve mantık.",
      "category": "Phi Ailesi",
      "model_id": "microsoft/Phi-3-mini-4k-instruct-gguf",
      "filename": "Phi-3-mini-4k-instruct-q4.gguf",
      "context_size": 4096,
      "gpu_layers": -1,
      "temperature": 0.5,
      "system_prompt": "Sen yardımsever bir asistansın."
    }
  }
}