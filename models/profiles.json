{
  "active_profile": "sentiric_6gb_qwen25_3b",
  "profiles": {
    "sentiric_6gb_qwen25_3b": {
      "display_name": "ğŸ”¥ Qwen 2.5 3B (Kalite)",
      "description": "6GB VRAM iÃ§in en kaliteli genel amaÃ§lÄ± model.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "Qwen/Qwen2.5-3B-Instruct-GGUF",
      "filename": "qwen2.5-3b-instruct-q5_k_m.gguf",
      "context_size": 4096,
      "gpu_layers": 85,
      "temperature": 0.45,
      "system_prompt": "YÃ¼ksek doÄŸrulukta, hÄ±zlÄ± ve net cevaplar Ã¼ret."
    },
    "sentiric_6gb_llama33_3b": {
      "display_name": "âš¡ Llama 3.3 3B (Stabil)",
      "description": "Meta'nÄ±n gÃ¼venilir, hÄ±zlÄ± ve tutarlÄ± modeli.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "MaziyarPanahi/calme-3.3-llamaloi-3b-GGUF",
      "filename": "calme-3.3-llamaloi-3b.Q5_K_M.gguf",
      "context_size": 4096,
      "gpu_layers": 90,
      "temperature": 0.45,
      "system_prompt": "KararlÄ± ve tutarlÄ± yanÄ±tlar ver."
    },
    "sentiric_6gb_phi35_mini": {
      "display_name": "ğŸ“š Phi-3.5 Mini 3.1B (Uzun Metin)",
      "description": "KÃ¼Ã§Ã¼k boyutuna raÄŸmen uzun metinlerde Ã§ok tutarlÄ±.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "bartowski/Phi-3.5-mini-instruct-GGUF",
      "filename": "Phi-3.5-mini-instruct-Q5_K_M.gguf",
      "context_size": 4096,
      "gpu_layers": 60,
      "temperature": 0.40,
      "system_prompt": "Uzun metinlerde tutarlÄ± ve mantÄ±klÄ± yanÄ±tlar Ã¼ret."
    },
    "sentiric_6gb_phi3_mini": {
      "display_name": "ğŸ¤– Phi-3 Mini 3.1B (AkÄ±llÄ±)",
      "description": "Genel kullanÄ±mda hafif ama beklenenden akÄ±llÄ±.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "microsoft/Phi-3-mini-4k-instruct-gguf",
      "filename": "Phi-3-mini-4k-instruct-q4.gguf",
      "context_size": 3072,
      "gpu_layers": 75,
      "temperature": 0.45,
      "system_prompt": "Genel iÅŸlerde hafif ama akÄ±llÄ± bir yardÄ±mcÄ±."
    },
    "sentiric_6gb_gemma2_2b": {
      "display_name": "ğŸª¶ Gemma 2 2B (Ultra Hafif)",
      "description": "En hafif ama stabil modellerden biri.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "bartowski/gemma-2-2b-it-GGUF",
      "filename": "gemma-2-2b-it-Q6_K.gguf",
      "context_size": 3072,
      "gpu_layers": 100,
      "temperature": 0.50,
      "system_prompt": "Hafif gÃ¶revler iÃ§in optimize edilmiÅŸ hÄ±zlÄ± model."
    },
    "sentiric_6gb_mistral_nemo_2b": {
      "display_name": "ğŸŒ¬ï¸ Mistral Nemo 2B (Dengeli)",
      "description": "2B boyutunda ÅŸaÅŸÄ±rtÄ±cÄ± derecede dengeli performans.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "mistralai/Ministral-3-3B-Instruct-2512-GGUF",
      "filename": "Ministral-3-3B-Instruct-2512-Q5_K_M.gguf",
      "context_size": 4096,
      "gpu_layers": 85,
      "temperature": 0.5,
      "system_prompt": "Dengeli, hafif ve hÄ±zlÄ± sonuÃ§lar Ã¼ret."
    },
    "sentiric_6gb_qwen25_0.5b": {
      "display_name": "ğŸ” Qwen 2.5 0.5B (RAG YardÄ±mcÄ±)",
      "description": "RAG/embedding tabanlÄ± hafif gÃ¶revler iÃ§in ideal.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "Qwen/Qwen2.5-0.5B-Instruct-GGUF",
      "filename": "qwen2.5-0.5b-instruct-q8_0.gguf",
      "context_size": 2048,
      "gpu_layers": 100,
      "temperature": 0.6,
      "system_prompt": "Bilgi tabanÄ± ve hafif gÃ¶revlerde kullanÄ±lacak mini model."
    },
    "sentiric_6gb_code_light": {
      "display_name": "ğŸ’» Qwen 2.5 Coder 1.5B (Hafif Kodlama)",
      "description": "Kodlama iÃ§in kÃ¼Ã§Ã¼k ama etkili bir model.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF",
      "filename": "qwen2.5-coder-1.5b-instruct-q8_0.gguf",
      "context_size": 4096,
      "gpu_layers": 80,
      "temperature": 0.1,
      "system_prompt": "Step-by-step analizle temiz ve optimal kodlar Ã¼ret."
    },
    "sentiric_6gb_deepseek_coder_1.3b": {
      "display_name": "ğŸ§  DeepSeek Coder 1.3B (Kod Alternatifi)",
      "description": "Kodlama iÃ§in hafif, hÄ±zlÄ± ve mantÄ±klÄ±.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "TheBloke/deepseek-coder-1.3b-instruct-GGUF",
      "filename": "deepseek-coder-1.3b-instruct.Q8_0.gguf",
      "context_size": 4096,
      "gpu_layers": 95,
      "temperature": 0.2,
      "system_prompt": "HÄ±zlÄ± ve hafif kodlama gÃ¶revleri iÃ§in ideal."
    },
    "sentiric_6gb_llama31_1b": {
      "display_name": "ğŸ§© Llama 3.1 1B (Mini Stabil)",
      "description": "Basit iÅŸlemler ve sistem task'leri iÃ§in Ã§ok dÃ¼ÅŸÃ¼k VRAM.",
      "category": "6 GB VRAM Modelleri",
      "model_id": "bartowski/Llama-3.2-1B-Instruct-GGUF",
      "filename": "Llama-3.2-1B-Instruct-Q8_0.gguf",
      "context_size": 2048,
      "gpu_layers": 100,
      "temperature": 0.6,
      "system_prompt": "Basit komutlarda hÄ±zlÄ± yanÄ±t veren mini model."
    },
    "sentiric_12gb_deepseek_v3": {
      "display_name": "ğŸš€ DeepSeek V3 8B (En GÃ¼Ã§lÃ¼)",
      "description": "12GB iÃ§in en gÃ¼Ã§lÃ¼ mantÄ±k ve analiz modeli.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "unsloth/DeepSeek-V3-GGUF",
      "filename": "DeepSeek-V3-Q5_K_M/DeepSeek-V3-Q5_K_M-00001-of-00010.gguf",
      "context_size": 16384,
      "gpu_layers": 40,
      "temperature": 0.55,
      "system_prompt": "Ãœst seviye reasoning, analiz ve baÄŸlam iÅŸleme."
    },
    "sentiric_12gb_qwen25_7b": {
      "display_name": "ğŸ‡¹ğŸ‡· Qwen 2.5 7B (TÃ¼rkÃ§e Lideri)",
      "description": "TÃ¼rkÃ§e anlama + mantÄ±k aÃ§Ä±sÄ±ndan en iyi model.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "Qwen/Qwen2.5-7B-Instruct-GGUF",
      "filename": "qwen2.5-7b-instruct-q5_k_m-00001-of-00002.gguf",
      "context_size": 16384,
      "gpu_layers": 32,
      "temperature": 0.55,
      "system_prompt": "TÃ¼rkÃ§e'de gÃ¼Ã§lÃ¼ mantÄ±k ve baÄŸlam analizi yap."
    },
    "sentiric_12gb_coder_pro": {
      "display_name": "ğŸ’» Qwen 2.5 Coder 7B (Pro Kodlama)",
      "description": "Profesyonel yazÄ±lÄ±m gÃ¶revleri iÃ§in en iyi aÃ§Ä±k kaynak model.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF",
      "filename": "qwen2.5-coder-7b-instruct-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 35,
      "temperature": 0.15,
      "system_prompt": "KarmaÅŸÄ±k yazÄ±lÄ±m gÃ¶revlerinde adÄ±m adÄ±m dÃ¼ÅŸÃ¼n ve optimal kod Ã¼ret."
    },
    "sentiric_12gb_mistral_nemo_7b": {
      "display_name": "ğŸŒ¬ï¸ Mistral Nemo 7B (Dengeli)",
      "description": "Genel kullanÄ±mda kaliteli + stabil performans.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "mistral/Mistral-Nemo-7B-GGUF",
      "filename": "mistral-nemo-7b-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 35,
      "temperature": 0.55,
      "system_prompt": "Denge, kalite ve hÄ±z saÄŸlayan 7B model."
    },
    "sentiric_12gb_llama33_8b": {
      "display_name": "ğŸ”· Llama 3.3 8B (Meta Stabil)",
      "description": "Meta'nÄ±n gÃ¼venilir, kararlÄ± ve kaliteli modeli.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "bartowski/Meta-Llama-3-8B-Instruct-GGUF",
      "filename": "Meta-Llama-3-8B-Instruct-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 33,
      "temperature": 0.6,
      "system_prompt": "KararlÄ± ve gÃ¼venli yanÄ±tlar ver."
    },
    "sentiric_12gb_qwen25_14b_q4": {
      "display_name": "ğŸ“š Qwen 2.5 14B (Uzun Context 32k)",
      "description": "32k context ile RAG ve belge analizi iÃ§in mÃ¼kemmel.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "Qwen/Qwen2.5-14B-Instruct-GGUF",
      "filename": "qwen2.5-14b-instruct-q4_k_m-00001-of-00003.gguf",
      "context_size": 32768,
      "gpu_layers": 17,
      "temperature": 0.55,
      "system_prompt": "Uzun belgelerde yÃ¼ksek tutarlÄ±lÄ±k saÄŸla."
    },
    "sentiric_12gb_mixtral_8x7b_q4": {
      "display_name": "ğŸ§  Mixtral 8x7B MoE (Reasoning)",
      "description": "MoE yapÄ±sÄ±yla daha gÃ¼Ã§lÃ¼ mantÄ±k yÃ¼rÃ¼tme kabiliyeti.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "mradermacher/Mistral-8-from-Mixtral-8x7B-v0.1-GGUF",
      "filename": "Mistral-8-from-Mixtral-8x7B-v0.1.Q4_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 16,
      "temperature": 0.55,
      "system_prompt": "KarmaÅŸÄ±k gÃ¶revlerde gÃ¼Ã§lÃ¼ reasoning Ã¼ret."
    },
    "sentiric_12gb_llama31_8b": {
      "display_name": "ğŸ”¹ Llama 3.1 8B (Dengeli Alternatif)",
      "description": "8B sÄ±nÄ±fÄ±nda hafif ama kaliteli bir seÃ§enek.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF",
      "filename": "Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 33,
      "temperature": 0.55,
      "system_prompt": "Hafif ve dengeli bir 8B model."
    },
    "sentiric_12gb_deepseek_coder_v2_7b": {
      "display_name": "ğŸ’» DeepSeek Coder V2 7B (Alternatif Kodlama)",
      "description": "Kodlama ve hata Ã§Ã¶zÃ¼mÃ¼nde gÃ¼Ã§lÃ¼ ikinci seÃ§enek.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "deepseek-ai/deepseek-coder-v2-7b-GGUF",
      "filename": "deepseek-coder-v2-7b-Q5_K_M.gguf",
      "context_size": 16384,
      "gpu_layers": 33,
      "temperature": 0.2,
      "system_prompt": "KarmaÅŸÄ±k yazÄ±lÄ±m gÃ¶revlerini analiz et ve Ã§Ã¶z."
    },
    "sentiric_12gb_neuralbeagle_7b_tr": {
      "display_name": "ğŸ‡¹ğŸ‡· NeuralBeagle 7B (TÃ¼rkÃ§e)",
      "description": "TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ yerel model.",
      "category": "12 GB VRAM Modelleri",
      "model_id": "TheBloke/NeuralBeagle14-7B-GGUF",
      "filename": "neuralbeagle14-7b.Q5_K_M.gguf",
      "context_size": 8192,
      "gpu_layers": 24,
      "temperature": 0.55,
      "system_prompt": "TÃ¼rkÃ§e odaklÄ± diyalog ve analizlerde gÃ¼Ã§lÃ¼ performans."
    }
  }
}