{
  "active_profile": "qwen3_4b_q5",
  "profiles": {
    "qwen3_4b_q5": {
      "display_name": "Qwen 3 (4B, Q5_K_M)",
      "description": "~2.8 GB VRAM. Akıl yürüten (Reasoning) yetenekli model.",
      "category": "Qwen Ailesi",
      "model_id": "Qwen/Qwen3-4B-GGUF",
      "filename": "Qwen3-4B-Q5_K_M.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.6,
      "system_prompt": "Sen yardımsever bir asistansın.\n\nKURALLAR:\n1. Kullanıcının sorusunu cevaplamadan önce derinlemesine düşün.\n2. Bu düşünme sürecini '<thought>' etiketi ile başlat ve '</thought>' etiketi ile bitir.\n3. Düşünme kısmı bittikten sonra, kullanıcıya doğrudan ve net bir cevap ver.\n4. Düşünme sürecini (thought) kullanıcıya hitaben yazma, kendine notlar al."
    },
    "qwen3_4b_q8": {
      "display_name": "Qwen 3 (4B, Q8_0)",
      "description": "~4.3 GB VRAM. Yüksek hassasiyetli akıl yürütme.",
      "category": "Qwen Ailesi",
      "model_id": "Qwen/Qwen3-4B-GGUF",
      "filename": "Qwen3-4B-Q8_0.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.6,
      "system_prompt": "Sen yardımsever bir asistansın.\n\nKURALLAR:\n1. Kullanıcının sorusunu cevaplamadan önce derinlemesine düşün.\n2. Bu düşünme sürecini '<thought>' etiketi ile başlat ve '</thought>' etiketi ile bitir.\n3. Düşünme kısmı bittikten sonra, kullanıcıya doğrudan ve net bir cevap ver."
    },
    "gemma3_1b_q8": {
      "display_name": "Gemma 3 (1B, Q8_0)",
      "description": "~1.8 GB VRAM. Hızlı cevap.",
      "category": "Gemma Ailesi",
      "model_id": "ggml-org/gemma-3-1b-it-GGUF",
      "filename": "gemma-3-1b-it-Q8_0.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.6,
      "system_prompt": "Sen hızlı ve pratik bir asistansın. Kısa ve öz cevaplar ver."
    },
    "llama3_8b_q4": {
      "display_name": "Llama 3.1 (8B, Q4_K_M)",
      "description": "~4.8 GB VRAM. Genel amaçlı.",
      "category": "Llama Ailesi",
      "model_id": "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF",
      "filename": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
      "context_size": 8192,
      "gpu_layers": -1,
      "temperature": 0.6,
      "system_prompt": "Sen Llama 3, Meta AI tarafından geliştirilen yardımcı bir asistansın."
    },
    "phi3_mini_4k_q4": {
      "display_name": "Phi-3 Mini (3.8B, Q4_K_M)",
      "description": "~2.2 GB VRAM. Kodlama ve mantık.",
      "category": "Phi Ailesi",
      "model_id": "microsoft/Phi-3-mini-4k-instruct-gguf",
      "filename": "Phi-3-mini-4k-instruct-q4.gguf",
      "context_size": 4096,
      "gpu_layers": -1,
      "temperature": 0.5,
      "system_prompt": "Sen Microsoft tarafından eğitilmiş, öz ve yaratıcı bir asistansın."
    }
  }
}