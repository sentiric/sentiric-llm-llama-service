# üêõ Sorun Giderme Rehberi

## YENƒ∞: Static Build Sorunlarƒ± ve √á√∂z√ºmleri

### Common Static Linking Issues

**Problem**: `libllama.so: cannot open shared object file`
```cmake
# √á√ñZ√úM: Static build flag'leri
set(LLAMA_STATIC ON)
set(BUILD_SHARED_LIBS OFF)
```

**Problem**: `libproto_lib.so: cannot open shared object file`  
```cmake
# √á√ñZ√úM: Proto library static yap
add_library(proto_lib STATIC ...)
```

### Yeni Build System Issues

**Problem**: `Could NOT find CURL` (llama.cpp > v0.9.0)
```dockerfile
# √á√ñZ√úM: CURL desteƒüini kapat
-DLLAMA_CURL=OFF
# VE: libcurl4-openssl-dev y√ºkle
RUN apt-get install -y libcurl4-openssl-dev
```

**Problem**: `llama.h: No such file or directory`
```cmake
# √á√ñZ√úM: Include path'leri manuel ayarla
include_directories(/opt/llama.cpp)
```

## YENƒ∞: Submodule-Free Architecture Best Practices

### Avantajlar
- ‚úÖ Daha hƒ±zlƒ± git clone
- ‚úÖ Submodule conflict yok  
- ‚úÖ Daha basit CI/CD pipeline
- ‚úÖ Reproducible builds

### Build Optimization
```dockerfile
# Layer caching i√ßin optimal sƒ±ra:
# 1. Baƒüƒ±mlƒ±lƒ±klar
# 2. llama.cpp build  
# 3. Ana proje build
# 4. Runtime image
```

## Hata Kataloƒüu

### 1. Library Loading Errors

**Hata**: `libllama.so: cannot open shared object file`
**Sebep**: Dynamic linking, static build deƒüil
**√á√∂z√ºm**:
```cmake
# CMakeLists.txt
set(LLAMA_STATIC ON)
set(BUILD_SHARED_LIBS OFF)
# ggml_static Sƒ∞Lƒ∞NECEK, sadece llama kalacak
```

**Hata**: `libgomp.so.1: cannot open shared object file`
**Sebep**: OpenMP runtime eksik
**√á√∂z√ºm**:
```dockerfile
# Dockerfile
RUN apt-get install -y libgomp1
```

### 2. Model Loading Issues

**Hata**: Model y√ºklenemiyor
**Kontrol Listesi**:
- ‚úÖ Model dosyasƒ± var mƒ±? `ls models/`
- ‚úÖ Disk alanƒ± yeterli mi? `df -h`
- ‚úÖ Model path doƒüru mu? `LLM_LOCAL_SERVICE_MODEL_PATH`
- ‚úÖ Permissions doƒüru mu? `chmod 644 models/*.gguf`

### 3. Build Failures

**Hata**: `ggml_static not found`
**Sebep**: Yeni llama.cpp'de bu library kaldƒ±rƒ±ldƒ±
**√á√∂z√ºm**:
```cmake
# ESKƒ∞ (Sƒ∞Lƒ∞NECEK)
target_link_libraries(llm_service PRIVATE llama ggml_static)

# YENƒ∞
target_link_libraries(llm_service PRIVATE llama)
```

**Hata**: Protobuf compilation failed
**Sebep**: vcpkg baƒüƒ±mlƒ±lƒ±klarƒ± eksik
**√á√∂z√ºm**:
```bash
docker compose down
docker system prune -f
docker compose up --build -d
```

### 4. Runtime Issues

**Hata**: Service restarting loop
**Kontrol**:
```bash
docker logs llm-llama-service
docker exec -it llm-llama-service ldd /usr/local/bin/llm_service
```

**Hata**: GRPC connection failed
**Kontrol**:
```bash
# Port a√ßƒ±k mƒ±?
netstat -tulpn | grep 16061

# Container √ßalƒ±≈üƒ±yor mu?
docker ps | grep llm-llama-service



### Hata: `error while loading shared libraries: libllama.so: cannot open shared object file`

**Sebep:**
Bu hata, `llm_service` uygulamasƒ±nƒ±n √ßalƒ±≈ümak i√ßin ihtiya√ß duyduƒüu `libllama.so` payla≈üƒ±lan k√ºt√ºphanesini bulamadƒ±ƒüƒ± anlamƒ±na gelir. Docker multi-stage build s√ºrecinde, bu `.so` dosyasƒ± `builder` katmanƒ±nda olu≈üturulmu≈ü ancak son `runtime` katmanƒ±na kopyalanmamƒ±≈ütƒ±r.

**√á√∂z√ºm:**
`Dockerfile` dosyasƒ±nƒ±n `runtime` a≈üamasƒ±na, `libllama.so` dosyasƒ±nƒ± standart bir k√ºt√ºphane yoluna kopyalayan ve ardƒ±ndan `ldconfig` ile linker √∂nbelleƒüini g√ºncelleyen adƒ±mlar eklenmelidir:

```dockerfile
# ... (runtime a≈üamasƒ±)

# YENƒ∞: Gerekli payla≈üƒ±lan k√ºt√ºphaneyi kopyala
COPY --from=builder /app/build/bin/libllama.so /usr/local/lib/

# YENƒ∞: Dinamik linker √∂nbelleƒüini g√ºncelle ki libllama.so bulunsun
RUN ldconfig

# ... (dosyanƒ±n geri kalanƒ±)
```

### Hata: `error while loading shared libraries: libXXX.so: cannot open shared object file`

**√ñrnekler:** `libllama.so`, `libggml.so`

**Sebep:**
Bu hata, `llm_service` uygulamasƒ±nƒ±n veya baƒüƒ±mlƒ±lƒ±klarƒ±ndan birinin, √ßalƒ±≈ümak i√ßin ihtiya√ß duyduƒüu bir payla≈üƒ±lan k√ºt√ºphaneyi (`.so` dosyasƒ±) bulamadƒ±ƒüƒ± anlamƒ±na gelir. `llama.cpp` projesi, `libllama.so`, `libggml.so` gibi birden fazla payla≈üƒ±lan k√ºt√ºphane √ºretir. Docker multi-stage build s√ºrecinde, bu k√ºt√ºphanelerin tamamƒ± `builder` katmanƒ±nda olu≈üturulmu≈ü ancak son `runtime` katmanƒ±na eksik kopyalanmƒ±≈ütƒ±r.

**√á√∂z√ºm:**
`Dockerfile`'ƒ±n `runtime` a≈üamasƒ±nda, `llama.cpp` tarafƒ±ndan √ºretilen **t√ºm** payla≈üƒ±lan k√ºt√ºphanelerin kopyalandƒ±ƒüƒ±ndan emin olunmalƒ±dƒ±r. Bunun en saƒülam yolu, `builder` katmanƒ±nƒ±n √ßƒ±ktƒ± dizinindeki (`/app/build/bin/`) t√ºm `.so` dosyalarƒ±nƒ± `runtime` katmanƒ±ndaki standart bir k√ºt√ºphane yoluna (`/usr/local/lib/`) kopyalamaktƒ±r. Ardƒ±ndan `ldconfig` √ßalƒ±≈ütƒ±rƒ±lmalƒ±dƒ±r.

```dockerfile
# ... (runtime a≈üamasƒ±)

# D√úZELTME: Sadece libllama.so deƒüil, GEREKLƒ∞ T√úM payla≈üƒ±lan k√ºt√ºphaneleri kopyala
COPY --from=builder /app/build/bin/*.so /usr/local/lib/

# Dinamik linker √∂nbelleƒüini g√ºncelle
RUN ldconfig

# ... (dosyanƒ±n geri kalanƒ±)
```

## Performance Issues

### High Memory Usage
- **Normal**: ~2.5GB (model + KV cache)
- **Anormal**: >4GB - memory leak ≈ü√ºphesi

### Slow Generation
- **Beklenen**: ~50 tokens/saniye
- **Yava≈ü**: <10 tokens/saniye - CPU throttle

### Debug Commands
```bash
# Memory usage
docker stats llm-llama-service

# CPU usage
top -p $(docker inspect llm-llama-service --format '{{.State.Pid}}')

# Model loading time
grep "LLM Engine initialized" docker.log
```

## Recovery Procedures

### Complete Reset
```bash
# Nuclear option
docker compose down
docker system prune -af
docker volume prune -f
./models/download.sh  # Re-download model
docker compose up --build -d
```

### Model Corruption
```bash
# Re-download model
rm models/phi-3-mini.q4.gguf
./models/download.sh
```

## Monitoring

### Health Metrics
```bash
# Automated health check
curl -s http://localhost:16060/health | jq '.model_ready'

# Response time
time curl -s http://localhost:16060/health > /dev/null
```

### Log Analysis
```bash
# Error patterns
docker logs llm-llama-service | grep -i error

# Performance issues
docker logs llm-llama-service | grep -i "slow\|timeout"
```

## Prevention

### Build Time
- Static linking kullan
- libgomp1 runtime ekle
- Multi-stage Docker build

### Runtime
- Health monitoring
- Resource limits
- Log aggregation