name: sentiric

x-common-settings: &common-settings
  dns:
    - ${DISCOVERY_SERVICE_IPV4_ADDRESS:-10.88.5.1}
    - ${PRIMARY_DNS:-8.8.8.8}
    - ${SECONDARY_DNS:-1.1.1.1}
  dns_search:
    - ${DISCOVERY_DNS_SEARCH_DOMAIN:-service.sentiric.cloud}
  restart: always
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [ gpu, compute, utility ]

networks:
  sentiric-net:
    driver: bridge
    name: sentiric-net
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET:-10.88.0.0/16}
          gateway: ${NETWORK_GATEWAY:-10.88.0.1}

volumes:
  llm-llama-models:
  llm-llama-loras:
  llm-llama-cache:

services:
  llm-llama-service:
    <<: *common-settings
    image: ghcr.io/sentiric/sentiric-llm-llama-service:latest-gpu
    build:
      context: ../sentiric-llm-llama-service
      dockerfile: Dockerfile.gpu
    volumes:
      - "${CERTIFICATES_REPO_PATH:-../sentiric-certificates}:/sentiric-certificates:ro"
      - llm-llama-models:/models
      - llm-llama-loras:/lora_adapters
    environment:
      # --- NETWORK ---
      LLM_LLAMA_SERVICE_HOST: llm-llama-service
      LLM_LLAMA_SERVICE_LISTEN_ADDRESS: 0.0.0.0
      LLM_LLAMA_SERVICE_HTTP_PORT: 16070
      LLM_LLAMA_SERVICE_GRPC_PORT: 16071
      LLM_LLAMA_SERVICE_METRICS_PORT: 16072
      
      # --- PERFORMANCE (ECO MODE) ---
      NVIDIA_VISIBLE_DEVICES: all
      # 4B model ~2.5GB tutar. 100 katman demek modelin tamamını GPU'ya yükle demektir.
      # Context 2048 ile KV Cache ~0.4GB tutar. Toplam ~3GB VRAM harcar.
      # Kalan 3GB VRAM, TTS ve STT servisleri için ayrılır.
      LLM_LLAMA_SERVICE_GPU_LAYERS: 100        
      LLM_LLAMA_SERVICE_CONTEXT_SIZE: 2048     
      LLM_LLAMA_SERVICE_MAX_BATCH_SIZE: 1      
      LLM_LLAMA_SERVICE_THREADS: 4             
      LLM_LLAMA_SERVICE_DEFAULT_TEMPERATURE: 0.2
      # --- INTELLIGENCE & ADAPTIVE LANGUAGE ---
      LLM_LLAMA_SERVICE_DEFAULT_SYSTEM_PROMPT: |
        Sen 'Sentirik'sin. Telefonda hizmet veren, yüksek empati yeteneğine sahip profesyonel bir asistansın. İstanbul Türkçesi ile konuş.

        ### DAVRANIŞSAL PROTOKOLLER:
        1. GÜLÜMSEYEN SES: Kelimelerin sıcak, yardımsever ve enerjik olsun. Robotik ve soğuk ifadelerden kaçın.
        2. EMPATİ VE PARTNERLİK: "Sizi anlıyorum", "Birlikte kontrol edelim" gibi 'biz' dilini kullan. Sorunla müşteriyle birlikte savaş.
        3. POZİTİF DİL: "-me/-ma" eklerini ve "hayır/yapamayız" gibi kesin reddedişleri minimize et. Bunun yerine her zaman alternatif veya çözüm odaklı bir cümle kur.
        4. L.A.S.T. YÖNTEMİ: Şikayetlerde önce dinle (onayla), durum için özür dile, çözümü sun ve teşekkür et.
        5. AYNALAMA (MIRRORING): Kullanıcının hızına ve tonuna hafifçe uyum sağla ama profesyonelliğini koru.

        ### İLETİŞİM KURALLARI:
        - DİL: Türkiye Türkçesi.
        - HİTAP: Müşteriye ismiyle hitap et (örn: "Ahmet Bey", "Ayşe Hanım").
        - RAG: Sadece [BİLGİ] bloğundaki veriyi kullan. Bilgi yoksa "Hemen kontrol ediyorum" diyerek nazikçe operatöre yönlendir.
        - GÜVENLİK: Acil durumlarda vakit kaybetmeden 112'ye yönlendir.

      LLM_LLAMA_SERVICE_DEFAULT_RAG_PROMPT: |
        [BİLGİ]
        {{rag_context}}
        [BİLGİ SONU]

        Yukarıdaki bağlamı kullanarak cevapla:
        Soru: {{user_prompt}}

      LLM_LLAMA_SERVICE_DEFAULT_RAG_PROMPT_LOW: |
        [TALİMAT]: Cevap vermeden önce kısaca düşün. Düşüncelerini <think> etiketleri içine al.

      LLM_LLAMA_SERVICE_DEFAULT_RAG_PROMPT_HIGH: |
        [TALİMAT]: Bu karmaşık bir görev. Adım adım derinlemesine düşün. Analizini <think> etiketleri içine al.

      # --- SECURITY ---
      GRPC_TLS_CA_PATH: /sentiric-certificates/certs/ca.crt
      LLM_LLAMA_SERVICE_CERT_PATH: /sentiric-certificates/certs/llm-llama-service-chain.crt
      LLM_LLAMA_SERVICE_KEY_PATH: /sentiric-certificates/certs/llm-llama-service.key
      
      # --- LOGGING ---
      LLM_LLAMA_SERVICE_LOG_LEVEL: info
      
    networks:
      sentiric-net:
        ipv4_address: ${LLM_LLAMA_SERVICE_IPV4_ADDRESS:-10.88.60.7}
    ports:
      - "${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}"
      - "${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}:${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}"
      - "${LLM_LLAMA_SERVICE_METRICS_PORT:-16072}:${LLM_LLAMA_SERVICE_METRICS_PORT:-16072}"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}/health" ]
      interval: 10s
      timeout: 5s
      retries: 5