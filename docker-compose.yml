services:
  llm-llama-service:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: llm-llama-service
    ports:
      - "16060:16060" # HTTP Health
      - "16061:16061" # gRPC
    volumes:
      - ./models:/models  # üìÅ Model klas√∂r√º
    environment:
      - LLM_LOCAL_SERVICE_MODEL_PATH=/models/phi-3-mini.q4.gguf
      - LOG_LEVEL=info
      - LLM_LOCAL_SERVICE_HTTP_PORT=16060
      - LLM_LOCAL_SERVICE_GRPC_PORT=16061
      - LLM_CONTEXT_SIZE=4096
      - LLM_THREADS=4
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:16060/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 5m