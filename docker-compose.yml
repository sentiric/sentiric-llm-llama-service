services:
  llm-llama-service:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: llm-llama-service
    ports:
      - "127.0.0.1:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}"
      - "127.0.0.1:${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}:${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}"
    volumes:
      - ./models:/models
    environment:
      # Tamamen standartlaştırılmış ortam değişkenleri
      - LLM_LLAMA_SERVICE_IPV4_ADDRESS=0.0.0.0
      - LLM_LLAMA_SERVICE_HTTP_PORT=${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}
      - LLM_LLAMA_SERVICE_GRPC_PORT=${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}
      - LLM_LLAMA_SERVICE_MODEL_PATH=${LLM_LLAMA_SERVICE_MODEL_PATH:-/models/phi-3-mini.q4.gguf}
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=${LLM_LLAMA_SERVICE_CONTEXT_SIZE:-4096}
      - LLM_LLAMA_SERVICE_THREADS=${LLM_LLAMA_SERVICE_THREADS:-4}
      - LLM_LLAMA_SERVICE_LOG_LEVEL=${LLM_LLAMA_SERVICE_LOG_LEVEL:-info}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 5m