# docker-compose.yml
# Bu dosya, LLM Llama hizmetini CPU üzerinde çalıştırmak için kullanılır.
# GPU desteği olmadan çalıştırmak için uygundur.
services:
  llm-llama-service:
    image: ghcr.io/sentiric/sentiric-llm-llama-service:latest
    container_name: llm-llama-service
    ports:
      - "127.0.0.1:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}"
      - "127.0.0.1:${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}:${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}"
    volumes:
      - ./models:/models
    environment:
      # --- Network Ayarları ---
      - LLM_LLAMA_SERVICE_IPV4_ADDRESS=0.0.0.0
      - LLM_LLAMA_SERVICE_HTTP_PORT=${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}
      - LLM_LLAMA_SERVICE_GRPC_PORT=${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}
      
      # --- Model Yönetimi ---
      - LLM_LLAMA_SERVICE_MODEL_DIR=/models
      - LLM_LLAMA_SERVICE_MODEL_ID=${LLM_LLAMA_SERVICE_MODEL_ID:-microsoft/Phi-3-mini-4k-instruct-gguf}
      - LLM_LLAMA_SERVICE_MODEL_FILENAME=${LLM_LLAMA_SERVICE_MODEL_FILENAME:-Phi-3-mini-4k-instruct-q4.gguf}
      # Eski MODEL_PATH değişkeni de ayarlanabilir, ancak ID/Filename önceliklidir.
      # - LLM_LLAMA_SERVICE_MODEL_PATH=/models/my-custom-model.gguf

      # --- Motor ve Log Ayarları ---
      # LLM_LLAMA_SERVICE_GPU_LAYERS: GPU'ya yüklenecek katman sayısı.
      # -1: Tüm katmanlar (GPU belleği yeterliyse).
      #  >0: Belirli sayıda katman. GPU belleği yetersizse düşürülmeli.
      - LLM_LLAMA_SERVICE_GPU_LAYERS=${LLM_LLAMA_SERVICE_GPU_LAYERS:-28}
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=${LLM_LLAMA_SERVICE_CONTEXT_SIZE:-4096}
      # LLM_LLAMA_SERVICE_THREADS: Eşzamanlı istek sayısı.
      # GPU belleği yetersizse azaltılmalı.
      - LLM_LLAMA_SERVICE_THREADS=${LLM_LLAMA_SERVICE_THREADS:-1}
      - LLM_LLAMA_SERVICE_LOG_LEVEL=${LLM_LLAMA_SERVICE_LOG_LEVEL:-info}

      # --- Varsayılan Sampling Parametreleri ---
      - LLM_LLAMA_SERVICE_DEFAULT_MAX_TOKENS=${LLM_LLAMA_SERVICE_DEFAULT_MAX_TOKENS:-1024}
      - LLM_LLAMA_SERVICE_DEFAULT_TEMPERATURE=${LLM_LLAMA_SERVICE_DEFAULT_TEMPERATURE:-0.8}
      - LLM_LLAMA_SERVICE_DEFAULT_TOP_K=${LLM_LLAMA_SERVICE_DEFAULT_TOP_K:-40}
      - LLM_LLAMA_SERVICE_DEFAULT_TOP_P=${LLM_LLAMA_SERVICE_DEFAULT_TOP_P:-0.95}
      - LLM_LLAMA_SERVICE_DEFAULT_REPEAT_PENALTY=${LLM_LLAMA_SERVICE_DEFAULT_REPEAT_PENALTY:-1.1}
      
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15m