services:
  llm-llama-service:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: sentiric/llm-llama-service:local-gpu
    env_file: [".env.gpu"]
    networks:
      - sentiric-net
