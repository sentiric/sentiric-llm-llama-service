# docker-compose.gpu.override.yml
# Yerel GPU geliştirme için kullanılır
services:
  llm-llama-service:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: sentiric/llm-llama-service:local-gpu
    environment:
      # --- OPTİMİZE EDİLMİŞ 6GB GPU AYARLARI ---
      - LLM_LLAMA_SERVICE_GPU_LAYERS=28
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=1024
      - LLM_LLAMA_SERVICE_THREADS=1
      - LLM_LLAMA_SERVICE_THREADS_BATCH=1
      
      # --- YENİ WARM-UP OPTİMİZASYONU ---
      - LLM_LLAMA_SERVICE_ENABLE_WARM_UP=true
      
      # --- PERFORMANS AYARLARI ---
      - LLM_LLAMA_SERVICE_USE_MMAP=true
      - LLM_LLAMA_SERVICE_KV_OFFLOAD=true
      
      # --- BATCHING (Geçici olarak devre dışı) ---
      - LLM_LLAMA_SERVICE_ENABLE_BATCHING=false
      - LLM_LLAMA_SERVICE_MAX_BATCH_SIZE=1
      - LLM_LLAMA_SERVICE_BATCH_TIMEOUT_MS=10