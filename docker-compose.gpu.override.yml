# docker-compose.gpu.override.yml
# Yerel GPU geliştirme için kullanılır
services:
  llm-llama-service:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: sentiric/llm-llama-service:local-gpu
    environment:
      # --- OPTİMİZE EDİLMİŞ 6GB GPU AYARLARI ---
      - LLM_LLAMA_SERVICE_GPU_LAYERS=28
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=1024
      - LLM_LLAMA_SERVICE_THREADS=1
      - LLM_LLAMA_SERVICE_THREADS_BATCH=1
      
      # --- YENİ ÖZELLİKLER ---
      - LLM_LLAMA_SERVICE_ENABLE_BATCHING=false
      - LLM_LLAMA_SERVICE_MAX_BATCH_SIZE=1
      - LLM_LLAMA_SERVICE_BATCH_TIMEOUT_MS=10
      - LLM_LLAMA_SERVICE_ENABLE_WARM_UP=true