services:
  llm-llama-service:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: sentiric/llm-llama-service:local-gpu
    # BURASI ÖNEMLİ: Hangi env dosyasını okuduğuna dikkat edin.
    # Eğer .env.gpu ise, o dosyadaki ayarlar geçerli olur.
    env_file: [".env.gpu"] 
    networks:
      - sentiric-net