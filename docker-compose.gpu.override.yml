services:
  llm-llama-service:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: sentiric/llm-llama-service:local-gpu
    environment:
      
      # --- GPU AYARLARI ---
      - LLM_LLAMA_SERVICE_GPU_LAYERS=28
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=1024
      
      # BURAYI GÃœNCELLE (1 -> 4)
      - LLM_LLAMA_SERVICE_THREADS=4
      - LLM_LLAMA_SERVICE_THREADS_BATCH=4
      
      - LLM_LLAMA_SERVICE_ENABLE_WARM_UP=true
      
      # BATCHING AYARLARI
      - LLM_LLAMA_SERVICE_ENABLE_BATCHING=true
      - LLM_LLAMA_SERVICE_MAX_BATCH_SIZE=4
      - LLM_LLAMA_SERVICE_BATCH_TIMEOUT_MS=10

      # --- PERFORMANS ---
      - LLM_LLAMA_SERVICE_USE_MMAP=true
      - LLM_LLAMA_SERVICE_KV_OFFLOAD=true