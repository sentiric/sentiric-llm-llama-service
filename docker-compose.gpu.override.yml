# docker-compose.gpu.override.yml
services:
  llm-llama-service:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: sentiric/llm-llama-service:local-gpu
    environment:
      # --- DEBUG MOD ---
      - LLM_LLAMA_SERVICE_LOG_LEVEL=debug
      
      # --- GPU AYARLARI ---
      - LLM_LLAMA_SERVICE_GPU_LAYERS=28
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=1024
      - LLM_LLAMA_SERVICE_THREADS=1
      - LLM_LLAMA_SERVICE_THREADS_BATCH=1
      
      # --- WARM-UP KAPAT ---
      - LLM_LLAMA_SERVICE_ENABLE_WARM_UP=false
      
      # --- BATCHING ---
      - LLM_LLAMA_SERVICE_ENABLE_BATCHING=true
      - LLM_LLAMA_SERVICE_MAX_BATCH_SIZE=4
      - LLM_LLAMA_SERVICE_BATCH_TIMEOUT_MS=10
      
      # --- PERFORMANS ---
      - LLM_LLAMA_SERVICE_USE_MMAP=true
      - LLM_LLAMA_SERVICE_KV_OFFLOAD=true