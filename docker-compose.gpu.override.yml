# docker-compose.gpu.override.yml
services:
  llm-llama-service:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: sentiric/llm-llama-service:local-gpu
    environment:
      # --- 6GB VRAM İÇİN OPTİMİZE EDİLMİŞ AYARLAR ---
      - LLM_LLAMA_SERVICE_GPU_LAYERS=-1 # Modeli olabildiğince GPU'ya yükle
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=4096 # Context boyutunu düşürerek her thread'in VRAM kullanımını azalt
      - LLM_LLAMA_SERVICE_THREADS=2 # Eş zamanlı istek sayısını VRAM'e sığacak şekilde düşür
      - LLM_LLAMA_SERVICE_THREADS_BATCH=2 # Genellikle THREADS ile aynı tutulur