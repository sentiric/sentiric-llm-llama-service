#include <iostream>
#include <string>
#include <vector>
#include <map>
#include <functional>
#include "spdlog/spdlog.h"
#include "grpc_client.h"
#include "health_check.h"
#include "benchmark.h"
#include "nlohmann/json.hpp"

void print_usage() {
    std::cout << R"(
 Sentiric LLM CLI v2.5

Kullan覺m:
  llm_cli [se癟enekler] <komut> [arg羹manlar]

Komutlar:
  generate <user_prompt>   - Zengin bir balam ile metin 羹retir (GRPC stream).
  health                   - Sistem sal覺k durumunu kontrol eder.
  wait-for-ready           - Servis haz覺r olana kadar bekler.
  benchmark                - Performans ve ezamanl覺l覺k testi 癟al覺t覺r覺r.
  interrupt-test           - Voice Gateway s繹z kesme (interruption) senaryosunu sim羹le eder.

Se癟enekler:
  --system-prompt <text>   - (Opsiyonel) AI'n覺n kiiliini belirleyen sistem talimat覺.
  --rag-context <text>     - (Opsiyonel) RAG i癟in kullan覺lacak bilgi metni.
  --history '<json_string>'- (Opsiyonel) Konuma ge癟mii.
  --grpc-endpoint <addr>   - GRPC endpoint (varsay覺lan: llm-llama-service:16071).
  --http-endpoint <addr>   - HTTP endpoint (varsay覺lan: llm-llama-service:16070).
  --timeout <seconds>      - 襤stek zaman a覺m覺 s羹resi (saniye, varsay覺lan: 120).
  --iterations <n>         - Benchmark: Seri iterasyon say覺s覺 (varsay覺lan: 10).
  --concurrent <n>         - Benchmark: Ezamanl覺 balant覺 say覺s覺 (varsay覺lan: 1).
  --requests <n>           - Benchmark: Balant覺 ba覺na istek say覺s覺 (varsay覺lan: 1).
  --output <file>          - Benchmark raporu i癟in 癟覺kt覺 dosyas覺.

rnekler:
  # Basit metin 羹retme
  llm_cli generate "T羹rkiye'nin bakenti neresidir?"

  # S繹z kesme testi (C++ ile)
  llm_cli interrupt-test
)";
}

int main(int argc, char** argv) {
    spdlog::set_level(spdlog::level::info);

    if (argc < 2) {
        print_usage();
        return 1;
    }

    std::vector<std::string> args(argv + 1, argv + argc);
    std::map<std::string, std::string> options;
    std::string command;
    std::vector<std::string> command_args;

    for (size_t i = 0; i < args.size(); ++i) {
        if (args[i].rfind("--", 0) == 0) {
            std::string key = args[i].substr(2);
            if (i + 1 < args.size() && args[i + 1].rfind("--", 0) != 0) {
                options[key] = args[++i];
            } else { options[key] = "true"; }
        } else if (command.empty()) {
            command = args[i];
        } else {
            command_args.push_back(args[i]);
        }
    }
    
    std::string grpc_endpoint = options.count("grpc-endpoint") ? options["grpc-endpoint"] : "llm-llama-service:16071";
    std::string http_endpoint = options.count("http-endpoint") ? options["http-endpoint"] : "llm-llama-service:16070";

    try {
        if (command == "generate") {
            if (command_args.empty()) { 
                spdlog::error("generate komutu i癟in bir kullan覺c覺 girdisi gereklidir.");
                print_usage();
                return 1;
            }
            std::string user_prompt;
            for (const auto& arg : command_args) { user_prompt += arg + " "; }
            user_prompt.pop_back();

            sentiric::llm::v1::GenerateStreamRequest request;
            request.set_user_prompt(user_prompt);
            
            if (options.count("system-prompt")) request.set_system_prompt(options["system-prompt"]);
            if (options.count("rag-context")) request.set_rag_context(options["rag-context"]);
            if (options.count("history")) {
                try {
                    auto history_json = nlohmann::json::parse(options["history"]);
                    for (const auto& item : history_json) {
                        auto* turn = request.add_history();
                        turn->set_role(item["role"]);
                        turn->set_content(item["content"]);
                    }
                } catch (const nlohmann::json::parse_error& e) {
                    spdlog::error("--history arg羹man覺 ge癟erli bir JSON deil: {}", e.what());
                    return 1;
                }
            }
            
            sentiric_llm_cli::GRPCClient client(grpc_endpoint);
            if (options.count("timeout")) client.set_timeout(std::stoi(options["timeout"]));

            std::cout << " Assistant: " << std::flush;
            
            bool success = client.generate_stream(request, [](const std::string& token) {
                std::cout << token << std::flush;
            });
            std::cout << std::endl;

            if (!success) {
                spdlog::error("Generation baar覺s覺z oldu.");
                return 1;
            }

        } else if (command == "health" || command == "wait-for-ready") {
             sentiric_llm_cli::HealthChecker checker(grpc_endpoint, http_endpoint);
             if (command == "health") {
                 checker.print_detailed_status();
             } else {
                 int timeout = options.count("timeout") ? std::stoi(options["timeout"]) : 300;
                 if (!checker.wait_for_ready(timeout)) return 1;
             }
        } else if (command == "benchmark") {
             int iterations = options.count("iterations") ? std::stoi(options["iterations"]) : 10;
             int concurrent = options.count("concurrent") ? std::stoi(options["concurrent"]) : 1;
             int requests = options.count("requests") ? std::stoi(options["requests"]) : 1;
             std::string output_file = options.count("output") ? options["output"] : "";
             
             sentiric_llm_cli::Benchmark benchmark(grpc_endpoint);
             sentiric_llm_cli::BenchmarkResult result;

             if (concurrent > 1) {
                 result = benchmark.run_concurrent_test(concurrent, requests);
             } else {
                 result = benchmark.run_performance_test(iterations);
             }
             benchmark.generate_report(result, output_file);
        } else if (command == "interrupt-test") {
            sentiric_llm_cli::Benchmark benchmark(grpc_endpoint);
            std::string initial = "Merhaba, sipariim ne alemde? Kargoya verildi mi, kargo takip numaras覺n覺 ve teslimat adresini 繹renebilir miyim?";
            std::string interrupt = "Pardon s繹z羹n羹 kestim, sadece kargo numaras覺n覺 alabilir miyim?";
            benchmark.run_interrupt_test(initial, interrupt);
        }
        else {
            spdlog::error("Ge癟ersiz komut: '{}'", command);
            print_usage();
            return 1;
        }

    } catch (const std::exception& e) {
        spdlog::critical("CLI hatas覺: {}", e.what());
        return 1;
    }
    
    return 0;
}